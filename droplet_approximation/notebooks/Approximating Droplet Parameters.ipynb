{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b5a1ae8",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook provides the ability to generate random droplet parameters, write them to disk, and \n",
    "train a neural network with said droplet parameters to approximate the underlying ODEs that govern \n",
    "the droplet parameters.  Once trained researchers can generate a Fortran 90 module that provides \n",
    "the ability to estimate droplet radius and temperature for some time in the future.  \n",
    "\n",
    "The intent is that a small, reasonably trained neural network can provide accurate enough droplet \n",
    "characteristic estimations that are significantly faster than an iterative Gauss-Newton technique.\n",
    "Initial testing indicates a small 4-layer network (roughly 2400 parameters) with Fortran 90 module\n",
    "generated by this notebook is 30-90x faster than the existing (as of 2024/09/25) iterative approach \n",
    "which results in roughly a factor of 2x overall simulation speedup.\n",
    "\n",
    "This notebook is broken down into the following sections:\n",
    "\n",
    "1. ODEs of interest\n",
    "2. Mapping data to/from $[-1, 1]$\n",
    "3. Generating random droplets\n",
    "4. Training a neural network\n",
    "5. Analyzing a network's performance\n",
    "6. Exporting a network to Fortran 90\n",
    "\n",
    "\n",
    "# Setup\n",
    "Since training a neural network to approximate ODEs has multiple workflows (e.g. training a model\n",
    "vs loading previous models to analyze their performance) there are several variables that need to\n",
    "be set to exercise all of this notebook's functionality.  In particular, the following should\n",
    "be reviewed and set depending on which workflows are of interest:\n",
    "\n",
    "- Loading a previously trained model: `load_model_flag`, `model_load_path`\n",
    "- Training a new model: `train_model_flag`, `training_data_path`, `number_epochs`\n",
    "- Saving a newly trained model: `save_model_path`\n",
    "\n",
    "These should be set at the top of the notebook.  Please adhere to the modification instructions\n",
    "where the variables are defined.\n",
    "\n",
    "## Training Data\n",
    "Training data is not included in the repository due to size, though creating data on the fly\n",
    "using `create_training_file()` is slow since it is single threaded and this notebook does\n",
    "not implement a parallel data generation process.\n",
    "\n",
    "Those who have access to Dr. Richter's group network drive can download previously created \n",
    "data sets.  Users external to the group can either slowly create data using the tools in this \n",
    "notebook or use scripts exported from an earlier version of this notebook \n",
    "(`generate_training_data.py` and `loop-training-data.sh`) to generate data in bulk on\n",
    "multiple cores.\n",
    "\n",
    "## Python Dependencies\n",
    "The following Python packages are needed to exercise full functionality in this notebook \n",
    "(versions tested in parentheses):\n",
    "\n",
    "- Python3 (3.11.5)\n",
    "- Matplotlib (3.7.1)\n",
    "- NumPy (1.24.3)\n",
    "- Pandas (1.5.3)\n",
    "- PyTorch (2.3.0)\n",
    "\n",
    "There is no fundamental dependence on any particular version of the dependencies and, barring\n",
    "any bugs encountered with the packages' main APIs, newer versions are expected to work.\n",
    "\n",
    "# Notebook Care and Feeding\n",
    "Notebooks are great for exploring ideas and rapidly iterating to a solution.  They are less\n",
    "great for maintaining a \"production\" workflow that needs to be used by multiple people\n",
    "on a semi-regular basis.  As such, the following guidelines should be followed when making\n",
    "updates to this so as to preserve everyone's sanity:\n",
    "\n",
    "- Do not commit the notebook with outputs from a previous run.  This greatly reduces the\n",
    "  file size in the repository and avoids unnecessary changes when someone re-runs a cell\n",
    "  whose output changes on each execution.\n",
    "- The notebook should always allow execution of all cells without errors.  Restarting the\n",
    "  kernel and running all cells should be run to confirm this (then restart and clear output).\n",
    "\n",
    "# Future Work\n",
    "\n",
    "## Hyperparameter Search on the Neural Network's Architecture\n",
    "The network architecture and hyperparameters were chosen because they:\n",
    "\n",
    "1. Resulted in a small network\n",
    "2. Had sizes that *should be* efficient to work with on the CPU\n",
    "\n",
    "A very limited hyperparameter search has settled on parameters that appear to reliably train\n",
    "performant networks though the path from start to where we're at was very ad hoc.  Fiddling\n",
    "with hyperparameters stopped as soon as a network that was reasonably accurate (and was\n",
    "fairly reproducible) and fast enough when implemented in Fortran. \n",
    "\n",
    "A more thorough exploration of the following could be done in an attempt to generate a more accurate\n",
    "neural network, or a smaller (faster) network so more particles could be simulated without impacting\n",
    "simulation run-times.\n",
    "\n",
    "Areas to explore include (roughly in priority order):\n",
    "\n",
    "1. Number and size of MLP-layers\n",
    "2. Learning rate and schedule\n",
    "3. Weights regularization (e.g. L1 or L2 penalties)\n",
    "\n",
    "## Improving Neural Network Performance\n",
    "There are a handful of non-architecture/hyperparameter-related things to explore to improve\n",
    "performance.  Unless otherwise specified, these are all speculations on Greg's part and aren't\n",
    "a sure bet.\n",
    "\n",
    "### Train on Sequences of Integration Times for the Same Parameter\n",
    "Currently training data \n",
    "\n",
    "### Normalize the Neural Networks's Integration Time Parameter\n",
    "Currently all of the neural network's inputs, except for integration time, are normalized into\n",
    "the range $[-1, 1]$.  Integration time remains in the range of $(0, 10)$ with a focus on $[10^{-3}, 10]$\n",
    "so as to cover both DNS and LES simulations.  The decision to leave integration time unnormalized\n",
    "was by accident rather than a conscious choice.\n",
    "\n",
    "That said, it is unknown as to whether this negatively impacts the performance of the model.  Since\n",
    "DNS simulations focus on smaller time steps (in the range of $[10^{-3}, 10^{-1})$) and LES simulations on \n",
    "larger (in the range of $(10^{-1}, 10)$) it may be worthwhile to perform the same log-scale normalization\n",
    "to the integration time as is done for the droplet's radius and salinity.  The thought is that the\n",
    "network would be less sensitive to the scale separation between DNS and LES time steps and learn\n",
    "them equally.  That said, no quantitative analysis has been performed that would suggest this - it is\n",
    "purely an unfounded hypothesis at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772aafd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from droplet_approximation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae14a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# NOTE: Do not change this cell directly!  Add a cell below and set new values for the\n",
    "#       variables of interest.  This makes it significantly easier to revert your changes\n",
    "#       when commiting changes to the repository - just delete the next cell.\n",
    "#\n",
    "\n",
    "# Change this to False if you've previously trained a model and want to to evaluate\n",
    "# its performance rather than a pre-trained model from disk\n",
    "load_model_flag = True\n",
    "\n",
    "# Path to the model, relative to this notebook, to load for performance analysis.\n",
    "model_load_path = \"../models/network_box_residual_l1_epoch_14.pth\"\n",
    "\n",
    "# Change this to True if you want to train a model from scratch (loading and continuing\n",
    "# training is not supported yet).  Since generating sufficient training data is typically\n",
    "# longer than most researchers are willing to wait (O(1 hour) minimum with multiple cores)\n",
    "# the training_data_path variable must also be set.\n",
    "train_model_flag = False\n",
    "\n",
    "# Path to previously created droplet parameters.\n",
    "#\n",
    "# NOTE: You must update this to where you've created/copied the training data!\n",
    "#\n",
    "# training_data_path = \"../data/time_log_spaced.data\"\n",
    "training_data_path = None\n",
    "\n",
    "# Path to validation data used to evaluate model performance during training.\n",
    "validation_data_path = None\n",
    "\n",
    "# Number of times the model should see the entirety of the training data\n",
    "# before stopping the optimization process.\n",
    "number_epochs      = 10\n",
    "\n",
    "# Name of the currently trained model.  This is used when writing the model's weights\n",
    "# as a Fortran module.  Default to the name of the loaded model, sans extension, as\n",
    "# that should match the default training configuration that generated it.\n",
    "model_name = model_load_path.split( \"/\" )[-1].split( \".\" )[0]\n",
    "\n",
    "# Path to the model, relative to this notebook, to save newly trained models.\n",
    "#\n",
    "# NOTE: We disable saving by default to avoid accidentally overwriting an existing model!\n",
    "#\n",
    "#model_save_path = \"../models/mlp_4layer-b=1024-lr=1e-3_halvingschedule-l2reg=1e-6.pth\"\n",
    "model_save_path = None\n",
    "\n",
    "# Prefix to generate model checkpoint pathnames from.  Default to the model's save\n",
    "# path, sans extension.\n",
    "model_checkpoint_prefix = None\n",
    "if model_save_path is not None:\n",
    "    model_checkpoint_prefix, _ = os.path.splittext( model_save_path )\n",
    "\n",
    "# Change this if the droplet_model.f90 module should be created.  This will be\n",
    "# generated from either 1) a newly trained model or 2) a previously trained, loaded\n",
    "# model, in that order.\n",
    "write_weights_flag = False\n",
    "\n",
    "# Ranges on the droplet parameters.\n",
    "#\n",
    "# NOTE: These *must* match both the training/evaluation data as well as the model.\n",
    "#\n",
    "parameter_ranges = get_parameter_ranges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e986a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training configuration.  Uncomment the variable assignments below to use.\n",
    "# This trains a file from data in the ../data/ directory and saves the weights in\n",
    "# the ../models/ directory.  It does not write out a new droplet_model.f90 module.\n",
    "\n",
    "train_model_flag        = True\n",
    "training_data_path      = \"../data/training/time_log_spaced.training_data\"\n",
    "validation_data_path    = None\n",
    "model_save_path         = \"../models/NEW-mlp_4layer-b=1024-lr=1e-3_halvingschedule-l2reg=1e-6-TEST.pth\"\n",
    "model_name              = model_save_path.split( \"/\" )[-1].split( \".\" )[0]\n",
    "model_load_path         = model_save_path\n",
    "write_weights_flag      = True\n",
    "custom_parameter_ranges = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4082b83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay the custom parameter ranges and set them for the remainder of the\n",
    "# notebook.\n",
    "parameter_ranges.update( custom_parameter_ranges )\n",
    "\n",
    "set_parameter_ranges( parameter_ranges )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599c1885",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Force NumPy to print more values on each line rather than wrapping\n",
    "# around 80 characters.\n",
    "np.set_printoptions( linewidth=120 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cbd2ee-fa2f-4f9a-a36d-b3f74055986f",
   "metadata": {},
   "source": [
    "# Differential Equations of Interest\n",
    "Example code that solves for a droplet's radius and temperature given its current radius and temperature\n",
    "as well as the environment it is in, described by:\n",
    "\n",
    "- The droplet's salinity\n",
    "- Air temperature\n",
    "- Relative humidity\n",
    "- $\\rho_a$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901fbcc8",
   "metadata": {},
   "source": [
    "Solve the ODEs for a set of parameters in the middle of each of the valid ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260959ff-4251-4b1a-8b74-00693462f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial values for the droplet.\n",
    "#\n",
    "# NOTE: We must use a list for the initial values rather than a NumPy array\n",
    "#       as an array will somehow cause a divide by zero.  I did not have\n",
    "#       enough time to figure out why this is the case.\n",
    "#\n",
    "y0 = [0, 0]\n",
    "y0[0] = np.float32( 1e-4 )  # Radius in meters.\n",
    "y0[1] = np.float32( 293 )   # Temperature Kelvin.\n",
    "\n",
    "# Environmental parameters to solve the ODEs with.\n",
    "#\n",
    "# Salinity (m_s)         ~ kg/m^3\n",
    "# Air temperature (Tf)   ~ Kelvin\n",
    "# Relative humidity (RH) ~ Fractional, typically in [0.5, 1.15]\n",
    "# rhoa                   ~ kg/m^3\n",
    "parameters = np.array( [1e-18, 291.0, 0.7, 1.0] )\n",
    "\n",
    "# The ODEs are valid over t_span and solutions are returned over t_eval.\n",
    "t_span = (0, 10)\n",
    "t_eval = np.linspace( 0, 10, 1000 )\n",
    "\n",
    "#\n",
    "# NOTE: vectorized=True doesn't allow calling with arrays, but it lets the ODE solver\n",
    "#       operate on multiple values at once if it chooses to.\n",
    "#\n",
    "solution = solve_ivp_float32_outputs( dydt, t_span, y0, method=\"Radau\", t_eval=t_eval, args=(parameters,) )\n",
    "\n",
    "plot_droplet_size_temperature( solution.y, solution.t )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac663ff",
   "metadata": {},
   "source": [
    "# Mapping Droplet Parameters to/from $[-1, 1]$\n",
    "Create routines for moving between the normal range of physical parameters and\n",
    "a mapping on the range [-1, 1].  The ODEs are solved with physical parameters,\n",
    "each with their own dynamic range, while the model operates on parameters that\n",
    "each have the same range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af71f6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of each of the parameter ranges, in the order within a droplet's parameters.\n",
    "parameter_names = [\"radius\",\n",
    "                   \"temperature\",\n",
    "                   \"salt_mass\",\n",
    "                   \"air_temperature\",\n",
    "                   \"relative_humidity\",\n",
    "                   \"rhoa\",\n",
    "                   \"time\"]\n",
    "\n",
    "# Print out the ranges so the user can review them.\n",
    "longest_name_length      = max( map( lambda name: len( name ), parameter_names ) )\n",
    "parameter_range_template = \"  {{:<{:d}s}} [{{:.2f}}, {{:.2f}}]\".format( longest_name_length + 4 )\n",
    "\n",
    "print( \"Parameter ranges used:\\n\" )\n",
    "for parameter_name in parameter_names:\n",
    "    print( parameter_range_template.format(\n",
    "        parameter_name + \":\",\n",
    "        parameter_ranges[parameter_name][0],\n",
    "        parameter_ranges[parameter_name][1] ) )\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e70b36-1058-4c37-ab71-d2815f31a808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test normalizing output parameters that are...\n",
    "#\n",
    "#   1. at the lower edge of the valid input space\n",
    "#   2. in the middle of the valid input space\n",
    "#   3. at the upper edge of the valid input space\n",
    "#   4. below the lower edge of the valid input space\n",
    "#   5. above the upper edge of the valid input space\n",
    "#\n",
    "# NOTE: We use a programmatic approach for selecting the values to test so that\n",
    "#       these are scaled to the current parameter ranges.  Otherwise, whatever\n",
    "#       values we select by hand will fail the test when the parameter ranges\n",
    "#       change.\n",
    "#\n",
    "# Normalization is followed by an inverse scaling to bring the values back to\n",
    "# physically interpretable ranges.  This is done three different times to\n",
    "# test:\n",
    "#\n",
    "#  - Just radius and temperature\n",
    "#  - Radius and temperature, along with the environmental parameters\n",
    "#  - Radius and temperature, the environmental parameters, and an integration time\n",
    "#\n",
    "\n",
    "# The following tuples represent the above positions in parameter space as\n",
    "# the index into the parameter range (0 for lower bound, 1 for upper bound)\n",
    "# and a scale value for said bound.\n",
    "LOWER_BOUND      = (0, 1)\n",
    "MID_RANGE        = (0, 1.5)\n",
    "UPPER_BOUND      = (1, 1)\n",
    "INVALID_TOO_LOW  = (0, 0.75)\n",
    "INVALID_TOO_HIGH = (1, 1.25)\n",
    "\n",
    "TEST_CASES = [LOWER_BOUND,\n",
    "              MID_RANGE,\n",
    "              UPPER_BOUND,\n",
    "              INVALID_TOO_LOW,\n",
    "              INVALID_TOO_HIGH]\n",
    "\n",
    "def param_value( parameter_ranges, parameter, test_tuple ):\n",
    "    return parameter_ranges[parameter][test_tuple[0]] * test_tuple[1]\n",
    "\n",
    "# Construct each of the test droplets, one at a time.\n",
    "test_droplets = []\n",
    "for test_case in TEST_CASES:\n",
    "    test_droplet = np.empty( (1, len( parameter_ranges.keys() )) )\n",
    "    \n",
    "    for parameter_index, parameter_name in enumerate( parameter_names ):\n",
    "        test_droplet[0, parameter_index] = param_value( parameter_ranges, parameter_name, test_case )\n",
    "        \n",
    "    # Radius, salt mass, and integration time have logarithmic ranges, so we\n",
    "    # convert them to linear values\n",
    "    test_droplet[0, 0] = 10**test_droplet[0, 0]\n",
    "    test_droplet[0, 2] = 10**test_droplet[0, 2]\n",
    "    test_droplet[0, 6] = 10**test_droplet[0, 6]\n",
    "    \n",
    "    test_droplets.append( test_droplet )\n",
    "\n",
    "# Convert the test parameters into a 2D array.\n",
    "test_parameters = np.vstack( test_droplets )\n",
    "\n",
    "# Each test looks a larger subset of the parameters.\n",
    "inputs_range          = range( 0, 2 )\n",
    "inputs_env_range      = range( 0, 6 )\n",
    "inputs_env_time_range = range( 0, 7 )\n",
    "\n",
    "if not np.allclose( scale_droplet_parameters( normalize_droplet_parameters( test_parameters[:, inputs_range] ) ),\n",
    "                    test_parameters[:, inputs_range] ):\n",
    "    raise ValueError( \"Scaling normalized outputs was not an inverse operation!\" )\n",
    "\n",
    "if not np.allclose( scale_droplet_parameters( normalize_droplet_parameters( test_parameters[:, inputs_env_range] ) ),\n",
    "                    test_parameters[:, inputs_env_range] ):\n",
    "    raise ValueError( \"Scaling normalized inputs (without t_final) was not an inverse operation!\" )\n",
    "\n",
    "if not np.allclose( scale_droplet_parameters( normalize_droplet_parameters( test_parameters[:, inputs_env_time_range] ) ),\n",
    "                    test_parameters[:, inputs_env_time_range] ):\n",
    "    raise ValueError( \"Scaling normalized inputs (with t_final) was not an inverse operation!\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aeda14-3f96-4495-8b15-38d5cbfcdaea",
   "metadata": {},
   "source": [
    "# GeneratingTraining Data\n",
    "We want to create an on-disk training data set to streamline the training process.\n",
    "Not training a model itself, per se, but rather making it easy to quickly\n",
    "explore different models that are comparable without having questions about\n",
    "what data were seen by each model.\n",
    "\n",
    "A secondary benefit to this is that the training process is much faster as\n",
    "we don't have to worry about the ability to quickly generate random data (which\n",
    "requires solving ODEs) and slowing down the actual training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef553c8-1f00-4370-ac2f-5c007381e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate that we can generate input parameters, without t_final, by sampling [-1, 1]\n",
    "# and scaling them to their physical range.\n",
    "#\n",
    "# NOTE: This does not generate random t_sample as the original code was not developed with\n",
    "#       that in mind.  Partially because the distribution is different and partially because\n",
    "#       t_final is not currently normalized on input to the model.  This should be evaluated.\n",
    "#\n",
    "random_inputs = scale_droplet_parameters( np.reshape( np.random.uniform( -1, 1, 30 ), (5, 6) ) )\n",
    "\n",
    "print( random_inputs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c312c0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate generating a batch of droplets with a small number of evaluations.\n",
    "number_droplets    = 128\n",
    "number_evaluations = 4\n",
    "\n",
    "[inputs,\n",
    " outputs,\n",
    " integration_times,\n",
    " weird_inputs,\n",
    " weird_outputs] = create_droplet_batch( number_droplets, number_evaluations=number_evaluations )\n",
    "\n",
    "number_weird_inputs  = sum( map( lambda type_name: len( weird_inputs[type_name] ), weird_inputs ) )\n",
    "number_weird_outputs = sum( map( lambda type_name: len( weird_outputs[type_name] ), weird_outputs ) )\n",
    "\n",
    "print( \"Generated {:d} droplet parameter{:s}.\\n\".format(\n",
    "    number_droplets,\n",
    "    \"\" if number_droplets == 1 else \"s\" ) )\n",
    "\n",
    "if number_weird_inputs > 0:\n",
    "    print( \"{:d} weird input{:s}:\\n\".format(\n",
    "        number_weird_inputs,\n",
    "        \"\" if number_weird_inputs == 1 else \"s\" ) )\n",
    "    \n",
    "    for type_name, weird_things in weird_inputs.items():\n",
    "        number_weird_things = len( weird_things )\n",
    "        if number_weird_things == 0:\n",
    "            continue\n",
    "            \n",
    "        print( \"    {:d} {:s}\".format(\n",
    "            number_weird_things,\n",
    "            type_name ) )\n",
    "        \n",
    "    print( \"\" )\n",
    "\n",
    "if number_weird_outputs > 0:\n",
    "    print( \"{:d} weird output{:s}:\\n\".format(\n",
    "        number_weird_outputs,\n",
    "        \"\" if number_weird_outputs == 1 else \"s\" ) )\n",
    "    \n",
    "    for type_name, weird_things in weird_outputs.items():\n",
    "        number_weird_things = len( weird_things )\n",
    "        if number_weird_things == 0:\n",
    "            continue\n",
    "            \n",
    "        print( \"    {:d} {:s}\".format(\n",
    "            number_weird_things,\n",
    "            type_name ) )\n",
    "        \n",
    "    print( \"\" )\n",
    "    \n",
    "write_weird_parameters_to_spreadsheet( \"test.xlsx\", weird_inputs, weird_outputs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84262077-9a12-487a-bf8d-9f24f36d5a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training file with a handful of droplets, filter out any invalid\n",
    "# parameters (this should no longer happen), and read them back in to\n",
    "# see the distribution of t_final.  \n",
    "training_test_path = \"foo.data\"\n",
    "weird_test_path    = \"foo.xlsx\"\n",
    "create_training_file( training_test_path, 1024, weird_file_name=weird_test_path )\n",
    "\n",
    "number_parameters, number_bad_parameters = clean_training_data( training_test_path )\n",
    "print( \"Removed {:d} parameter{:s} from {:d} parameter{:s} ({:.2f}%).\".format(\n",
    "    number_bad_parameters,\n",
    "    \"\" if number_bad_parameters == 1 else \"s\",\n",
    "    number_parameters,\n",
    "    \"\" if number_parameters == 1 else \"s\",\n",
    "    number_bad_parameters / number_parameters * 100.0 ) )\n",
    "\n",
    "input_parameters, output_parameters, times = read_training_file( training_test_path )\n",
    "\n",
    "fig_h, ax_h = plt.subplots( 1, 1 )\n",
    "ax_h.plot( np.log10( times ), \".\" )\n",
    "ax_h.set_xlabel( \"Droplet #\" )\n",
    "ax_h.set_ylabel( \"log10( $t_{final}$ )\" )\n",
    "ax_h.set_title( \"Distribution of $t_{final}$\" )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24c65f1-9042-4f46-a821-1f52c26df2e9",
   "metadata": {},
   "source": [
    "# Training a Model\n",
    "We define a simple model architecture in PyTorch, setup our loss function and basic\n",
    "hyperparameters, and then randomly walk through the training data one or more times.\n",
    "If configured, the trained model is saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7ed01a-ae54-4af0-9021-37cd242ce3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using a local GPU if we have one, otherwise stay on the CPU.  While\n",
    "# SimpleNet isn't a large model having GPU acceleration for large batch\n",
    "# counts and a non-trivial number of droplet parameters (>100 million) certainly\n",
    "# helps.\n",
    "device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\" )\n",
    "\n",
    "print( \"Training with '{}'.\".format( device ) )\n",
    "\n",
    "# Create an instance of ResidualNet and configure its optimization parameters:\n",
    "#\n",
    "#   - We use L1 loss since the model's outputs are in [-1, 1] making\n",
    "#     \"normal\" errors smaller than 1, and MSE would result in even\n",
    "#     smaller loss terms.\n",
    "#   - We use Adam so we have momentum when performing gradient descent\n",
    "#   - Given that we have a relatively large batch size we use a large\n",
    "#     initial learning rate of 1e-3.\n",
    "#   - We want smaller weights so we use a L2 regularization penalty of 1e-6\n",
    "#     to encourage that (as well as having non-zero weights rather than\n",
    "#     leaning heavily on just a subset of weights)\n",
    "#\n",
    "model     = ResidualNet()\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam( model.parameters(), lr=1e-3, weight_decay=1e-6 )\n",
    "\n",
    "# Move the model to the device we're training with.\n",
    "model = model.to( device )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bf7edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss( model, epoch_number, optimizer, training_loss, validation_loss ):\n",
    "    \"\"\"\n",
    "    Training callback that prints the mean training and validation loss to standard\n",
    "    output.\n",
    "    \n",
    "    Takes 5 arguments:\n",
    "    \n",
    "      model           - Torch model object.\n",
    "      epoch_number    - Epoch number that just completed.\n",
    "      optimizer       - Torch optimizer object.\n",
    "      training_loss   - Sequence of training loss values for the last epoch.\n",
    "      validation_loss - Scalar validation loss for the last epoch.\n",
    "    \n",
    "    Returns nothing.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print( \"Epoch #{:d} loss - T: {:f}, V: {:f}\".format(\n",
    "        epoch_number,\n",
    "        np.mean( training_loss ),\n",
    "        validation_loss ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7b5bdb-791b-4837-ad85-57f9da737370",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if train_model_flag:\n",
    "    if training_data_path is None:\n",
    "        raise ValueError( \"Set training_data_path to where the training data are located!\" )\n",
    "\n",
    "    (training_loss,\n",
    "     validation_loss) = train_model( model, \n",
    "                                     criterion,\n",
    "                                     optimizer,\n",
    "                                     device,\n",
    "                                     number_epochs, \n",
    "                                     training_data_path,\n",
    "                                     validation_file=validation_data_path,\n",
    "                                     epoch_callback=print_loss )\n",
    "\n",
    "    # Determine how many minibatches were seen so we can correctly\n",
    "    # plot the validation loss.\n",
    "    number_mini_batches = len( training_loss ) // number_epochs\n",
    "    \n",
    "    # Plot the training loss to qualitatively assess how the model is doing.\n",
    "    # Loss should consistently decrease over mini-batches.\n",
    "    #\n",
    "    # NOTE: This is only one aspect to evaluate a model's performance as the\n",
    "    #       loss reported is on *training* data and not independent test\n",
    "    #       data.  As a result, this curve will be overly optimistic.\n",
    "    #\n",
    "    fig_h, ax_h = plt.subplots( 1, 1, figsize=(4, 4) )\n",
    "\n",
    "    ax_h.plot( np.log10( training_loss ), \".\", label=\"Training\" )\n",
    "    ax_h.plot( range( number_mini_batches - 1, len( training_loss ), number_mini_batches), np.log10( validation_loss ), \".\", label=\"Validaiton\" )\n",
    "    ax_h.set_xlabel( \"Mini Batch Number\" )\n",
    "    ax_h.set_ylabel( \"log10( loss )\" )\n",
    "    ax_h.set_title( \"Training and Validation Loss - {:d} Epoch{:s}\".format(\n",
    "        number_epochs,\n",
    "        \"\" if number_epochs == 1 else \"s\"\n",
    "    ) )\n",
    "    ax_h.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34f3b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_checkpoint_prefix is None:\n",
    "    warnings.warn( \"Please set model_checkpoint_prefix if you wish to save the model to disk.  This is intentionally *NOT* set to avoid accidentally overwriting an existing model.\" )\n",
    "else:\n",
    "    checkpoint_path = save_model_checkpoint( model_checkpoint_prefix,\n",
    "                                             -1,\n",
    "                                             model,\n",
    "                                             optimizer,\n",
    "                                             criterion,\n",
    "                                             training_loss )\n",
    "    \n",
    "    print( \"Saved the current model to '{:s}'.\".format( checkpoint_path ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9df101-9961-4dec-a890-b2a6257adec8",
   "metadata": {},
   "source": [
    "# Performance Analysis\n",
    "Load a previously trained model, if requested, and qualitatively assess its performance relative\n",
    "to the ODEs it was trained to approximate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a4252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model_flag:\n",
    "    model = ResidualNet()\n",
    "    load_model_checkpoint( model_load_path, model )\n",
    "    model = model.to( device )\n",
    "    \n",
    "    print( \"Loaded a model from '{:s}'.\".format( model_load_path ) )\n",
    "else:\n",
    "    print( \"Using the previously trained model.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcd1538-54d8-42f5-965b-291603ebf417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the model matches the ODEs outputs for a single parameter.\n",
    "test_inputs      = np.array( [9.58937380e-05, 3.03864258e+02, 7.20879248e-16, 2.77617767e+02, 1.04355168e+00, 8.33695471e-01] ).reshape( (1, -1) )\n",
    "test_time        = np.array( [1.0] )\n",
    "\n",
    "t_span   = (0, 10)\n",
    "solution = solve_ivp_float32_outputs( dydt,\n",
    "                                      t_span,\n",
    "                                      [test_inputs[0, 0], test_inputs[0, 1]],\n",
    "                                      method=\"Radau\",\n",
    "                                      t_eval=test_time,\n",
    "                                      args=(test_inputs[0, 2:],) )\n",
    "\n",
    "#\n",
    "# NOTE: We must transpose our expected outputs as solve_ivp() returns vectors\n",
    "#       in a different direction as do_inference().\n",
    "#\n",
    "expected_outputs = np.array( [solution.y[0], solution.y[1]] ).T\n",
    "\n",
    "# What does our model estimate for this set of inputs?\n",
    "test_outputs = do_inference( test_inputs, test_time, model, device )\n",
    "\n",
    "# Sanity check that we're within half a percent relative difference for radius\n",
    "# and temperature.\n",
    "#\n",
    "# NOTE: Changing the model may require an updated tolerance as new models\n",
    "#       may not have learned to approximate this particular input.  Be\n",
    "#       careful when adjusting this tolerance!\n",
    "#\n",
    "# NOTE: We report a warning instead of raising an exception since training\n",
    "#       an underperforming model shouldn't preclude the analysis and weights\n",
    "#       export process.\n",
    "#\n",
    "if not np.allclose( test_outputs, expected_outputs, rtol=5e-3 ):\n",
    "    warnings.warn( \"The model did not compute the expected size/temperature ({}) for {} ({})\".format(\n",
    "        expected_outputs,\n",
    "        test_inputs,\n",
    "        test_outputs ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ef8cbb-50ab-4133-88ef-4460d4a93f9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_parameters = scale_droplet_parameters( np.random.uniform( low=-1.0, high=1.0, size=(1, 6) ).astype( \"float32\" ) )\n",
    "analyze_model_performance( model, input_parameters )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad20ea",
   "metadata": {},
   "source": [
    "# Serializing Model Weights into Fortran\n",
    "Generate a simple export of the model's weight into a format that is usable for a naive implementation of inference with SimpleNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42311d19",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Overwrite the module used by NTLP so future simulations use the current weights.\n",
    "if write_weights_flag:\n",
    "    \n",
    "    # Pick the most descriptive name we have so researchers have an idea where\n",
    "    # the weights came from.\n",
    "    if load_model_flag:\n",
    "        exported_model_name = model_load_path.split( \"/\" )[-1]\n",
    "    else:\n",
    "        exported_model_name = model_name\n",
    "    \n",
    "    exported_file_path = \"../../droplet_model.f90\"\n",
    "    \n",
    "    generate_fortran_module( exported_file_path,\n",
    "                             exported_model_name,\n",
    "                             model.state_dict() )\n",
    "    \n",
    "    print( \"Wrote model weights and inferencing code to '{:s}'.\".format(\n",
    "        exported_file_path ) )\n",
    "else:\n",
    "    print( \"Skipped writing model weights.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c80deb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
