{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b5a1ae8",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook provides the ability to generate random droplet parameters, write them to disk, and \n",
    "train a neural network with said droplet parameters to approximate the underlying ODEs that govern \n",
    "the droplet parameters.  Once trained researchers can generate a Fortran 90 module that provides \n",
    "the ability to estimate droplet radius and temperature for some time in the future.  \n",
    "\n",
    "The intent is that a small, reasonably trained neural network can provide accurate enough droplet \n",
    "characteristic estimations that are significantly faster than an iterative Gauss-Newton technique.\n",
    "Initial testing indicates a small 4-layer network (roughly 2400 parameters) with Fortran 90 module\n",
    "generated by this notebook is 30-90x faster than the existing (as of 2024/09/25) iterative approach \n",
    "which results in roughly a factor of 2x overall simulation speedup.\n",
    "\n",
    "This notebook is broken down into the following sections:\n",
    "\n",
    "1. ODEs of interest\n",
    "2. Mapping data to/from $[-1, 1]$\n",
    "3. Generating random droplets\n",
    "4. Training a neural network\n",
    "5. Analyzing a network's performance\n",
    "6. Exporting a network to Fortran 90\n",
    "\n",
    "\n",
    "# Setup\n",
    "Since training a neural network to approximate ODEs has multiple workflows (e.g. training a model\n",
    "vs loading previous models to analyze their performance) there are several variables that need to\n",
    "be set to exercise all of this notebook's functionality.  In particular, the following should\n",
    "be reviewed and set depending on which workflows are of interest:\n",
    "\n",
    "- Loading a previously trained model: `load_model_flag`, `model_load_path`\n",
    "- Training a new model: `train_model_flag`, `training_data_path`, `number_epochs`\n",
    "- Saving a newly trained model: `save_model_path`\n",
    "\n",
    "These should be set at the top of the notebook.  Please adhere to the modification instructions\n",
    "where the variables are defined.\n",
    "\n",
    "## Training Data\n",
    "Training data is not included in the repository due to size, though creating data on the fly\n",
    "using `create_training_file()` is slow since it is single threaded and this notebook does\n",
    "not implement a parallel data generation process.\n",
    "\n",
    "Those who have access to Dr. Richter's group network drive can download previously created \n",
    "data sets.  Users external to the group can either slowly create data using the tools in this \n",
    "notebook or use scripts exported from an earlier version of this notebook \n",
    "(`generate_training_data.py` and `loop-training-data.sh`) to generate data in bulk on\n",
    "multiple cores.\n",
    "\n",
    "## Python Dependencies\n",
    "The following Python packages are needed to exercise full functionality in this notebook \n",
    "(versions tested in parentheses):\n",
    "\n",
    "- Python3 (3.11.5)\n",
    "- Matplotlib (3.7.1)\n",
    "- NumPy (1.24.3)\n",
    "- Pandas (1.5.3)\n",
    "- PyTorch (2.3.0)\n",
    "\n",
    "There is no fundamental dependence on any particular version of the dependencies and, barring\n",
    "any bugs encountered with the packages' main APIs, newer versions are expected to work.\n",
    "\n",
    "# Notebook Care and Feeding\n",
    "Notebooks are great for exploring ideas and rapidly iterating to a solution.  They are less\n",
    "great for maintaining a \"production\" workflow that needs to be used by multiple people\n",
    "on a semi-regular basis.  As such, the following guidelines should be followed when making\n",
    "updates to this so as to preserve everyone's sanity:\n",
    "\n",
    "- Do not commit the notebook with outputs from a previous run.  This greatly reduces the\n",
    "  file size in the repository and avoids unnecessary changes when someone re-runs a cell\n",
    "  whose output changes on each execution.\n",
    "- The notebook should always allow execution of all cells without errors.  Restarting the\n",
    "  kernel and running all cells should be run to confirm this (then restart and clear output).\n",
    "\n",
    "# Future Work\n",
    "\n",
    "## Hyperparameter Search on the Neural Network's Architecture\n",
    "The network architecture and hyperparameters were chosen because they:\n",
    "\n",
    "1. Resulted in a small network\n",
    "2. Had sizes that *should be* efficient to work with on the CPU\n",
    "\n",
    "A very limited hyperparameter search has settled on parameters that appear to reliably train\n",
    "performant networks though the path from start to where we're at was very ad hoc.  Fiddling\n",
    "with hyperparameters stopped as soon as a network that was reasonably accurate (and was\n",
    "fairly reproducible) and fast enough when implemented in Fortran. \n",
    "\n",
    "A more thorough exploration of the following could be done in an attempt to generate a more accurate\n",
    "neural network, or a smaller (faster) network so more particles could be simulated without impacting\n",
    "simulation run-times.\n",
    "\n",
    "Areas to explore include (roughly in priority order):\n",
    "\n",
    "1. Number and size of MLP-layers\n",
    "2. Learning rate and schedule\n",
    "3. Weights regularization (e.g. L1 or L2 penalties)\n",
    "\n",
    "## Improving Neural Network Performance\n",
    "There are a handful of non-architecture/hyperparameter-related things to explore to improve\n",
    "performance.  Unless otherwise specified, these are all speculations on Greg's part and aren't\n",
    "a sure bet.\n",
    "\n",
    "### Train on Sequences of Integration Times for the Same Parameter\n",
    "Currently training data \n",
    "\n",
    "### Normalize the Neural Networks's Integration Time Parameter\n",
    "Currently all of the neural network's inputs, except for integration time, are normalized into\n",
    "the range $[-1, 1]$.  Integration time remains in the range of $(0, 10)$ with a focus on $[10^{-3}, 10]$\n",
    "so as to cover both DNS and LES simulations.  The decision to leave integration time unnormalized\n",
    "was by accident rather than a conscious choice.\n",
    "\n",
    "That said, it is unknown as to whether this negatively impacts the performance of the model.  Since\n",
    "DNS simulations focus on smaller time steps (in the range of $[10^{-3}, 10^{-1})$) and LES simulations on \n",
    "larger (in the range of $(10^{-1}, 10)$) it may be worthwhile to perform the same log-scale normalization\n",
    "to the integration time as is done for the droplet's radius and salinity.  The thought is that the\n",
    "network would be less sensitive to the scale separation between DNS and LES time steps and learn\n",
    "them equally.  That said, no quantitative analysis has been performed that would suggest this - it is\n",
    "purely an unfounded hypothesis at this point.\n",
    "\n",
    "## Factor This Notebook into a Python Module\n",
    "This notebook was developed in pieces as the concept of using a neural network was explored and\n",
    "evaluated for viability.  At the time of initial development (September 2024) having a single\n",
    "model that is accurate enough is sufficient to update NTLP and move on to other things, so factoring\n",
    "this notebook was not a priority during the implementation effort.  Given that the models trained\n",
    "are approximating a fixed set of ODEs, the data distribution they need to learn is essentially\n",
    "forever fixed which greatly reduces the need to factor this out to enable more streamlined workflows.\n",
    "\n",
    "Said differently, since training a new model will be rather rare, this notebook is an acceptable\n",
    "vehicle for repeating the current process with slight tweaks (e.g. different ODEs, modified \n",
    "parameter normalization techniques, different parameter counts).  Factoring a Python module would\n",
    "make it easy to automate certain activities (e.g. data generation and large-scale quantitative model\n",
    "evaluation) but then requires managing the installation process so it's available to both notebooks\n",
    "and other scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772aafd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.integrate import solve_ivp\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae14a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# NOTE: Do not change this cell directly!  Add a cell below and set new values for the\n",
    "#       variables of interest.  This makes it significantly easier to revert your changes\n",
    "#       when commiting changes to the repository - just delete the next cell.\n",
    "#\n",
    "\n",
    "# Change this to False if you've previously trained a model and want to to evaluate\n",
    "# its performance rather than a pre-trained model from disk\n",
    "load_model_flag = True\n",
    "\n",
    "# Path to the model, relative to this notebook, to load for performance analysis.\n",
    "model_load_path = \"../models/mlp_4layer-b=1024-lr=1e-3_halvingschedule-l2reg=1e-6.pth\"\n",
    "\n",
    "# Change this to True if you want to train a model from scratch (loading and continuing\n",
    "# training is not supported yet).  Since generating sufficient training data is typically\n",
    "# longer than most researchers are willing to wait (O(1 hour) minimum with multiple cores)\n",
    "# the training_data_path variable must also be set.\n",
    "train_model_flag = False\n",
    "\n",
    "# Path to previously created droplet parameters.\n",
    "#\n",
    "# NOTE: You must update this to where you've created/copied the training data!\n",
    "#\n",
    "# training_data_path = \"../data/time_log_spaced.data\"\n",
    "training_data_path = None\n",
    "\n",
    "# Number of times the model should see the entirety of the training data\n",
    "# before stopping the optimization process.\n",
    "number_epochs      = 10\n",
    "\n",
    "# Name of the currently trained model.  This is used when writing the model's weights\n",
    "# as a Fortran module.  Default to the name of the loaded model, sans extension, as\n",
    "# that should match the default training configuration that generated it.\n",
    "model_name = model_load_path.split( \"/\" )[-1].split( \".\" )[0]\n",
    "\n",
    "# Path to the model, relative to this notebook, to save newly trained models.\n",
    "#\n",
    "# NOTE: We disable saving by default to avoid accidentally overwriting an existing model!\n",
    "#\n",
    "#model_save_path = \"../models/mlp_4layer-b=1024-lr=1e-3_halvingschedule-l2reg=1e-6.pth\"\n",
    "model_save_path = None\n",
    "\n",
    "# Change this if the droplet_model.f90 module should be created.  This will be\n",
    "# generated from either 1) a newly trained model or 2) a previously trained, loaded\n",
    "# model, in that order.\n",
    "write_weights_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e986a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training configuration.  Uncomment the variable assignments below to use.\n",
    "# This trains a file from data in the ../data/ directory and saves the weights in\n",
    "# the ../models/ directory.  It does not write out a new droplet_model.f90 module.\n",
    "\n",
    "#train_model_flag   = True\n",
    "#training_data_path = \"../data/time_log_spaced-combined.data\"\n",
    "#model_save_path    = \"../models/NEW-mlp_4layer-b=1024-lr=1e-3_halvingschedule-l2reg=1e-6.pth\"\n",
    "#model_name         = model_save_path.split( \"/\" )[-1].split( \".\" )[0]\n",
    "#model_load_path    = model_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599c1885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force NumPy to print more values on each line rather than wrapping\n",
    "# around 80 characters.\n",
    "np.set_printoptions( linewidth=120 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cbd2ee-fa2f-4f9a-a36d-b3f74055986f",
   "metadata": {},
   "source": [
    "# Differential Equations of Interest\n",
    "Example code that solves for a droplet's radius and temperature given its current radius and temperature\n",
    "as well as the environment it is in, described by:\n",
    "\n",
    "- The droplet's salinity\n",
    "- Air temperature\n",
    "- Relative humidity\n",
    "- $\\rho_a$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eba98e5-d8d9-4b98-92c8-650be76a1920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dydt( t, y, parameters ):\n",
    "    \"\"\"\n",
    "    Differential equations governing a water droplet's radius and temperature as\n",
    "    a function of time, given the specified physical parameters.\n",
    "    \n",
    "    Adapted from code provided by David Richter (droplet_integrator.py) in August 2024.\n",
    "    Changes made include:\n",
    "    \n",
    "      - Accepting NumPy arrays for the parameters instead of having hard-coded values.\n",
    "      \n",
    "        NOTE: While it allows for multiple parameters to be specified via additional rows\n",
    "              this does not necessarily work with scipy.solve_ivp()!\n",
    "              \n",
    "      - Parameters are promoted to 64-bit floating point values so all internal\n",
    "        calculations are performed at maximum precision, resulting in 64-bit outputs.\n",
    "        It is the caller's responsibility for casting the results to a different precision.\n",
    "    \n",
    "    Takes 3 arguments:\n",
    "    \n",
    "      t          - Unused argument.  Required for the use of scipy.solve_ivp().\n",
    "      y          - NumPy array of droplet radii and temperatures.  May be specified\n",
    "                   as either a 1D vector (of length 2), or a 2D array (sized\n",
    "                   number_droplets x 2) though must be shape compatible with the\n",
    "                   parameters array.\n",
    "      parameters - NumPy array of droplet parameters containing salinity, air temperature,\n",
    "                   relative humidity, and rhoa.  May be specified as either a\n",
    "                   1D vector (of length 2), or a 2D array (sized number_droplets x 2)\n",
    "                   though must be shape compatible with the y array.\n",
    "    \n",
    "    Returns 2 values:\n",
    "    \n",
    "      dradius_dt      - The derivative of the droplet's radius with respect to time, shaped\n",
    "                        number_droplets x 1.\n",
    "      dtemperature_dt - The derivative of the droplet's temperature with respect to time,\n",
    "                        shaped number_droplets x 1.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #\n",
    "    # NOTE: We work in 64-bit precision regardless of the input\n",
    "    #       so we get an as accurate as possible answer.\n",
    "    #\n",
    "    \n",
    "    m_s  = parameters[..., 0].astype( \"float64\" )\n",
    "    Tf   = parameters[..., 1].astype( \"float64\" )\n",
    "    RH   = parameters[..., 2].astype( \"float64\" )\n",
    "    rhoa = parameters[..., 3].astype( \"float64\" )\n",
    "    \n",
    "    rhow = np.float64( 1000 )\n",
    "    Cpp  = np.float64( 4190 )\n",
    "    Mw   = np.float64( 0.018015 )\n",
    "    Ru   = np.float64( 8.3144 )\n",
    "    Ms   = np.float64( 0.05844 )\n",
    "    Gam  = np.float64( 7.28e-2 )\n",
    "    Ion  = np.float64( 2.0 )\n",
    "    Os   = np.float64( 1.093 )\n",
    "    Shp  = np.float64( 2 )\n",
    "    Sc   = np.float64( 0.61 )\n",
    "    Pra  = np.float64( 0.71 )\n",
    "    Cpa  = np.float64( 1006.0 )\n",
    "    nuf  = np.float64( 1.57e-5 )\n",
    "    Lv   = np.float64( (25.0 - 0.02274*26)*10**5 )\n",
    "    Nup  = np.float64( 2 )\n",
    "\n",
    "    einf = 611.2*np.exp(17.67*(Tf-273.15)/(Tf-29.65))\n",
    "\n",
    "    qinf = 0.75/rhoa*(einf*Mw/Ru/Tf)\n",
    "\n",
    "\n",
    "    qinf = RH/rhoa*(einf*Mw/Ru/Tf);\n",
    "\n",
    "    Volp = 4/3*np.pi*y[0]**3\n",
    "    rhop = (m_s + Volp*rhow)/Volp\n",
    "    taup = rhop*(2*y[0])**2/18/nuf/rhoa\n",
    "\n",
    "\n",
    "    qstar = einf*Mw/Ru/y[1]/rhoa*np.exp(Lv*Mw/Ru*(1/Tf - 1/y[1]) + 2*Mw*Gam/Ru/rhow/y[0]/y[1] - Ion*Os*m_s*(Mw/Ms)/(Volp*rhow))\n",
    "\n",
    "    dy1dt = 1/9*Shp/Sc*rhop/rhow*y[0]/taup*(qinf - qstar)\n",
    "    dy2dt = -1/3*Nup/Pra*Cpa/Cpp*rhop/rhow/taup*(y[1] - Tf) + 3*Lv/y[0]/Cpp*dy1dt\n",
    "\n",
    "    return [dy1dt, dy2dt]\n",
    "\n",
    "def solve_ivp_float32_outputs( dydt, t_span, y0, **kwargs ):\n",
    "    \"\"\"\n",
    "    Solves an initial value problem and returns the solutions in 32-bit precision.\n",
    "    \n",
    "    NOTE: This is a wrapper around scipy.integrate.solve_ivp(), so it takes the\n",
    "          same arguments and returns the same values.  See that function's\n",
    "          help for a (way more) detailed explanation of each argument and value.\n",
    "    \n",
    "    Takes 4 arguments:\n",
    "    \n",
    "      dydt   - Right-hand side of the system to solve.  The calling signature\n",
    "               is 'dydt( t, y, parameters )'.\n",
    "      t_span - 2-member sequence specifying the interval of integration.  dydt\n",
    "               is integrated from t_span[0] until t_span[1].\n",
    "      y0     - Initial state of the system to solve.  Must be compatible with\n",
    "               dydt.\n",
    "      kwargs - Optional keyword arguments to supply to solve_ivp().\n",
    "    \n",
    "    Returns 1 value:\n",
    "    \n",
    "      solution - Object containing fields related to the integration process,\n",
    "                 including:\n",
    "                 \n",
    "                   t - Vector of times, of length number_times, where dydt was\n",
    "                       evaluated.\n",
    "                   y - Array of solutions, shaped number_variables x numbe_times,\n",
    "                       where dydt was evaluated.\n",
    "                       \n",
    "                 Solutions will be returned according to the evaluation window,\n",
    "                 t_eval, either supplied via kwargs or selected as a default from\n",
    "                 t_span.\n",
    "      \n",
    "    \"\"\"\n",
    "\n",
    "    # Solve the ODE in the precision supplied by the caller.\n",
    "    solution = solve_ivp( dydt, t_span, y0, **kwargs )\n",
    "\n",
    "    # Return the outputs as the requested precision.\n",
    "    solution.t = solution.t.astype( \"float32\" )\n",
    "    solution.y = solution.y.astype( \"float32\" )\n",
    "\n",
    "    return solution\n",
    "\n",
    "def plot_droplet_size_temperature( size_temperatures, times ):\n",
    "    \"\"\"\n",
    "    Plots a droplet's size and temperature as a function of time.  \n",
    "    \n",
    "    Takes 2 arguments:\n",
    "    \n",
    "      size_temperatures - NumPy array, sized 2 x number_times, containing\n",
    "                          droplet radii and temperatures in the first and second\n",
    "                          columns, respectively.\n",
    "      times             - NumPy vector, of length number_times, containing the\n",
    "                          times corresponding to each entry in size_temperatures.\n",
    "      \n",
    "    Returns 2 values:\n",
    "    \n",
    "      fig_h - Figure handle created.\n",
    "      ax_h  - Sequence of two axes handles, one for the size plot and another\n",
    "              for the temperature plot.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    fig_h, ax_h = plt.subplots( 1, 2, figsize=(9, 4) )\n",
    "\n",
    "    fig_h.suptitle( \"Droplet Size and Temperature (Truth)\" )\n",
    "\n",
    "    ax_h[0].plot( times, size_temperatures[0, :], label=\"radius\" )\n",
    "    ax_h[0].set_xlabel( \"Time (s)\" )\n",
    "    ax_h[0].set_ylabel( \"Radius (m)\" )\n",
    "    ax_h[0].set_xscale( \"log\" )\n",
    "    ax_h[0].set_yscale( \"log\" )\n",
    "    \n",
    "    ax_h[1].plot( times, size_temperatures[1, :], label=\"temp\" )\n",
    "    ax_h[1].set_xlabel( \"Time (s)\" )\n",
    "    ax_h[1].set_ylabel( \"Temperature (K)\" )\n",
    "    ax_h[1].set_xscale( \"log\" )\n",
    "    \n",
    "    return fig_h, ax_h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901fbcc8",
   "metadata": {},
   "source": [
    "Solve the ODEs for a set of parameters in the middle of each of the valid ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260959ff-4251-4b1a-8b74-00693462f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial values for the droplet.\n",
    "#\n",
    "# NOTE: We must use a list for the initial values rather than a NumPy array\n",
    "#       as an array will somehow cause a divide by zero.  I did not have\n",
    "#       enough time to figure out why this is the case.\n",
    "#\n",
    "y0 = [0, 0]\n",
    "y0[0] = np.float32( 1e-4 )  # Radius in logspace(-8,-3)\n",
    "y0[1] = np.float32( 293 )   # Temperature in (273,310)\n",
    "\n",
    "# Environmental parameters to solve the ODEs with.\n",
    "#\n",
    "# Salinity (m_s)         ~ logspace(-22, -10)\n",
    "# Air temperature (Tf)   ~ linspace(273, 310)\n",
    "# Relative humidity (RH) ~ linspace(0.65, 1.1)\n",
    "# rhoa                   ~ linspace(0.8, 1.2)\n",
    "parameters = np.array( [1e-18, 291.0, 0.7, 1.0] )\n",
    "\n",
    "# The ODEs are valid over t_span and solutions are returned over t_eval.\n",
    "t_span = (0, 10)\n",
    "t_eval = np.linspace( 0, 10, 1000 )\n",
    "\n",
    "#\n",
    "# NOTE: vectorized=True doesn't allow calling with arrays, but it lets the ODE solver\n",
    "#       operate on multiple values at once if it chooses to.\n",
    "#\n",
    "solution = solve_ivp_float32_outputs( dydt, t_span, y0, method=\"Radau\", t_eval=t_eval, args=(parameters,) )\n",
    "\n",
    "plot_droplet_size_temperature( solution.y, solution.t )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac663ff",
   "metadata": {},
   "source": [
    "# Mapping Droplet Parameters to/from $[-1, 1]$\n",
    "Create routines for moving between the normal range of physical parameters and\n",
    "a mapping on the range [-1, 1].  The ODEs are solved with physical parameters,\n",
    "each with their own dynamic range, while the model operates on parameters that\n",
    "each have the same range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e70b36-1058-4c37-ab71-d2815f31a808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the range of each of the input parameters so we can normalize\n",
    "# each to [-1, 1], allowing the models to weight each equally.  If we don't\n",
    "# perform this mapping then optimization process will effectively ignore\n",
    "# the small parameters (e.g. radius) as they won't contribute as much as\n",
    "# the big parameters (e.g. temperature).  As a result, we use log-scale\n",
    "# for the parameters that have a large dynamic range.\n",
    "DROPLET_RADIUS_LOG_RANGE          = np.array( (-8, -3) )\n",
    "DROPLET_TEMPERATURE_RANGE         = np.array( (273, 310) )\n",
    "DROPLET_SALINITY_LOG_RANGE        = np.array( (-22, -10) )\n",
    "DROPLET_AIR_TEMPERATURE_RANGE     = np.array( (273, 310) )\n",
    "DROPLET_RELATIVE_HUMIDITY_RANGE   = np.array( (0.65, 1.1) )\n",
    "DROPLET_RHOA_RANGE                = np.array( (0.8, 1.2) )\n",
    "\n",
    "def normalize_droplet_parameters( droplet_parameters ):\n",
    "    \"\"\"\n",
    "    Normalizes an array of droplet parameters into the range [-1, 1].  Operates\n",
    "    on both input and parameters.\n",
    "    \n",
    "    Takes 1 argument:\n",
    "\n",
    "      droplet_parameters - NumPy array of parameters to normalize, shaped \n",
    "                           number_droplets x number_parameters.  May either be\n",
    "                           1D, for a single droplet, or 2D for multiple droplets.\n",
    "                           number_parameters must be either 2 (output parameters),\n",
    "                           6 (input parameters without t_final), or 7 (input\n",
    "                           parameters with t_final).\n",
    "\n",
    "    Returns 1 value:\n",
    "    \n",
    "      normalized_droplet_parameters - NumPy array of normalized parameters, shaped\n",
    "                                      number_droplets x number_parameters.  Has the\n",
    "                                      same shape as droplet_parameters.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #\n",
    "    # NOTE: We take care to handle arbitrary rank arrays!  While we expect either \n",
    "    #       rank-1 or rank-2, the code is written to handle larger ranks naturally\n",
    "    #       with the inner dimension representing a single droplet's parameters.\n",
    "    #\n",
    "    \n",
    "    number_parameters = droplet_parameters.shape[-1]\n",
    "\n",
    "    # Bail if we didn't get 1) output parameters, 2) input parameters without a t_final,\n",
    "    # or 3) input parameters with a t_final.\n",
    "    if number_parameters != 2 and number_parameters != 6 and number_parameters != 7:\n",
    "        raise ValueError( \"Unknown number of parameters to normalize ({:d})!\".format( number_parameters ) )\n",
    "\n",
    "    normalized_droplet_parameters = np.empty_like( droplet_parameters )\n",
    "    \n",
    "    # We always have radius and temperature.\n",
    "    normalized_droplet_parameters[..., 0] = (np.log10( droplet_parameters[..., 0] ) - np.mean( DROPLET_RADIUS_LOG_RANGE )) / (np.diff( DROPLET_RADIUS_LOG_RANGE ) / 2)\n",
    "    normalized_droplet_parameters[..., 1] = (droplet_parameters[..., 1] - np.mean( DROPLET_TEMPERATURE_RANGE )) / (np.diff( DROPLET_TEMPERATURE_RANGE ) / 2)\n",
    "\n",
    "    # Sometimes we have the remaining parameters.\n",
    "    if number_parameters > 2:\n",
    "        normalized_droplet_parameters[..., 2] = (np.log10( droplet_parameters[..., 2] ) - np.mean( DROPLET_SALINITY_LOG_RANGE )) / (np.diff( DROPLET_SALINITY_LOG_RANGE ) / 2)\n",
    "        normalized_droplet_parameters[..., 3] = (droplet_parameters[..., 3] - np.mean( DROPLET_AIR_TEMPERATURE_RANGE )) / (np.diff( DROPLET_AIR_TEMPERATURE_RANGE ) / 2)\n",
    "        normalized_droplet_parameters[..., 4] = (droplet_parameters[..., 4] - np.mean( DROPLET_RELATIVE_HUMIDITY_RANGE )) / (np.diff( DROPLET_RELATIVE_HUMIDITY_RANGE ) / 2)\n",
    "        normalized_droplet_parameters[..., 5] = (droplet_parameters[..., 5] - np.mean( DROPLET_RHOA_RANGE )) / (np.diff( DROPLET_RHOA_RANGE ) / 2)\n",
    "\n",
    "        # Copy the evaluation times when they're present.\n",
    "        if number_parameters > 6:\n",
    "            normalized_droplet_parameters[..., 6] = droplet_parameters[..., 6]\n",
    "\n",
    "    return normalized_droplet_parameters\n",
    "\n",
    "def scale_droplet_parameters( droplet_parameters ):\n",
    "    \"\"\"\n",
    "    Scales an array of droplet parameters from the range [-1, 1] to their\n",
    "    expected ranges of physical values.  Operates on both input and parameters.\n",
    "    \n",
    "    Takes 1 argument:\n",
    "\n",
    "      droplet_parameters - NumPy array of parameters to scale, shaped \n",
    "                           number_droplets x number_parameters.  May either be\n",
    "                           1D, for a single droplet, or 2D for multiple droplets.\n",
    "                           number_parameters must be either 2 (output parameters),\n",
    "                           6 (input parameters without t_final), or 7 (input\n",
    "                           parameters with t_final).\n",
    "\n",
    "    Returns 1 value:\n",
    "    \n",
    "      scaled_droplet_parameters - NumPy array of scaled parameters, shaped\n",
    "                                  number_droplets x number_parameters.  Has the\n",
    "                                  same shape as droplet_parameters.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #\n",
    "    # NOTE: We take care to handle arbitrary rank arrays!  While we expect either \n",
    "    #       rank-1 or rank-2, the code is written to handle larger ranks naturally\n",
    "    #       with the inner dimension representing a single droplet's parameters.\n",
    "    #\n",
    "\n",
    "    number_parameters = droplet_parameters.shape[-1]\n",
    "\n",
    "    # Bail if we didn't get 1) output parameters, 2) input parameters without a t_final,\n",
    "    # or 3) input parameters with a t_final.\n",
    "    if number_parameters != 2 and number_parameters != 6 and number_parameters != 7:\n",
    "        raise ValueError( \"Unknown number of parameters to scale ({:d})!\".format( number_parameters ) )\n",
    "\n",
    "    scaled_droplet_parameters = np.empty_like( droplet_parameters )\n",
    "    \n",
    "    # We always have radius and temperature.\n",
    "    scaled_droplet_parameters[..., 0] = 10.0 ** (droplet_parameters[..., 0] * (np.diff( DROPLET_RADIUS_LOG_RANGE ) / 2) + np.mean( DROPLET_RADIUS_LOG_RANGE ))\n",
    "    scaled_droplet_parameters[..., 1] = droplet_parameters[..., 1] * (np.diff( DROPLET_TEMPERATURE_RANGE ) / 2) + np.mean( DROPLET_TEMPERATURE_RANGE ) \n",
    "\n",
    "    # Sometimes we have the remaining parameters.\n",
    "    if number_parameters > 2:\n",
    "        scaled_droplet_parameters[..., 2] = 10.0 ** (droplet_parameters[..., 2] * (np.diff( DROPLET_SALINITY_LOG_RANGE ) / 2) + np.mean( DROPLET_SALINITY_LOG_RANGE ))\n",
    "        scaled_droplet_parameters[..., 3] = droplet_parameters[..., 3] * (np.diff( DROPLET_AIR_TEMPERATURE_RANGE ) / 2) + np.mean( DROPLET_AIR_TEMPERATURE_RANGE )\n",
    "        scaled_droplet_parameters[..., 4] = droplet_parameters[..., 4] * (np.diff( DROPLET_RELATIVE_HUMIDITY_RANGE ) / 2) + np.mean( DROPLET_RELATIVE_HUMIDITY_RANGE ) \n",
    "        scaled_droplet_parameters[..., 5] = droplet_parameters[..., 5] * (np.diff( DROPLET_RHOA_RANGE ) / 2) + np.mean( DROPLET_RHOA_RANGE )\n",
    "\n",
    "        # Copy the evaluation times when they're present.\n",
    "        if number_parameters > 6:\n",
    "            scaled_droplet_parameters[..., 6] = droplet_parameters[..., 6]\n",
    "        \n",
    "    return scaled_droplet_parameters\n",
    "\n",
    "# Test normalizing output parameters that are...\n",
    "#\n",
    "#   1. at the lower edge of the valid input space\n",
    "#   2. in the middle of the valid input space\n",
    "#   3. at the upper edge of the valid input space\n",
    "#   4. below the lower edge of the valid input space\n",
    "#   5. above the upper edge of the valid input space\n",
    "#\n",
    "test_parameters = np.array( [[10**-8, 273],\n",
    "                             [10**-5.5, 291.5],\n",
    "                             [10**-3, 310],\n",
    "                             [10**-9, 250],\n",
    "                             [10**-2, 320]] )\n",
    "\n",
    "if not np.allclose( scale_droplet_parameters( normalize_droplet_parameters( test_parameters ) ),\n",
    "                    test_parameters ):\n",
    "    raise ValueError( \"Scaling normalized outputs was not an inverse operation!\" )\n",
    "\n",
    "# Test normalizing input parameters without a t_final.  The parameters chosen check\n",
    "# the same cases as the output tests above.\n",
    "test_parameters_full = np.array( [[10**-8,   273,   10**-22, 273,   0.65,  0.8],\n",
    "                                  [10**-5.5, 291.5, 10**-16, 291.5, 0.875, 1.0],\n",
    "                                  [10**-3,   310,   10**-10, 310,   1.1,   1.2],\n",
    "                                  [10**-9,   250,   10**-23, 250,   0.5,   0.7],\n",
    "                                  [10**-2,   320,    10**-9, 320,   1.2,   1.3]] )\n",
    "\n",
    "if not np.allclose( scale_droplet_parameters( normalize_droplet_parameters( test_parameters_full ) ),\n",
    "                    test_parameters_full ):\n",
    "    raise ValueError( \"Scaling normalized inputs (without t_final) was not an inverse operation!\" )\n",
    "    \n",
    "# Test normalizing input parameters with a t_final.  The parameters chosen check\n",
    "# the same cases as the output tests above.\n",
    "test_parameters_full = np.array( [[10**-8,   273,   10**-22, 273,   0.65,  0.8, 10**-3],\n",
    "                                  [10**-5.5, 291.5, 10**-16, 291.5, 0.875, 1.0, 10**-2],\n",
    "                                  [10**-3,   310,   10**-10, 310,   1.1,   1.2, 10**1],\n",
    "                                  [10**-9,   250,   10**-23, 250,   0.5,   0.7, 10**-3.2],\n",
    "                                  [10**-2,   320,    10**-9, 320,   1.2,   1.3, 10**1.1]] )\n",
    "\n",
    "if not np.allclose( scale_droplet_parameters( normalize_droplet_parameters( test_parameters_full ) ),\n",
    "                    test_parameters_full ):\n",
    "    raise ValueError( \"Scaling normalized inputs (with t_final) was not an inverse operation!\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aeda14-3f96-4495-8b15-38d5cbfcdaea",
   "metadata": {},
   "source": [
    "# GeneratingTraining Data\n",
    "We want to create an on-disk training data set to streamline the training process.\n",
    "Not training a model itself, per se, but rather making it easy to quickly\n",
    "explore different models that are comparable without having questions about\n",
    "what data were seen by each model.\n",
    "\n",
    "A secondary benefit to this is that the training process is much faster as\n",
    "we don't have to worry about the ability to quickly generate random data (which\n",
    "requires solving ODEs) and slowing down the actual training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef553c8-1f00-4370-ac2f-5c007381e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate that we can generate input parameters, without t_final, by sampling [-1, 1]\n",
    "# and scaling them to their physical range.\n",
    "#\n",
    "# NOTE: This does not generate random t_sample as the original code was not developed with\n",
    "#       that in mind.  Partially because the distribution is different and partially because\n",
    "#       t_final is not currently normalized on input to the model.  This should be evaluated.\n",
    "#\n",
    "random_inputs = scale_droplet_parameters( np.reshape( np.random.uniform( -1, 1, 30 ), (5, 6) ) )\n",
    "\n",
    "print( random_inputs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eae00d8-d7db-4234-a745-31b234072d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_droplet_batch( number_droplets, linear_time_flag=False, number_evaluations=1 ):\n",
    "    \"\"\"\n",
    "    Creates a batch of random droplets' input parameters, with t_final.  t_final is sampled\n",
    "    from a slightly larger distribution than the anticipated use cases (spanning [1e-3, 1e1])\n",
    "    so as to increase the performance at the edges of the range.\n",
    "    \n",
    "    Weird parameters encountered, both as inputs to the ODEs and the outputs returned, are\n",
    "    logged and replaced with valid inputs and outputs to ensure the batch is suitable\n",
    "    for training.  Logging returns lists of parameters categorized by the reason they were\n",
    "    filtered out.\n",
    "    \n",
    "    The following categories exist for weird input parameters:\n",
    "    \n",
    "      evaluation_warning - The parameters generated a floating point exception during\n",
    "                           evaluation of the ODEs\n",
    "      failed_solve       - The ODEs failed to converge as a small enough timestep could\n",
    "                           not be identified\n",
    "      invalid_inputs     - XXX: Greg can't remember what triggered this and scipy.solve_ivp()\n",
    "                                can raise ValueError in a number of ways\n",
    "\n",
    "    The following categories exist for weird output parameters:\n",
    "    \n",
    "      radius_nonpositive      - The ODEs generated a physically impossible, negative radius\n",
    "      radius_too_small        - The ODEs generated a radius smaller than the expected range\n",
    "      radius_too_large        - The ODEs generated a radius larger than the expected range\n",
    "      temperature_nonpositive - The ODEs generated a physically impossible, negative temperature\n",
    "      temperature_too_small   - The ODEs generated a temperature smaller than the expected range\n",
    "      temperature_too_large   - The ODEs generated a temperature larger than the expected range\n",
    "\n",
    "    NOTE: The use of linear_time_flag==True leads to a poorly sampled t_final parameter\n",
    "          that results in poor model performance for DNS time scales (i.e. t_final in \n",
    "          [1e-3, 1e-1]) unless trained on a *lot* of droplets.  The default leads to\n",
    "          decent performance on both DNS time scales and LES time scales (i.e. t_final\n",
    "          in [1e-1, 1e1]).\n",
    "\n",
    "    Takes 3 arguments:\n",
    "\n",
    "      number_droplets    - Number of droplets to generate parameters for.\n",
    "      linear_time_flag   - Optional boolean specifying whether integration times should\n",
    "                           be sampled uniformly through the time range or log spaced.\n",
    "                           Defaults to False so that short term dynamics are captured.\n",
    "      number_evaluations - Optional number of integration times to evaluate each\n",
    "                           parameter at so that a temporal window can be learned.  If\n",
    "                           omitted defaults to a single point in time per parameter.\n",
    "\n",
    "    Returns 5 values:\n",
    "\n",
    "      random_inputs     - Array, sized number_droplets x 6, containing the droplets\n",
    "                          radii, temperatures, salinity, air temperature, relative\n",
    "                          humidity, and rhoa.\n",
    "      random_outputs    - Array, sized number_droplets x 2, containing the droplets\n",
    "                          radii and temperatures.\n",
    "      integration_times - Array, sized number_droplets, containing the times corresponding\n",
    "                          to the associated random_inputs and random_outputs.\n",
    "      weird_inputs      - Dictionary containing lists of weird input parameters.  Each\n",
    "                          key represents a category of \"weird\".\n",
    "      weird_outputs     - Dictionary containing lists of weird output parameters.  Each\n",
    "                          key represents a category of \"weird\".\n",
    "      \n",
    "    \"\"\"\n",
    "\n",
    "    # Promote run-time warnings to errors so we get an exception whenever the ODEs\n",
    "    # are evaluated in problematic corners of the parameter space.  This not only\n",
    "    # prevents them from showing up on standard error but also lets us log them and\n",
    "    # ignore them so we only sample the \"good\" parts of the space.\n",
    "    warnings.simplefilter( \"error\", RuntimeWarning )\n",
    "\n",
    "    random_inputs     = np.empty( (number_droplets * number_evaluations, 6), dtype=np.float32 )\n",
    "    random_inputs[::number_evaluations, :] = scale_droplet_parameters( np.reshape( np.random.uniform( -1, 1, number_droplets*6 ),\n",
    "                                                                      (number_droplets, 6) ).astype( \"float32\" ) )\n",
    "    # Duplicate each unique droplet parameter once for each evaluation.\n",
    "    # This keeps them in parameter order.\n",
    "    for droplet_index in np.arange( number_droplets ):\n",
    "        start_index = droplet_index * number_evaluations\n",
    "        end_index   = (droplet_index + 1) * number_evaluations\n",
    "\n",
    "        random_inputs[start_index+1:end_index, :] = random_inputs[start_index, :]\n",
    "\n",
    "    random_outputs = np.empty_like( random_inputs, shape=(number_droplets * number_evaluations, 2) )\n",
    "    \n",
    "    # Size of the time window to sample when we're generating multiple temporal\n",
    "    # evaluations.  In linear units when linear_time_flag==True (e.g. 1 second),\n",
    "    # otherwise in log-space (e.g. 1 order of magnitude).\n",
    "    TIME_WINDOW_SIZE = 1\n",
    "\n",
    "    # We generate data for a time window that is slightly larger than what we're interested\n",
    "    # in (logspace( -3, 1 )) so we can learn the endpoints.\n",
    "    TIME_RANGE = (10.0**-3.2, 10.0**1.1)\n",
    "\n",
    "    integration_times = np.empty_like( random_inputs, shape=(number_droplets * number_evaluations) )\n",
    "    if linear_time_flag:\n",
    "        # Generate the starting point for each of the evaluation groups.  We\n",
    "        # construct the other times in the group below.\n",
    "        integration_times[::number_evaluations] = np.random.uniform( TIME_RANGE[0], TIME_RANGE[1], number_droplets ).astype( \"float32\" )\n",
    "\n",
    "        if number_evaluations > 1:\n",
    "            for droplet_index in np.arange( number_droplets ):\n",
    "                start_index = droplet_index * number_evaluations\n",
    "                end_index   = (droplet_index + 1) * number_evaluations\n",
    "\n",
    "                # Generate the remaining points in this group while taking care\n",
    "                # to not go beyond the largest time we're allowed.\n",
    "                integration_times[start_index:end_index] = np.linspace( integration_times[start_index],\n",
    "                                                                        min( integration_times[start_index] + TIME_WINDOW_SIZE, TIME_RANGE[1] ),\n",
    "                                                                        number_evaluations )\n",
    "                \n",
    "    else:\n",
    "        # Generate the starting point for each of the evaluation groups.  We\n",
    "        # construct the other times in the group below.\n",
    "        integration_times[::number_evaluations] = (10.0**np.random.uniform( np.log10( TIME_RANGE[0] ),\n",
    "                                                                            np.log10( TIME_RANGE[1] ),\n",
    "                                                                            number_droplets )).astype( \"float32\" )\n",
    "                                                                            \n",
    "        if number_evaluations > 1:\n",
    "            for droplet_index in np.arange( number_droplets ):\n",
    "                start_index = droplet_index * number_evaluations\n",
    "                end_index   = (droplet_index + 1) * number_evaluations\n",
    "\n",
    "                # Generate the remaining points in this group while taking care\n",
    "                # to not go beyond the largest time we're allowed.\n",
    "                starting_exponent = np.log10( integration_times[start_index] )\n",
    "                integration_times[start_index:end_index] = 10.0**np.linspace( starting_exponent,\n",
    "                                                                              min( starting_exponent + TIME_WINDOW_SIZE, np.log10( TIME_RANGE[1] ) ),\n",
    "                                                                              number_evaluations )\n",
    "\n",
    "    # We count the number of problematic parameters we encounter.  Additionally,\n",
    "    # track the reason why we found the parameters problematic along with the\n",
    "    # parameters themselves.\n",
    "    number_warnings = 0\n",
    "    weird_inputs    = { \"evaluation_warning\": [],\n",
    "                        \"failed_solve\":       [],\n",
    "                        \"invalid_inputs\":     [] }\n",
    "    weird_outputs   = { \"radius_nonpositive\":      [],\n",
    "                        \"radius_too_small\":        [],\n",
    "                        \"radius_too_large\":        [],\n",
    "                        \"temperature_nonpositive\": [],\n",
    "                        \"temperature_too_small\":   [],\n",
    "                        \"temperature_too_large\":   [] }\n",
    "\n",
    "    # XXX: Need to figure out if we can simply evaluate the ODEs at number_evaluations-many points\n",
    "    #      and re-roll only the points that are outside the expected ranges.  Right now we evaluate\n",
    "    #      the ODEs for every t_final even when we could evaluate it once for a list of multiple so\n",
    "    #      that we discard all of the evaluations when one of them is problematic.\n",
    "    for droplet_index in np.arange( number_droplets * number_evaluations ):\n",
    "        \n",
    "        # Emulate a do/while loop so we always evaluate at least one parameter before\n",
    "        # deciding whether to keep it or not.\n",
    "        while True:\n",
    "            y0         = random_inputs[droplet_index, :2]\n",
    "            parameters = random_inputs[droplet_index, 2:]\n",
    "            t_final    = integration_times[droplet_index]\n",
    "    \n",
    "            try:\n",
    "                solution = solve_ivp( dydt, [0, t_final], y0, method=\"Radau\", t_eval=[t_final], args=(parameters,) )\n",
    "\n",
    "                if solution.success:\n",
    "                    random_outputs[droplet_index, 0] = solution.y[0][0]\n",
    "                    random_outputs[droplet_index, 1] = solution.y[1][0]\n",
    "\n",
    "                    good_parameters_flag = True\n",
    "                    \n",
    "                    # Check that we didn't get a physically impossible solution.  These\n",
    "                    # will be logged and we'll reroll the dice to replace them.\n",
    "                    #\n",
    "                    # NOTE: We don't strictly need to check for negative temperatures as\n",
    "                    #       that will get covered in validate_output_parameters() but\n",
    "                    #       we leave it here so it is easy to identify physically impossible\n",
    "                    #       cases.\n",
    "                    #\n",
    "                    if solution.y[0][0] <= 0.0:\n",
    "                        weird_outputs[\"radius_nonpositive\"].append( [y0, parameters, t_final, random_outputs[droplet_index, :]] )\n",
    "                        good_parameters_flag = False\n",
    "                    elif solution.y[1][0] <= 0.0:\n",
    "                        weird_outputs[\"temperature_nonpositive\"].append( [y0, parameters, t_final, random_outputs[droplet_index, :]] )\n",
    "                        good_parameters_flag = False\n",
    "\n",
    "                    # Check that we didn't get strange solutions that are outside of the\n",
    "                    # expected ranges.  These will also be logged and replaced.\n",
    "                    if solution.y[0][0] < 10.0**DROPLET_RADIUS_LOG_RANGE[0] * (100 - 3) / 100:\n",
    "                        weird_outputs[\"radius_too_small\"].append( [y0, parameters, t_final, random_outputs[droplet_index, :]] )\n",
    "                        good_parameters_flag = False\n",
    "                    elif solution.y[0][0] > 10.0**DROPLET_RADIUS_LOG_RANGE[1] * (100 + 3) / 100:\n",
    "                        weird_outputs[\"radius_too_large\"].append( [y0, parameters, t_final, random_outputs[droplet_index, :]] )\n",
    "                        good_parameters_flag = False\n",
    "\n",
    "                    if solution.y[1][0] < DROPLET_TEMPERATURE_RANGE[0] * (100 - 3) / 100:\n",
    "                        weird_outputs[\"temperature_too_small\"].append( [y0, parameters, t_final, random_outputs[droplet_index, :]] )\n",
    "                        good_parameters_flag = False\n",
    "                    elif solution.y[1][0] > DROPLET_TEMPERATURE_RANGE[1] * (100 + 3) / 100:\n",
    "                        weird_outputs[\"temperature_too_large\"].append( [y0, parameters, t_final, random_outputs[droplet_index, :]] )\n",
    "                        good_parameters_flag = False\n",
    "\n",
    "                    # Jump to the next droplet's parameters if we didn't detect a problem.\n",
    "                    if good_parameters_flag:\n",
    "                        break\n",
    "                else:\n",
    "                    # Record the cases that fail to converge.\n",
    "                    weird_inputs[\"failed_solve\"].append( [y0, parameters, t_final, solution.message] )\n",
    "\n",
    "            except RuntimeWarning as e:\n",
    "                weird_inputs[\"evaluation_warning\"].append( [y0, parameters, t_final, str( e )] )\n",
    "            except ValueError as e:\n",
    "                weird_inputs[\"invalid_inputs\"].append( [y0, parameters, t_final, str( e )] ) \n",
    "            \n",
    "            # We failed to create acceptable parameters.  Reroll the dice for \n",
    "            # this droplet and try again.\n",
    "            random_inputs[droplet_index, :] = scale_droplet_parameters( \n",
    "                np.random.uniform( -1, 1, 6 ).astype( \"float32\" ) )\n",
    "    \n",
    "    # XXX: Warn users if there were strange droplets?  No simple way to see if any\n",
    "    #      of the lists associated with weird_*'s keys are non-empty.\n",
    "    \n",
    "    return random_inputs, random_outputs, integration_times, weird_inputs, weird_outputs\n",
    "\n",
    "def merge_weird_parameters( parameters_1, parameters_2 ):\n",
    "    \"\"\"\n",
    "    Merge two dictionaries of lists into a separate copy containing the\n",
    "    concatenated lists of both.  All list entries of the first dictionary\n",
    "    come before the first entry of the second dictionary when the first\n",
    "    has a non-empty list.\n",
    "    \n",
    "    Takes 2 arguments:\n",
    "    \n",
    "      parameters_1 - 1st dictionary of lists to merge.\n",
    "      parameters_2 - 2nd dictionary of lists to merge.\n",
    "      \n",
    "    Returns 1 value:\n",
    "    \n",
    "      merged_parameers - Dictionary containing the merged lists.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    merged_parameters = parameters_1.copy()\n",
    "\n",
    "    for type_name, weird_things in parameters_2.items():\n",
    "        if type_name in merged_parameters:\n",
    "            merged_parameters[type_name].extend( weird_things )\n",
    "        else:\n",
    "            merged_parameters[type_name] = weird_things\n",
    "\n",
    "    return merged_parameters\n",
    "\n",
    "def write_weird_parameters_to_spreadsheet( file_name, weird_inputs, weird_outputs ):\n",
    "    \"\"\"\n",
    "    Creates an Excel file containing one spreadsheet per type of weird inputs or\n",
    "    outputs.\n",
    "    \n",
    "    See create_droplet_batch() for details on weird droplet inputs and outputs.\n",
    "\n",
    "    Takes 3 arguments:\n",
    "\n",
    "      file_name     - Path to the spreadsheet to create.  If it exists it is\n",
    "                      overwritten.\n",
    "      weird_inputs  - Dictionary of lists representing weird input droplet\n",
    "                      parameters.  \n",
    "      weird_outputs - Dictionary of lists representing weird output droplet\n",
    "                      parameters.\n",
    "\n",
    "    Returns nothing.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # XXX: move this\n",
    "    import pandas as pd\n",
    "\n",
    "    # Open the file for writing, overwriting an existing file if necessary.\n",
    "    with pd.ExcelWriter( file_name ) as writer:\n",
    "\n",
    "        # Create a DataFrame for each category of input parameters weirdness.\n",
    "        for type_name, weird_things in weird_inputs.items():\n",
    "            sheet_name = \"inputs-{:s}\".format( type_name )\n",
    "\n",
    "            # Columns in the DataFrame.\n",
    "            columns = [\"initial_radius\",\n",
    "                       \"initial_temperature\",\n",
    "                       \"salinity\",\n",
    "                       \"air_temperature\",\n",
    "                       \"relative_humidity\",\n",
    "                       \"rhoa\",\n",
    "                       \"t_final\",\n",
    "                       \"error\"]\n",
    "\n",
    "            # Convert our list of lists of NumPy arrays to lists of lists\n",
    "            # so spreadsheet cells contain a single value.\n",
    "            weird_things_listified = []\n",
    "            for weird_thing in weird_things:\n",
    "                weird_thing_listified = []\n",
    "\n",
    "                weird_thing_listified.extend( weird_thing[0].tolist() )\n",
    "                weird_thing_listified.extend( weird_thing[1].tolist() )\n",
    "                weird_thing_listified.extend( weird_thing[2:] )\n",
    "\n",
    "                weird_things_listified.append( weird_thing_listified )\n",
    "\n",
    "            # Build a DataFrame and write it as a new sheet.\n",
    "            filtered_df = pd.DataFrame( weird_things_listified, columns=columns )\n",
    "            filtered_df.to_excel( writer, sheet_name=sheet_name, index=False )\n",
    "\n",
    "        # Create a DataFrame for each category of output parameters weirdness.\n",
    "        for type_name, weird_things in weird_outputs.items():\n",
    "            sheet_name = \"outputs-{:s}\".format( type_name )\n",
    "\n",
    "            # Columns in the DataFrame.\n",
    "            columns = [\"initial_radius\",\n",
    "                       \"initial_temperature\",\n",
    "                       \"salinity\",\n",
    "                       \"air_temperature\",\n",
    "                       \"relative_humidity\",\n",
    "                       \"rhoa\",\n",
    "                       \"t_final\",\n",
    "                       \"radius\",\n",
    "                       \"temperature\"]\n",
    "\n",
    "            # Convert our list of lists of NumPy arrays to lists of lists\n",
    "            # so spreadsheet cells contain a single value.\n",
    "            weird_things_listified = []\n",
    "            for weird_thing in weird_things:\n",
    "                weird_thing_listified = []\n",
    "\n",
    "                weird_thing_listified.extend( weird_thing[0].tolist() )\n",
    "                weird_thing_listified.extend( weird_thing[1].tolist() )\n",
    "                weird_thing_listified.append( weird_thing[2] )\n",
    "                weird_thing_listified.extend( weird_thing[3].tolist() )\n",
    "\n",
    "                weird_things_listified.append( weird_thing_listified )\n",
    "\n",
    "            # Build a DataFrame and write it as a new sheet.\n",
    "            filtered_df = pd.DataFrame( weird_things_listified, columns=columns )\n",
    "            filtered_df.to_excel( writer, sheet_name=sheet_name, index=False )\n",
    "            \n",
    "def create_training_file( file_name, number_droplets, weird_file_name=None, user_batch_size=None ):\n",
    "    \"\"\"\n",
    "    Generates random droplet parameters, both inputs and their corresponding\n",
    "    ODE outputs, and writes them as fixed-size binary records to a file.  This\n",
    "    makes for efficient access, both sequential and random, for training and\n",
    "    analysis.\n",
    "    \n",
    "    The output file written is comprised of one or more 36-byte records, one\n",
    "    per droplet.  Each record holds 9x 32-bit, floating point in host-byte\n",
    "    order (typically little-endian):\n",
    "\n",
    "      1. Input radius, in meters\n",
    "      2. Input temperature, in Kelvin\n",
    "      3. Input salinity, aka the mass of disolved salt, in kilograms\n",
    "      4. Input air temperature, in Kelvin\n",
    "      5. Input relative humidity, as a non-dimensional value with 100% humidity at 1.0\n",
    "      6. Input rhoa, in non-dimensional units\n",
    "      7. Input evaluation time, in seconds\n",
    "      8. Output radius, in meters\n",
    "      9. Output temperature, in Kelvin\n",
    "\n",
    "    The file generated can be read via read_training_file(). \n",
    "    \n",
    "    Takes 4 arguments:\n",
    "    \n",
    "      file_name       - Path to the file to write the droplet parameters.  This\n",
    "                        is overwritten if it exists.\n",
    "      number_droplets - The number of droplets to generate.\n",
    "      weird_file_name - Optional path to write any weird parameters encountered\n",
    "                        during batch generation.  If specified an Excel spreadsheet\n",
    "                        file is written, otherwise weird parameters are silently\n",
    "                        ignored.\n",
    "      user_batch_size - Optional batch size specifying the number of parameters\n",
    "                        to generate at once.  If omitted, defaults to a \"small\"\n",
    "                        number of parameters that balances memory footprint,\n",
    "                        time to generate, and file write size.  This does not\n",
    "                        need to evenly divide number_droplets.\n",
    "    \n",
    "    Returns nothing.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Balance the time it takes to generate a single batch vs the efficiency of\n",
    "    # writing it out.  Each droplet's parameters takes 36 bytes (9x 32-bit floats\n",
    "    # comprised of the 7x input parameters and the 2x output parameters).\n",
    "    if user_batch_size is not None:\n",
    "        BATCH_SIZE = user_batch_size\n",
    "    else:\n",
    "        BATCH_SIZE = 1024 * 10\n",
    "    \n",
    "    # Number of batches to create including the last, partial batch when\n",
    "    # the batch size does not evenly divide the number of droplets.\n",
    "    number_batches = (number_droplets + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    \n",
    "    # Dictionaries for tracking weird parameters.\n",
    "    weird_inputs  = {}\n",
    "    weird_outputs = {}\n",
    "\n",
    "    with open( file_name, \"wb\" ) as output_fp:\n",
    "        for batch_index in range( number_batches ):\n",
    "\n",
    "            # Determine how many droplets to create in this batch.\n",
    "            if batch_index != (number_batches - 1):\n",
    "                batch_size = BATCH_SIZE\n",
    "            else:\n",
    "                batch_size = number_droplets % BATCH_SIZE\n",
    "            \n",
    "            # Get the next batch of droplets.\n",
    "            (inputs,\n",
    "             outputs,\n",
    "             times,\n",
    "             batch_weird_inputs,\n",
    "             batch_weird_outputs) = create_droplet_batch( batch_size )\n",
    "    \n",
    "            # Track the weirdness for post-mortem analysis.\n",
    "            weird_inputs  = merge_weird_parameters( weird_inputs, batch_weird_inputs )\n",
    "            weird_outputs = merge_weird_parameters( weird_outputs, batch_weird_outputs )                    \n",
    "\n",
    "            inputs_outputs = np.hstack( (inputs,\n",
    "                                         times.reshape( (batch_size, 1) ),\n",
    "                                         outputs) )\n",
    "\n",
    "            # Serialize the array \n",
    "            inputs_outputs.tofile( output_fp )\n",
    "\n",
    "    if weird_file_name is not None:\n",
    "        write_weird_parameters_to_spreadsheet( weird_file_name,\n",
    "                                               weird_inputs,\n",
    "                                               weird_outputs )\n",
    "\n",
    "def read_training_file( file_name ):\n",
    "    \"\"\"\n",
    "    Reads all of the fixed-size binary records from the path specified and returns\n",
    "    NumPy arrays containing input parameters, output parameters, and integration\n",
    "    times.\n",
    "    \n",
    "    Takes 1 arguments:\n",
    "    \n",
    "      file_name - Path to the file to parse.\n",
    "      \n",
    "    Returns 3 values:\n",
    "    \n",
    "      inputs  - NumPy array, shaped number_droplets x 6, containing the input parameters.\n",
    "      outputs - NumPy array, shaped number_droplets x 2, containing the output parameters.\n",
    "      times   - NumPy array, shaped number_droplets x 1, containing the integration times.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    inputs_outputs = np.fromfile( file_name, dtype=np.float32 ).reshape( (-1, 9) )\n",
    "    inputs         = inputs_outputs[:, :6]\n",
    "    times          = inputs_outputs[:, 6]\n",
    "    outputs        = inputs_outputs[:, 7:]\n",
    "\n",
    "    return inputs, outputs, times\n",
    "\n",
    "def clean_training_data( file_name ):\n",
    "    \"\"\"\n",
    "    Removes invalid droplet parameters found in the supplied file and overwrites it\n",
    "    so it only contains valid parameters.  Currently this only filters output\n",
    "    parameters that are outside of the expected ranges by more than 3% on either end.\n",
    "    \n",
    "    NOTE: This function isn't normally needed, but there may be times where invalid\n",
    "          outputs have been written but the vast majority are good and need to be\n",
    "          salvaged.\n",
    "    \n",
    "    Takes 1 argument:\n",
    "    \n",
    "      file_name - Path to the file containing droplet parameters written by\n",
    "                  create_training_file().  This file is overwritten with the\n",
    "                  filtered droplet parameters.\n",
    "                  \n",
    "    Returns 2 values:\n",
    "    \n",
    "      number_parameters     - Number of droplet parameters in the original file.\n",
    "      number_bad_parameters - Number of droplet parameters that were filtered out.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #\n",
    "    # NOTE: This isn't terribly efficient and assumes there is enough RAM to\n",
    "    #       read, make a copy, and write back out.\n",
    "    #\n",
    "    input_parameters, output_parameters, times = read_training_file( file_name )\n",
    "\n",
    "    bad_data_mask = ((output_parameters[:, 0] < 10.0**DROPLET_RADIUS_LOG_RANGE[0] * (100 - 3) / 100) |\n",
    "                     (output_parameters[:, 0] > 10.0**DROPLET_RADIUS_LOG_RANGE[1] * (100 + 3) / 100) |\n",
    "                     (output_parameters[:, 1] < DROPLET_TEMPERATURE_RANGE[0] * (100 - 3) / 100) |\n",
    "                     (output_parameters[:, 1] > DROPLET_TEMPERATURE_RANGE[1] * (100 + 3) / 100))\n",
    "\n",
    "    input_outputs = np.hstack( (input_parameters,\n",
    "                                times.reshape( (-1, 1) ),\n",
    "                                output_parameters) )\n",
    "\n",
    "    # Overwrite the original file with only the good data.\n",
    "    input_outputs[~bad_data_mask, :].tofile( file_name )\n",
    "\n",
    "    # Return the counts of good and bad data so the caller knows what was done.\n",
    "    return input_parameters.shape[0], bad_data_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c312c0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate generating a batch of droplets with a small number of evaluations.\n",
    "number_droplets    = 128\n",
    "number_evaluations = 4\n",
    "\n",
    "[inputs,\n",
    " outputs,\n",
    " integration_times,\n",
    " weird_inputs,\n",
    " weird_outputs] = create_droplet_batch( number_droplets, number_evaluations=number_evaluations )\n",
    "\n",
    "number_weird_inputs  = sum( map( lambda type_name: len( weird_inputs[type_name] ), weird_inputs ) )\n",
    "number_weird_outputs = sum( map( lambda type_name: len( weird_outputs[type_name] ), weird_outputs ) )\n",
    "\n",
    "print( \"Generated {:d} droplet parameter{:s}.\\n\".format(\n",
    "    number_droplets,\n",
    "    \"\" if number_droplets == 1 else \"s\" ) )\n",
    "\n",
    "if number_weird_inputs > 0:\n",
    "    print( \"{:d} weird input{:s}:\\n\".format(\n",
    "        number_weird_inputs,\n",
    "        \"\" if number_weird_inputs == 1 else \"s\" ) )\n",
    "    \n",
    "    for type_name, weird_things in weird_inputs.items():\n",
    "        number_weird_things = len( weird_things )\n",
    "        if number_weird_things == 0:\n",
    "            continue\n",
    "            \n",
    "        print( \"    {:d} {:s}\".format(\n",
    "            number_weird_things,\n",
    "            type_name ) )\n",
    "        \n",
    "    print( \"\" )\n",
    "\n",
    "if number_weird_outputs > 0:\n",
    "    print( \"{:d} weird output{:s}:\\n\".format(\n",
    "        number_weird_outputs,\n",
    "        \"\" if number_weird_outputs == 1 else \"s\" ) )\n",
    "    \n",
    "    for type_name, weird_things in weird_outputs.items():\n",
    "        number_weird_things = len( weird_things )\n",
    "        if number_weird_things == 0:\n",
    "            continue\n",
    "            \n",
    "        print( \"    {:d} {:s}\".format(\n",
    "            number_weird_things,\n",
    "            type_name ) )\n",
    "        \n",
    "    print( \"\" )\n",
    "    \n",
    "write_weird_parameters_to_spreadsheet( \"test.xlsx\", weird_inputs, weird_outputs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84262077-9a12-487a-bf8d-9f24f36d5a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training file with a handful of droplets, filter out any invalid\n",
    "# parameters (this should no longer happen), and read them back in to\n",
    "# see the distribution of t_final.  \n",
    "training_test_path = \"foo.data\"\n",
    "weird_test_path    = \"foo.xlsx\"\n",
    "create_training_file( training_test_path, 1024, weird_file_name=weird_test_path )\n",
    "\n",
    "number_parameters, number_bad_parameters = clean_training_data( training_test_path )\n",
    "print( \"Removed {:d} parameter{:s} from {:d} parameter{:s} ({:.2f}%).\".format(\n",
    "    number_bad_parameters,\n",
    "    \"\" if number_bad_parameters == 1 else \"s\",\n",
    "    number_parameters,\n",
    "    \"\" if number_parameters == 1 else \"s\",\n",
    "    number_bad_parameters / number_parameters * 100.0 ) )\n",
    "\n",
    "input_parameters, output_parameters, times = read_training_file( training_test_path )\n",
    "\n",
    "fig_h, ax_h = plt.subplots( 1, 1 )\n",
    "ax_h.plot( np.log10( times ), \".\" )\n",
    "ax_h.set_xlabel( \"Droplet #\" )\n",
    "ax_h.set_ylabel( \"log10( $t_{final}$ )\" )\n",
    "ax_h.set_title( \"Distribution of $t_{final}$\" )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24c65f1-9042-4f46-a821-1f52c26df2e9",
   "metadata": {},
   "source": [
    "# Training a Model\n",
    "We define a simple model architecture in PyTorch, setup our loss function and basic\n",
    "hyperparameters, and then randomly walk through the training data one or more times.\n",
    "If configured, the trained model is saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ac2bf1-93af-4fea-b739-dee735389cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet( nn.Module ):\n",
    "    \"\"\"\n",
    "    4-layer multi-layer perceptron (MLP) with ReLU activations.  This aims to\n",
    "    balance parameter count vs computational efficiency so that inferencing with \n",
    "    it is faster than Gauss-Newton iterative solvers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__( self) :\n",
    "        super().__init__()\n",
    "        \n",
    "        #\n",
    "        # NOTE: These sizes were chosen without any consideration other than creating\n",
    "        #       a small network (wrt parameter count) and should have good computational\n",
    "        #       efficiency (wrt memory alignment and cache lines).  No effort has been\n",
    "        #       spent to improve upon the initial guess.\n",
    "        #\n",
    "        self.fc1 = nn.Linear( 7, 32 )\n",
    "        self.fc2 = nn.Linear( 32, 32 )\n",
    "        self.fc3 = nn.Linear( 32, 32 )\n",
    "        self.fc4 = nn.Linear( 32, 2 )\n",
    "        \n",
    "    def forward( self, x ):\n",
    "        x = torch.relu( self.fc1( x ) )\n",
    "        x = torch.relu( self.fc2( x ) )\n",
    "        x = torch.relu( self.fc3( x ) )\n",
    "        x = self.fc4( x )\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7ed01a-ae54-4af0-9021-37cd242ce3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using a local GPU if we have one, otherwise stay on the CPU.  While\n",
    "# SimpleNet isn't a large model having GPU acceleration for large batch\n",
    "# counts and a non-trivial number of droplet parameters (>100 million) certainly\n",
    "# helps.\n",
    "device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\" )\n",
    "\n",
    "print( \"Training with '{}'.\".format( device ) )\n",
    "\n",
    "# Create an instance of SimpleNet and configure its optimization parameters:\n",
    "#\n",
    "#   - We use mean-squared error loss so we chase outliers\n",
    "#   - We use Adam so we have momentum when performing gradient descent\n",
    "#   - Given that we have a relatively large batch size we use a large\n",
    "#     initial learning rate of 1e-3.\n",
    "#   - We want smaller weights so we use a L2 regularization penalty of 1e-6\n",
    "#     to encourage that (as well as having non-zero weights rather than\n",
    "#     leaning heavily on just a subset of weights)\n",
    "#\n",
    "model     = SimpleNet()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam( model.parameters(), lr=1e-3, weight_decay=1e-6 )\n",
    "\n",
    "# Move the model to the device we're training with.\n",
    "model = model.to( device )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35239ff-5f5e-4efc-8eec-0089a163ee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model( model, criterion, optimizer, device, number_epochs, training_file ):\n",
    "    \"\"\"\n",
    "    Trains the supplied model for one or more epochs using all of the droplet parameters\n",
    "    in an on-disk training file.  The parameters are read into memory once and then\n",
    "    randomly sampled each epoch.  Any weird parameters encountered in the training file\n",
    "    are logged and \n",
    "    \n",
    "    Takes 6 arguments:\n",
    "    \n",
    "      model         - PyTorch model to optimize.\n",
    "      criterion     - PyTorch loss object to use during optimization.\n",
    "      optimizer     - PyTorch optimizer associated with model.\n",
    "      device        - Device string indicating where the optimization is \n",
    "                      being performed.\n",
    "      number_epochs - Number of epochs to train model for.  All training\n",
    "                      data in training_file will be seen by the model \n",
    "                      this many times.\n",
    "      training_file - Path to the file containing training data created by\n",
    "                      create_training_file().\n",
    "    \n",
    "    Returns 1 value:\n",
    "    \n",
    "      loss_history - List of training losses, one per mini-batch during\n",
    "                     the training process.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #\n",
    "    # NOTE: This is inefficient and requires the entire training data set to reside in\n",
    "    #       RAM.  We could read chunks of the file on demand but that would require\n",
    "    #       a more sophisticated training loop that performs I/O in a separate thread\n",
    "    #       while the optimization process executes.\n",
    "    #\n",
    "    #       TL;DR Find a big machine to train on.\n",
    "    #\n",
    "    input_parameters, output_parameters, integration_times = read_training_file( training_file )\n",
    "\n",
    "    BATCH_SIZE      = 1024\n",
    "    MINI_BATCH_SIZE = 1024\n",
    "\n",
    "    #\n",
    "    # NOTE: This ignores parameters if the last batch isn't complete.\n",
    "    #\n",
    "    NUMBER_BATCHES  = input_parameters.shape[0] // BATCH_SIZE\n",
    "    \n",
    "    # Track each mini-batch's training loss for analysis.\n",
    "    loss_history = []\n",
    "\n",
    "    batch_indices = np.arange( NUMBER_BATCHES )\n",
    "    \n",
    "    for epoch_index in range( number_epochs ):\n",
    "        model.train()\n",
    "\n",
    "        # Reset our training loss.\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # \"Shuffle\" our training data so each epoch sees it in a different\n",
    "        # order.  Note that we generate a permutation of batch indices so\n",
    "        # we don't actually rearrange the training data in memory and\n",
    "        # dramatically slow things down.\n",
    "        permuted_batch_indices = np.random.permutation( batch_indices )\n",
    "        \n",
    "        for batch_index in range( NUMBER_BATCHES ):\n",
    "            start_index = permuted_batch_indices[batch_index]*BATCH_SIZE\n",
    "            end_index   = start_index + BATCH_SIZE\n",
    "            \n",
    "            # Get the next batch of droplets.\n",
    "            inputs  = input_parameters[start_index:end_index, :]\n",
    "            outputs = output_parameters[start_index:end_index, :]\n",
    "            times   = integration_times[start_index:end_index]\n",
    "            \n",
    "            # Normalize the inputs and outputs to [-1, 1].\n",
    "            normalized_inputs  = normalize_droplet_parameters( inputs )\n",
    "            normalized_outputs = normalize_droplet_parameters( outputs )\n",
    "\n",
    "            # XXX: Need to rethink how we handle time as an input.  This is annoying\n",
    "            #      to have to stack a reshaped vector each time.\n",
    "            normalized_inputs = np.hstack( (normalized_inputs,\n",
    "                                            times.reshape( (BATCH_SIZE, 1) )) )\n",
    "            \n",
    "            normalized_inputs  = torch.from_numpy( normalized_inputs ).to( device )\n",
    "            normalized_outputs = torch.from_numpy( normalized_outputs ).to( device )\n",
    "                        \n",
    "            # Reset the parameter gradients.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Perform the forward pass.\n",
    "            normalized_approximations = model( normalized_inputs )\n",
    "            \n",
    "            # Estimate the loss.\n",
    "            loss = criterion( normalized_approximations, normalized_outputs )\n",
    "\n",
    "            # Backwards pass and optimization.\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if batch_index > 0 and batch_index % MINI_BATCH_SIZE == 0:\n",
    "                running_loss /= MINI_BATCH_SIZE\n",
    "\n",
    "                print( \"Epoch #{:d}, batch #{:d} loss: {:g}\".format(\n",
    "                    epoch_index + 1,\n",
    "                    batch_index + 1,\n",
    "                    running_loss ) )\n",
    "                loss_history.append( running_loss )\n",
    "\n",
    "                # Break out of the batch loop if we ever encounter an input\n",
    "                # that causes the loss to spike.  Training data generation\n",
    "                # should produce good inputs and outputs though should something\n",
    "                # slip through we want to immediately stop training so we can\n",
    "                # understand what went wrong - there is no way to recover\n",
    "                # from a loss spike that is O(10) when our target loss is O(1e-4).\n",
    "                if running_loss > 10:\n",
    "                    print( \"Crazy loss!\" )\n",
    "                    print( inputs, outputs )\n",
    "                    break\n",
    "                \n",
    "                running_loss = 0.0\n",
    "        else:\n",
    "            # We finished all of the batches.  Adjust the learning rate and\n",
    "            # go to the next epoch.\n",
    "            for parameter_group in optimizer.param_groups:\n",
    "                parameter_group[\"lr\"] /= 2\n",
    "            \n",
    "            continue\n",
    "\n",
    "        # We didn't complete all of the batches in this epoch.\n",
    "        break\n",
    "        \n",
    "    # Handle the case where we didn't have enough data to complete a mini-batch.\n",
    "    if len( loss_history ) == 0:\n",
    "        running_loss /= number_epochs * NUMBER_BATCHES\n",
    "        loss_history.append( running_loss )\n",
    "        \n",
    "        print( \"Epoch #{:d}, batch #{:d} loss: {:g}\".format(\n",
    "            number_epochs,\n",
    "            NUMBER_BATCHES,\n",
    "            running_loss ) )\n",
    "                \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7b5bdb-791b-4837-ad85-57f9da737370",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if train_model_flag:\n",
    "    if training_data_path is None:\n",
    "        raise ValueError( \"Set training_data_path to where the training data are located!\" )\n",
    "\n",
    "    loss_history = train_model( model, \n",
    "                                criterion,\n",
    "                                optimizer,\n",
    "                                device,\n",
    "                                number_epochs, \n",
    "                                training_data_path )\n",
    "\n",
    "    # Plot the training loss to qualitatively assess how the model is doing.\n",
    "    # Loss should consistently decrease over mini-batches.\n",
    "    #\n",
    "    # NOTE: This is only one aspect to evaluate a model's performance as the\n",
    "    #       loss reported is on *training* data and not independent test\n",
    "    #       data.  As a result, this curve will be overly optimistic.\n",
    "    #\n",
    "    fig_h, ax_h = plt.subplots( 1, 1, figsize=(4, 4) )\n",
    "\n",
    "    ax_h.plot( np.log10( loss_history ), \".\" )\n",
    "    ax_h.set_xlabel( \"Mini Batch Number\" )\n",
    "    ax_h.set_ylabel( \"log10( MSE )\" )\n",
    "    ax_h.set_title( \"Training Loss\" )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print( \"Trained on {:d} mini-batches.\".format(\n",
    "        len( loss_history ) ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34f3b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_save_path is None:\n",
    "    warnings.warn( \"Please set model_path if you wish to save the model to disk.  This is intentionally *NOT* set to avoid accidentally overwriting an existing model.\" )\n",
    "else:\n",
    "    torch.save( model.state_dict(), model_save_path )\n",
    "    \n",
    "    print( \"Saved the current model to '{:s}'.\".format( model_save_path ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9df101-9961-4dec-a890-b2a6257adec8",
   "metadata": {},
   "source": [
    "# Performance Analysis\n",
    "Load a previously trained model, if requested, and qualitatively assess its performance relative\n",
    "to the ODEs it was trained to approximate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a4252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model_flag:\n",
    "    model = SimpleNet()\n",
    "    model.load_state_dict( torch.load( model_load_path ) )\n",
    "    model = model.to( device )\n",
    "    \n",
    "    print( \"Loaded a model from '{:s}'.\".format( model_load_path ) )\n",
    "else:\n",
    "    print( \"Using the previously trained model.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcd1538-54d8-42f5-965b-291603ebf417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_inference( input_parameters, times, model, device ):\n",
    "    \"\"\"\n",
    "    Estimates droplet parameters using a specified model.  Model evaluation is\n",
    "    performed on the CPU\n",
    "    \n",
    "    Takes 4 arguments:\n",
    "    \n",
    "      input_parameters - NumPy array, sized number_droplets x 6, containing the\n",
    "                         input parameters for number_droplets-many droplets.  These are\n",
    "                         provided in their natural, physical ranges.\n",
    "      times            - NumPy array, sized number_droplets, containing the integration\n",
    "                         times to evaluate each droplet at.\n",
    "      model            - PyTorch model to use.\n",
    "      device           - XXX\n",
    "    \n",
    "    Returns 1 value:\n",
    "    \n",
    "      output_parameters - NumPy array, sized number_droplets x 2, containing the\n",
    "                          estimated radius and temperature for each of the droplets\n",
    "                          at the specified integraition times.  These are in their\n",
    "                          natural physical ranges.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    eval_model = model.to( device )\n",
    "    eval_model.eval()\n",
    "    \n",
    "    normalized_inputs = np.hstack( (normalize_droplet_parameters( input_parameters ),\n",
    "                                    times.reshape( (-1, 1) )) ).astype( \"float32\" )\n",
    "    normalized_outputs = eval_model( torch.from_numpy( normalized_inputs ).to( device ) ).to( \"cpu\" ).detach().numpy()\n",
    "    \n",
    "    return scale_droplet_parameters( normalized_outputs )\n",
    "\n",
    "# Verify that the model matches the ODEs outputs for a single parameter.\n",
    "test_inputs      = np.array( [9.58937380e-05, 3.03864258e+02, 7.20879248e-16, 2.77617767e+02, 1.04355168e+00, 8.33695471e-01] ).reshape( (1, -1) )\n",
    "test_time        = np.array( [1.0] )\n",
    "\n",
    "t_span   = (0, 10)\n",
    "solution = solve_ivp_float32_outputs( dydt,\n",
    "                                      t_span,\n",
    "                                      [test_inputs[0, 0], test_inputs[0, 1]],\n",
    "                                      method=\"Radau\",\n",
    "                                      t_eval=test_time,\n",
    "                                      args=(test_inputs[0, 2:],) )\n",
    "\n",
    "#\n",
    "# NOTE: We must transpose our expected outputs as solve_ivp() returns vectors\n",
    "#       in a different direction as do_inference().\n",
    "#\n",
    "expected_outputs = np.array( [solution.y[0], solution.y[1]] ).T\n",
    "\n",
    "# What does our model estimate for this set of inputs?\n",
    "test_outputs = do_inference( test_inputs, test_time, model, device )\n",
    "\n",
    "# Sanity check that we're within half a percent relative difference for radius\n",
    "# and temperature.\n",
    "#\n",
    "# NOTE: Changing the model may require an updated tolerance as new models\n",
    "#       may not have learned to approximate this particular input.  Be\n",
    "#       careful when adjusting this tolerance!\n",
    "#\n",
    "# NOTE: We report a warning instead of raising an exception since training\n",
    "#       an underperforming model shouldn't preclude the analysis and weights\n",
    "#       export process.\n",
    "#\n",
    "if not np.allclose( test_outputs, expected_outputs, rtol=5e-3 ):\n",
    "    warnings.warn( \"The model did not compute the expected size/temperature ({}) for {} ({})\".format(\n",
    "        expected_outputs,\n",
    "        test_inputs,\n",
    "        test_outputs ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f258cfe-afa8-45e8-9c54-9062bb388ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_performance( model, input_parameters=None, figure_size=None ):\n",
    "    \"\"\"\n",
    "    Creates a figure with four plots to qualitatively assess the supplied model's\n",
    "    performance on a single droplet's parameters.  For both the radius and\n",
    "    temperature variables the ODE outputs are plotted against the model's estimates.\n",
    "    Additionally, the relative and absolute differences of each variable are\n",
    "    plotted so fine-grained differences in the solutions can be reviewed.\n",
    "    \n",
    "    XXX: This should return the normalized RMSE for radius and temperature.\n",
    "    \n",
    "    Takes 3 arguments:\n",
    "    \n",
    "      model            - PyTorch model to compare against the ODEs.\n",
    "      input_parameters - Optional NumPy array of input parameters to evaluate performance\n",
    "                         on.  If omitted, a random set of input parameters are sampled.\n",
    "      figure_size      - Optional sequence, of length 2, containing the width\n",
    "                         and height of the figure created.  If omitted, defaults\n",
    "                         to something big enough to comfortably assess a single\n",
    "                         model's outputs.\n",
    "    \n",
    "    Returns nothing.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Default to something that fits a set of four plots (in a 2x2 grid) comfortably.\n",
    "    if figure_size is None:\n",
    "        figure_size = (9, 8)\n",
    "    \n",
    "    # Smoothly evaluate the solution for 10 seconds into the future.\n",
    "    FINAL_TIME         = 10.0\n",
    "    NUMBER_TIME_POINTS = 1000\n",
    "\n",
    "    # Specify the colors/styles in our plots.\n",
    "    TRUTH_COLOR    = \"b\"\n",
    "    MODEL_COLOR    = \"r.\"\n",
    "    RELATIVE_COLOR = \"darkmagenta\"\n",
    "    ABSOLUTE_COLOR = \"dodgerblue\"\n",
    "    \n",
    "    # Sample the times in log-space so we get good coverage in both the DNS and\n",
    "    # LES regions.\n",
    "    t_eval = np.logspace( -3, np.log10( FINAL_TIME ), NUMBER_TIME_POINTS )\n",
    "    \n",
    "    # Get a random droplet if we weren't provided one.\n",
    "    if input_parameters is None:\n",
    "        # XXX: This is wasteful.  Just call np.random.uniform()\n",
    "        (input_parameters,\n",
    "         _,\n",
    "         _,\n",
    "         _,\n",
    "         _) = create_droplet_batch( 1 )\n",
    "\n",
    "    # Report the droplet we're evaluating in case we randomly sampled it.\n",
    "    print( \"Inputs: {}\".format( input_parameters ) )\n",
    "        \n",
    "    # Get truth from the ODEs.\n",
    "    y0           = (input_parameters[0, 0], input_parameters[0, 1])    \n",
    "    solution     = solve_ivp( dydt, [0, FINAL_TIME], y0, method=\"Radau\", t_eval=t_eval, args=(input_parameters[0, 2:],) )\n",
    "    truth_output = np.vstack( (solution.y[0][:], solution.y[1][:]) ).T\n",
    "\n",
    "    # Get the model's estimate.\n",
    "    model_output = do_inference( np.tile( input_parameters, (t_eval.shape[0], 1) ),\n",
    "                                 t_eval,\n",
    "                                 model,\n",
    "                                 \"cpu\" )\n",
    "\n",
    "    # Compute the normalized RMSE across the entire time scale.\n",
    "    rmse_0 = np.sqrt( np.mean( (truth_output[:, 0] - model_output[:, 0])**2 ) ) / np.mean( truth_output[:, 0] ) \n",
    "    rmse_1 = np.sqrt( np.mean( (truth_output[:, 1] - model_output[:, 1])**2 ) ) / np.mean( truth_output[:, 1] ) \n",
    "    print( \"NRMSE: {:g}%, {:g}%\".format( rmse_0 * 100, rmse_1 * 100) )\n",
    "    \n",
    "    # Compute the normalized RMSE for different regions (DNS and LES) for\n",
    "    # finer-grained performance assessment.\n",
    "    t_eval_mask = np.empty( (NUMBER_TIME_POINTS, 4),\n",
    "                            dtype=np.bool_ )\n",
    "    t_eval_mask[:, 0] = (t_eval  < 1e-2)\n",
    "    t_eval_mask[:, 1] = (t_eval >= 1e-2) & (t_eval < 1e-1)\n",
    "    t_eval_mask[:, 2] = (t_eval >= 1e-1) & (t_eval < 1e0)\n",
    "    t_eval_mask[:, 3] = (t_eval >= 1e0) \n",
    "    \n",
    "    rmse_0 = np.empty( (4,), dtype=np.float32 )\n",
    "    rmse_1 = np.empty( (4,), dtype=np.float32 )\n",
    "    \n",
    "    for scale_index in np.arange( t_eval_mask.shape[1] ):\n",
    "        rmse_0[scale_index] = np.sqrt( np.mean( (truth_output[t_eval_mask[:, scale_index], 0] - model_output[t_eval_mask[:, scale_index], 0])**2 ) ) / np.mean( truth_output[t_eval_mask[:, scale_index], 0] ) \n",
    "        rmse_1[scale_index] = np.sqrt( np.mean( (truth_output[t_eval_mask[:, scale_index], 1] - model_output[t_eval_mask[:, scale_index], 1])**2 ) ) / np.mean( truth_output[t_eval_mask[:, scale_index], 1] ) \n",
    "    \n",
    "    print( \"NRMSE: {}%, {}%\".format( rmse_0 * 100, rmse_1 * 100 ) )\n",
    "    \n",
    "    # Create our figure and embed the parameters that were evaluated.\n",
    "    fig_h, ax_h = plt.subplots( 2, 2, figsize=figure_size )\n",
    "    fig_h.suptitle( \"Droplet Size and Temperature\\nRadius={:g}, Temperature={:g}, m_s={:g}, Air Temp={:g}, RH={:g}, rhoa={:g}\".format(\n",
    "        input_parameters[0, 0],\n",
    "        input_parameters[0, 1],\n",
    "        input_parameters[0, 2],\n",
    "        input_parameters[0, 3],\n",
    "        input_parameters[0, 4],\n",
    "        input_parameters[0, 5]\n",
    "        ) )\n",
    "\n",
    "    # Truth vs model predictions.\n",
    "    ax_h[0][0].plot( t_eval, truth_output[:, 0], TRUTH_COLOR,\n",
    "                     t_eval, model_output[:, 0], MODEL_COLOR )\n",
    "    ax_h[0][1].plot( t_eval, truth_output[:, 1], TRUTH_COLOR,\n",
    "                     t_eval, model_output[:, 1], MODEL_COLOR )\n",
    "\n",
    "    # Relative difference between truth and model.\n",
    "    ax_h[1][0].plot( t_eval, \n",
    "                     np.abs( truth_output[:, 0] - model_output[:, 0] ) / truth_output[:, 0] * 100,\n",
    "                     color=RELATIVE_COLOR )\n",
    "    ax_h[1][0].tick_params( axis=\"y\", labelcolor=RELATIVE_COLOR )\n",
    "    ax_h[1][1].plot( t_eval, \n",
    "                     np.abs( truth_output[:, 1] - model_output[:, 1] ) / truth_output[:, 1] * 100,\n",
    "                     color=RELATIVE_COLOR )\n",
    "    ax_h[1][1].tick_params( axis=\"y\", labelcolor=RELATIVE_COLOR )\n",
    "\n",
    "    # Aboslute difference between truth and model.\n",
    "    ax_h_twin_radius = ax_h[1][0].twinx()\n",
    "    ax_h_twin_radius.plot( t_eval, np.abs( truth_output[:, 0] - model_output[:, 0] ), color=ABSOLUTE_COLOR )\n",
    "    ax_h_twin_radius.set_ylabel( \"Absolute Difference (m)\" )\n",
    "    ax_h_twin_radius.tick_params( axis=\"y\", labelcolor=ABSOLUTE_COLOR )\n",
    "\n",
    "    ax_h_twin_temperature = ax_h[1][1].twinx()\n",
    "    ax_h_twin_temperature.plot( t_eval, np.abs( truth_output[:, 1] - model_output[:, 1] ), color=ABSOLUTE_COLOR )\n",
    "    ax_h_twin_temperature.set_ylabel( \"Absolute Difference (K)\" )\n",
    "    ax_h_twin_temperature.tick_params( axis=\"y\", labelcolor=ABSOLUTE_COLOR )\n",
    "\n",
    "    # Label the comparison plots' lines.\n",
    "    ax_h[0][0].legend( [\"Truth\", \"Model\"] )\n",
    "    ax_h[0][1].legend( [\"Truth\", \"Model\"] )\n",
    "\n",
    "    # Label the columns of plots.\n",
    "    ax_h[0][0].set_title( \"Radius\" )\n",
    "    ax_h[0][1].set_title( \"Temperature\" )\n",
    "    \n",
    "    ax_h[0][0].set_ylabel( \"Radius (m)\" )\n",
    "    ax_h[0][1].set_ylabel( \"Temperature (K)\" )\n",
    "    ax_h[1][0].set_ylabel( \"Relative Difference (%)\" )\n",
    "    ax_h[1][0].set_xlabel( \"Time (s)\" )\n",
    "    ax_h[1][1].set_ylabel( \"Relative Difference (%)\" )\n",
    "    ax_h[1][1].set_xlabel( \"Time (s)\" )\n",
    "\n",
    "    # Show time in log-scale as well as the droplets' radius.\n",
    "    ax_h[0][0].set_xscale( \"log\" )\n",
    "    ax_h[0][0].set_yscale( \"log\" )\n",
    "    ax_h[0][1].set_xscale( \"log\" )\n",
    "    ax_h[0][1].set_yscale( \"log\" )\n",
    "    ax_h[1][0].set_xscale( \"log\" )\n",
    "    ax_h[1][1].set_xscale( \"log\" )\n",
    "    \n",
    "    fig_h.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ef8cbb-50ab-4133-88ef-4460d4a93f9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_parameters = scale_droplet_parameters( np.random.uniform( low=-1.0, high=1.0, size=(1, 6) ).astype( \"float32\" ) )\n",
    "analyze_model_performance( model, input_parameters )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad20ea",
   "metadata": {},
   "source": [
    "# Serializing Model Weights into Fortran\n",
    "Generate a simple export of the model's weight into a format that is usable for a naive implementation of inference with SimpleNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c6e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fortran_module( model_name, model_state, output_path ):\n",
    "    \"\"\"\n",
    "    Creates a Fortran 2003 module that allows use of the supplied model with a\n",
    "    batch size of 1 during inference.\n",
    "    \n",
    "    NOTE: This currently expects the supplied model state to represent a 4-layer\n",
    "          MLP with a specific set weights/biases names.\n",
    "    \n",
    "    Takes 2 arguments:\n",
    "    \n",
    "      model_state     - PyTorch model state dictionary for the model to expose.\n",
    "      output_path     - File to write to.  If this exists it is overwritten.\n",
    "      \n",
    "    Returns nothing.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #\n",
    "    # NOTE: This is hard coded against the SimpleNet class' implementation.  This\n",
    "    #       strange coupling could be better (less strange?) by moving this routine\n",
    "    #       into SimpleNet.\n",
    "    #\n",
    "    expected_weights = [\"fc1.weight\",\n",
    "                        \"fc2.weight\",\n",
    "                        \"fc3.weight\",\n",
    "                        \"fc4.weight\"]\n",
    "    expected_biases  = [\"fc1.bias\",\n",
    "                        \"fc2.bias\",\n",
    "                        \"fc3.bias\",\n",
    "                        \"fc4.bias\"]\n",
    "    \n",
    "    def write_module_prolog( model_name, model_state, output_fp ):\n",
    "        \"\"\"     \n",
    "        Writes the module's prolog to the supplied file handle.\n",
    "\n",
    "        Takes 3 arguments:\n",
    "        \n",
    "          model_name  - Name of the model to write in the generated module's comments\n",
    "                        so as to identify where the weights came from.\n",
    "          model_state - PyTorch model state dictionary for the model exposed by the module.\n",
    "          output_fp   - File handle to write to.\n",
    "          \n",
    "        Returns nothing.\n",
    "\n",
    "        \"\"\"\n",
    "           \n",
    "        import datetime\n",
    "            \n",
    "        expected_parameters = set( expected_weights + expected_biases )\n",
    "        \n",
    "        # Confirm we have the weights and biases that we expect.\n",
    "        if expected_parameters != set( model_state.keys() ):\n",
    "            raise ValueError( \"Model provided does not match a 4-layer MLP!\" )\n",
    "        \n",
    "        # Ensure that the weights are 2D matrices.\n",
    "        #\n",
    "        # NOTE: We could be more thorough to ensure that the output of one layer\n",
    "        #       matches the input of the next layer but we're in a rush right now...\n",
    "        #\n",
    "        for weights_name in expected_weights:\n",
    "            if len( model_state[weights_name].shape ) != 2:\n",
    "                raise ValueError( \"'{:s}' is not a rank-2 tensor!\".format( weights_name ) )\n",
    "                \n",
    "        # Ensure that the biases are vectors.\n",
    "        #\n",
    "        # NOTE: We could be more thorough to ensure that the output of one layer\n",
    "        #       matches the input of the next layer but we're in a rush right now...\n",
    "        #\n",
    "        for bias_name in expected_biases:\n",
    "            if len( model_state[bias_name].shape ) != 1:\n",
    "                raise ValueError( \"'{:s}' is not a rank-1 tensor!\".format( bias_name ) )\n",
    "        \n",
    "        # Get the current date so people have an idea of when/how this module was\n",
    "        # created.\n",
    "        right_now             = datetime.datetime.now()\n",
    "        current_date_time_str = right_now.strftime( \"%Y/%m/%d at %H:%M:%S\" )\n",
    "        \n",
    "        preamble_str = \"\"\"!\n",
    "! NOTE: This module was generated from {model_name:s} on {creation_date:s}.\n",
    "!       Do not modify this directly!  Update the module generator's template and\n",
    "!       regenerate this file!\n",
    "!\n",
    "\n",
    "module droplet_model\n",
    "\n",
    "    ! Module containing a model of droplet size and temperature using a 4-layer\n",
    "    ! multi-layer perceptron (MLP) neural network.  Once initialized, a single\n",
    "    ! droplet's parameters and timestep can be used to estimate its future size\n",
    "    ! and temperature.  This approach avoids an iterative search using\n",
    "    ! Gauss-Newton to estimate its future charueacteristics and is typically much\n",
    "    ! faster (O(30x) observed for small parameter count MLPs).\n",
    "    !\n",
    "    ! The module must be initialized with a call to initialize_model() and then\n",
    "    ! droplet estimation, estimate(), can be performed as many times as\n",
    "    ! necessary.  Weights for the MLP are hardcoded so initialization does not\n",
    "    ! require any external resources.  An example calling sequence is the\n",
    "    ! following:\n",
    "    !\n",
    "    !     ! Somewhere during simulation startup.\n",
    "    !     call initialize_model()\n",
    "    !\n",
    "    !     ...\n",
    "    !\n",
    "    !     ! Particle simulation code.\n",
    "    !     real*4 :: input_parameters(NUMBER_INPUTS)\n",
    "    !     real*4 :: output_parameters(NUMBER_OUTPUTS)\n",
    "    !\n",
    "    !     do particle_index = 1, number_particles\n",
    "    !\n",
    "    !         input_parameters = [radius, temperature, salinity, air_temperature, rh, rhoa, t_final]\n",
    "    !         call estimate( input_parameters, output_parameters )\n",
    "    !\n",
    "    !         new_radius      = output_parameters(RADIUS_INDEX)\n",
    "    !         new_temperature = output_parameters(TEMPERATURE_INDEX)\n",
    "    !\n",
    "    !     end do\n",
    "    !\n",
    "    ! Inputs to the estimation, as are outputs, are provided in physical units\n",
    "    ! and are internally normalized before using the model allowing this to be a\n",
    "    ! drop-in replacement for alternative approaches for estimating droplet\n",
    "    ! characteristics.\n",
    "    !\n",
    "    ! The model was trained on droplet parameters sampled from the product space\n",
    "    ! of each of the individual parameters.  While this simple approach allowed\n",
    "    ! for rapid development of a prototype it does include combinations of\n",
    "    ! parameters that may be physically impossible which may have been\n",
    "    ! under-sampled during training.  As a result the estimations in these\n",
    "    ! corners of the parameter space may be of lower accuracy than more sampled,\n",
    "    ! better behaved regions of the space.  The initial parameter ranges were\n",
    "    ! selected by David Richter during prototyping and aim to cover all\n",
    "    ! simulations of interest.\n",
    "    !\n",
    "    ! Things to note:\n",
    "    !\n",
    "    !   1. This approach was developed for code that would operate on a single\n",
    "    !      droplet at a time vs multiple droplets at once.  As a result the\n",
    "    !      application of a MLP network is simply a series of matrix-vector\n",
    "    !      multiplications, vector-vector additions, and an activation function\n",
    "    !      applied to the result.  We implement this case rather than attempt to\n",
    "    !      generalize as the performance gain is already non-trivial and it is\n",
    "    !      unclear when the code (if ever) would support batch estimation.\n",
    "    !\n",
    "    !   2. We could statically initialize each layer's weights and biases at\n",
    "    !      compile time, albeit at the expense of readability.  By separating\n",
    "    !      the weights and biases' definitions from their values we retain the\n",
    "    !      ability to understand the structure and internal components of the\n",
    "    !      MLP while requiring a single subroutine call at startup.\n",
    "    !\n",
    "    !   3. This is a naive implementation of a 4-layer MLP (it was written in 15\n",
    "    !      minutes!)  and does not implement all potential optimizations.  In\n",
    "    !      particular, each intermediate layer has its own array and no attempt\n",
    "    !      to reuse arrays or work out of one large array has been made, so as\n",
    "    !      to minimize the MLP's run-time footprint.  As initially developed the\n",
    "    !      MLP is very small (O(2500) parameters) and uses O(10 KiB) storage,\n",
    "    !      though should this ever be used for much larger MLPs, or with larger\n",
    "    !      batch sizes (see above) then some of the omitted optimizations should\n",
    "    !      be revisited.\n",
    "    !\n",
    "    !   4. While the MLP was trained on the product space of the input\n",
    "    !      parameters and has poor performance on certain (likely) physically\n",
    "    !      impossible parameter combinations, it is unclear what need to be done\n",
    "    !      to improve the situation.  Re-training on a subset of the input\n",
    "    !      parameters might improve performance and could be done without\n",
    "    !      changing this implementation (albeit with undefined behavior for any\n",
    "    !      physically impossible input) but it remains to be seen how much\n",
    "    !      accuracy would be gained from this.\n",
    "    !\n",
    "    !   5. This approach is *NOT* OpenMP safe and is inherently single-threaded!\n",
    "    !      Should multiple estimations need to be performed in parallel, an\n",
    "    !      additional interface is needed that provides the intermediate layers\n",
    "    !      to the estimation subroutine.  All of the weights and biases are\n",
    "    !      effectively read-only and can be shared across threads.  This is\n",
    "    !      straight forward to implement though it was not necessary for the\n",
    "    !      initial implementation.\n",
    "    !\n",
    "    !   6. Single precision floating point is used throughout the module as it\n",
    "    !      has been deemed to produce an acceptable level of accuracy for the\n",
    "    !      radius and temperature parameters.  The input and output arrays\n",
    "    !      must be real*4.\n",
    "    !\n",
    "    ! NOTE: This file was automatically generated from the the MLP's training\n",
    "    !       code!  Do not modify this unless you really know what you're doing!\n",
    "    !\n",
    "\n",
    "    ! Indices into the input and output vectors.  The input vector uses all of\n",
    "    ! the indices while the output vector only uses the first two.\n",
    "    integer, parameter :: RADIUS_INDEX          = 1\n",
    "    integer, parameter :: TEMPERATURE_INDEX     = 2\n",
    "    integer, parameter :: SALINITY_INDEX        = 3\n",
    "    integer, parameter :: AIR_TEMPERATURE_INDEX = 4\n",
    "    integer, parameter :: RH_INDEX              = 5\n",
    "    integer, parameter :: RHOA_INDEX            = 6\n",
    "    integer, parameter :: TFINAL_INDEX          = 7\n",
    "\n",
    "    integer, parameter :: NUMBER_INPUTS  = {number_inputs:d}\n",
    "    integer, parameter :: NUMBER_OUTPUTS = {number_outputs:d}\n",
    "\n",
    "    integer, parameter :: NUMBER_HIDDEN_LAYER1_NEURONS = {number_layer1_neurons:d}\n",
    "    integer, parameter :: NUMBER_HIDDEN_LAYER2_NEURONS = {number_layer2_neurons:d}\n",
    "    integer, parameter :: NUMBER_HIDDEN_LAYER3_NEURONS = {number_layer3_neurons:d}\n",
    "\n",
    "    ! The input variables must be normalized into the range of [-1, 1] before\n",
    "    ! feeding them into the model.  The following ranges, means, and widths are\n",
    "    ! used to map into, and from, the range of [-1, 1].\n",
    "    !\n",
    "    ! NOTE: These *must* match the training data!  Do not change these without\n",
    "    !       retraining the model!\n",
    "    !\n",
    "    real*4, parameter :: RADIUS_LOG_RANGE(2)      = [{radius_start:.1f}, {radius_end:.1f}]\n",
    "    real*4, parameter :: TEMPERATURE_RANGE(2)     = [{temperature_start:.1f}, {temperature_end:.1f}]\n",
    "    real*4, parameter :: SALINITY_LOG_RANGE(2)    = [{salinity_start:.1f}, {salinity_end:.1f}]\n",
    "    real*4, parameter :: AIR_TEMPERATURE_RANGE(2) = [{air_temperature_start:.1f}, {air_temperature_end:.1f}]\n",
    "    real*4, parameter :: RH_RANGE(2)              = [{rh_start:.2f}, {rh_end:.2f}]\n",
    "    real*4, parameter :: RHOA_RANGE(2)            = [{rhoa_start:.1f}, {rhoa_end:.1f}]\n",
    "\n",
    "    real*4, parameter :: RADIUS_LOG_MEAN      = SUM( RADIUS_LOG_RANGE ) / 2\n",
    "    real*4, parameter :: TEMPERATURE_MEAN     = SUM( TEMPERATURE_RANGE ) / 2\n",
    "    real*4, parameter :: SALINITY_LOG_MEAN    = SUM( SALINITY_LOG_RANGE ) / 2\n",
    "    real*4, parameter :: AIR_TEMPERATURE_MEAN = SUM( AIR_TEMPERATURE_RANGE ) / 2\n",
    "    real*4, parameter :: RH_MEAN              = SUM( RH_RANGE ) / 2\n",
    "    real*4, parameter :: RHOA_MEAN            = SUM( RHOA_RANGE ) / 2\n",
    "\n",
    "    !\n",
    "    ! NOTE: We have be careful about lines no longer than 132 characters so we\n",
    "    !       eliminate whitespace in the expressions.\n",
    "    !\n",
    "    real*4, parameter :: RADIUS_LOG_WIDTH      = (RADIUS_LOG_RANGE(2)-RADIUS_LOG_RANGE(1))/2\n",
    "    real*4, parameter :: TEMPERATURE_WIDTH     = (TEMPERATURE_RANGE(2)-TEMPERATURE_RANGE(1))/2\n",
    "    real*4, parameter :: SALINITY_LOG_WIDTH    = (SALINITY_LOG_RANGE(2)-SALINITY_LOG_RANGE(1))/2\n",
    "    real*4, parameter :: AIR_TEMPERATURE_WIDTH = (AIR_TEMPERATURE_RANGE(2)-AIR_TEMPERATURE_RANGE(1))/2\n",
    "    real*4, parameter :: RH_WIDTH              = (RH_RANGE(2)-RH_RANGE(1))/2\n",
    "    real*4, parameter :: RHOA_WIDTH            = (RHOA_RANGE(2)-RHOA_RANGE(1))/2\n",
    "\n",
    "    ! Weights for each of the layers.\n",
    "    real*4, dimension(NUMBER_INPUTS, NUMBER_HIDDEN_LAYER1_NEURONS)                :: layer1_weights\n",
    "    real*4, dimension(NUMBER_HIDDEN_LAYER1_NEURONS, NUMBER_HIDDEN_LAYER2_NEURONS) :: layer2_weights\n",
    "    real*4, dimension(NUMBER_HIDDEN_LAYER2_NEURONS, NUMBER_HIDDEN_LAYER3_NEURONS) :: layer3_weights\n",
    "    real*4, dimension(NUMBER_HIDDEN_LAYER3_NEURONS, NUMBER_OUTPUTS)               :: output_weights\n",
    "\n",
    "    ! Biases for each of the layers.\n",
    "    real*4, dimension(NUMBER_HIDDEN_LAYER1_NEURONS) :: layer1_biases\n",
    "    real*4, dimension(NUMBER_HIDDEN_LAYER2_NEURONS) :: layer2_biases\n",
    "    real*4, dimension(NUMBER_HIDDEN_LAYER3_NEURONS) :: layer3_biases\n",
    "    real*4, dimension(NUMBER_OUTPUTS)               :: output_biases\n",
    "\n",
    "    ! Arrays to hold the intermediate results between layers.\n",
    "    !\n",
    "    ! NOTE: We don't do anything fancy like detecting when we could reuse an\n",
    "    !       intermediate.\n",
    "    !\n",
    "    real*4, dimension(NUMBER_HIDDEN_LAYER1_NEURONS) :: layer1_intermediate\n",
    "    real*4, dimension(NUMBER_HIDDEN_LAYER2_NEURONS) :: layer2_intermediate\n",
    "    real*4, dimension(NUMBER_HIDDEN_LAYER3_NEURONS) :: layer3_intermediate\n",
    "\n",
    "    contains\n",
    "\"\"\".format(\n",
    "    # XXX: Double check the hidden layer neuron's sizes!  This was not\n",
    "    #      thoroughly tested since the target model had identical sizes\n",
    "    #      throughout.\n",
    "    model_name=model_name,\n",
    "    creation_date=current_date_time_str,\n",
    "    number_inputs=model_state[\"fc1.weight\"].shape[1],\n",
    "    number_outputs=model_state[\"fc4.weight\"].shape[0],\n",
    "    number_layer1_neurons=model_state[\"fc1.weight\"].shape[0],\n",
    "    number_layer2_neurons=model_state[\"fc2.weight\"].shape[0],\n",
    "    number_layer3_neurons=model_state[\"fc3.weight\"].shape[0],\n",
    "    radius_start=DROPLET_RADIUS_LOG_RANGE[0],\n",
    "    radius_end=DROPLET_RADIUS_LOG_RANGE[1],\n",
    "    temperature_start=DROPLET_TEMPERATURE_RANGE[0],\n",
    "    temperature_end=DROPLET_TEMPERATURE_RANGE[1],\n",
    "    salinity_start=DROPLET_SALINITY_LOG_RANGE[0],\n",
    "    salinity_end=DROPLET_SALINITY_LOG_RANGE[1],\n",
    "    air_temperature_start=DROPLET_AIR_TEMPERATURE_RANGE[0],\n",
    "    air_temperature_end=DROPLET_AIR_TEMPERATURE_RANGE[1],\n",
    "    rh_start=DROPLET_RELATIVE_HUMIDITY_RANGE[0],\n",
    "    rh_end=DROPLET_RELATIVE_HUMIDITY_RANGE[1],\n",
    "    rhoa_start=DROPLET_RHOA_RANGE[0],\n",
    "    rhoa_end=DROPLET_RHOA_RANGE[1]\n",
    "        )\n",
    "        \n",
    "        print( preamble_str, file=output_fp )\n",
    "\n",
    "    def write_model_initializaiton_routine( model_state, output_fp, indentation_str ):\n",
    "        \"\"\"\n",
    "        Writes the model's initialization routine to the supplied file handle with\n",
    "        a specific amount of indentation.\n",
    "        \n",
    "        Takes 3 arguments:\n",
    "        \n",
    "          model_state     - PyTorch model state dictionary for the model to initialize.\n",
    "          output_fp       - File handle to write to.\n",
    "          indentation_str - String prepended to each line of the subroutine written.\n",
    "          \n",
    "        Returns nothing.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        def write_model_weights( model_state, output_fp, indentation_str ):\n",
    "            \"\"\"\n",
    "            Internal routine that generates the weight initialization expressions for\n",
    "            all of the weights in the supplied model state.  This does not generate\n",
    "            initialization for biases nor does it generate a fully functional subroutine\n",
    "            (i.e. pre-amble/epilog).\n",
    "            \n",
    "            Takes 3 arguments:\n",
    "            \n",
    "              model_state     - PyTorch model state dictionary for the model to initialize.\n",
    "              output_fp       - File handle to write to.\n",
    "              indentation_str - String prepended to each line of the subroutine written.\n",
    "\n",
    "            Returns nothing.\n",
    "            \"\"\"\n",
    "            \n",
    "            # We rename each of the weights to Fortran-compatible symbols that are\n",
    "            # self-descriptive.\n",
    "            original_layer_names = expected_weights\n",
    "            new_layer_names      = [\"layer1_weights\",\n",
    "                                    \"layer2_weights\",\n",
    "                                    \"layer3_weights\",\n",
    "                                    \"output_weights\"]\n",
    "            \n",
    "            for original_layer_name, new_layer_name in zip( original_layer_names, new_layer_names ):\n",
    "                layer_parameters = model_state[original_layer_name]\n",
    "                \n",
    "                # Get the shape of the array while reversing the order from \n",
    "                # row-major (C, Python) to column-major (Fortran)\n",
    "                outer_size, inner_size = layer_parameters.shape\n",
    "                \n",
    "                # Start the array assignment to this weight's variable.\n",
    "                weights_definition_str = \"{:s}{:s} = reshape( [\".format(\n",
    "                    indentation_str,\n",
    "                    new_layer_name )\n",
    "                \n",
    "                # Create enough indentation so all of the weight's values are aligned\n",
    "                # at the first column after the open bracket.\n",
    "                weights_indentation_str = \" \" * len( weights_definition_str ) \n",
    "                \n",
    "                # Template to join successive weight's values together with.  Note that\n",
    "                # this is not applied to the last one.\n",
    "                definition_join_str    = \", &\\n{:s}\".format( weights_indentation_str )\n",
    "                \n",
    "                # Build the list of indented weight values, each printed with 10 digits\n",
    "                # after the decimal point, and aligned so that positive weights have a\n",
    "                # leading space to match negative weights' alignment.  We ignore the\n",
    "                # weight's shape since we'll reshape a 1D vector at run-time of the\n",
    "                # compiled Fortran executable.\n",
    "                #\n",
    "                # NOTE: This assumes the weights values' magnitudes are such that 10 digits\n",
    "                #       of precision is sufficient.\n",
    "                #\n",
    "                weights_values_str = definition_join_str.join( \n",
    "                    map( lambda x: \"{: .10f}\".format( x ),\n",
    "                         layer_parameters.cpu().numpy().ravel() ) )\n",
    "\n",
    "                # Create the dimensions array supplied to reshape(), as well as the closing\n",
    "                # parenthsis.\n",
    "                weights_reshape_dimensions_str = \"[{:d}, {:d}] )\".format(\n",
    "                    inner_size,\n",
    "                    outer_size )\n",
    "\n",
    "                print( \"{:s}{:s}], &\\n{:s}{:s}\".format(\n",
    "                    weights_definition_str,\n",
    "                    weights_values_str,\n",
    "                    weights_indentation_str,\n",
    "                    weights_reshape_dimensions_str ),\n",
    "                      file=output_fp )\n",
    "\n",
    "        def write_model_biases( model_state, output_fp, indentation_str ):\n",
    "            \"\"\"\n",
    "            Internal routine that generates the biases initialization expressions for\n",
    "            all of the biases in the supplied model state.  This does not generate\n",
    "            initialization for weights nor does it generate a fully functional subroutine\n",
    "            (i.e. pre-amble/epilog).\n",
    "            \n",
    "            Takes 3 arguments:\n",
    "            \n",
    "              model_state     - PyTorch model state dictionary for the model to initialize.\n",
    "              output_fp       - File handle to write to.\n",
    "              indentation_str - String prepended to each line of the subroutine written.\n",
    "\n",
    "            Returns nothing.\n",
    "            \"\"\"\n",
    "            \n",
    "            # We rename each of the biases to Fortran-compatible symbols that are\n",
    "            # self-descriptive.\n",
    "            original_layer_names = expected_biases\n",
    "            new_layer_names      = [\"layer1_biases\",\n",
    "                                    \"layer2_biases\",\n",
    "                                    \"layer3_biases\",\n",
    "                                    \"output_biases\"]\n",
    "            \n",
    "            for original_layer_name, new_layer_name in zip( original_layer_names, new_layer_names ):\n",
    "                layer_parameters = model_state[original_layer_name]\n",
    "                \n",
    "                # Start the array assignment to this bias' variable.\n",
    "                biases_definition_str = \"{:s}{:s} = [\".format(\n",
    "                    indentation_str,\n",
    "                    new_layer_name )\n",
    "                \n",
    "                # Create enough indentation so all of the bias values are aligned\n",
    "                # at the first column after the open bracket.\n",
    "                biases_indentation_str = \" \" * len( biases_definition_str ) \n",
    "                \n",
    "                # Template to join successive bias values together with.  Note that\n",
    "                # this is not applied to the last one.\n",
    "                definition_join_str    = \", &\\n{:s}\".format( biases_indentation_str )\n",
    "                \n",
    "                # Build the list of indented bias values, each printed with 10 digits\n",
    "                # after the decimal point, and aligned so that positive biases have a\n",
    "                # leading space to match negative biases' alignment.\n",
    "                #\n",
    "                # NOTE: This assumes the bias values' magnitudes are such that 10 digits\n",
    "                #       of precision is sufficient.\n",
    "                #\n",
    "                bias_values_str = definition_join_str.join( \n",
    "                    map( lambda x: \"{: .10f}\".format( x ),\n",
    "                         layer_parameters.cpu().numpy() ) )\n",
    "                \n",
    "                print( \"{:s}{:s}]\".format(\n",
    "                    biases_definition_str,\n",
    "                    bias_values_str,\n",
    "                    biases_indentation_str ),\n",
    "                      file=output_fp )\n",
    "        \n",
    "        # Our initialization routine is inside of a \"contains\" block so we're\n",
    "        # indented one level deeper than the caller.\n",
    "        inner_indentation_str = \"    {:s}\".format( indentation_str )\n",
    "        \n",
    "        # Prolog and epilog of our initialization routine, along with extra\n",
    "        # newlines so we survive the application of .splitlines() below.\n",
    "        initialization_prolog_str = \"subroutine initialize_model()\\n\\n    implicit none\\n\\n\"\n",
    "        initialization_epilog_str = \"\\nend subroutine initialize_model\"\n",
    "        \n",
    "        # Write the subroutine's prolog.\n",
    "        for initialization_line in initialization_prolog_str.splitlines():\n",
    "            print( \"{:s}{:s}\".format(\n",
    "                indentation_str if len( initialization_line ) > 0 else \"\",\n",
    "                initialization_line ),\n",
    "                file=output_fp )\n",
    "            \n",
    "        # Write the weights and biases separated by an empty line.\n",
    "        write_model_weights( model_state, output_fp, inner_indentation_str )\n",
    "        print( \"\", file=output_fp )\n",
    "        write_model_biases( model_state, output_fp, inner_indentation_str )\n",
    "        \n",
    "        # Write the subroutine's epilog.\n",
    "        for initialization_line in initialization_epilog_str.splitlines():\n",
    "            print( \"{:s}{:s}\".format(\n",
    "                indentation_str if len( initialization_line ) > 0 else \"\",\n",
    "                initialization_line ),\n",
    "                file=output_fp )\n",
    "\n",
    "    \n",
    "    def write_model_inference_routine( model_state, output_fp, indentation_str ):\n",
    "        \"\"\"\n",
    "        Writes the model's inference routine to the supplied file handle with\n",
    "        a specific amount of indentation.\n",
    "        \n",
    "        Takes 3 arguments:\n",
    "        \n",
    "          model_state     - Unused argument kept for a consistent signature.\n",
    "          output_fp       - File handle to write to.\n",
    "          indentation_str - String prepended to each line of the subroutine written.\n",
    "\n",
    "        Returns nothing.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        inference_str = \"\"\"\n",
    "! Estimates a droplet's future size and temperature based on it's current size\n",
    "! and temperature, as well as key parameters from the environment its in.\n",
    "! Inputs and outputs are provided/returned in un-normalized physical scales.\n",
    "subroutine estimate( input, output )\n",
    "\n",
    "    implicit none\n",
    "\n",
    "    !\n",
    "    ! NOTE: This is hardcoded to assume a batch size of 1 so we can have\n",
    "    !       a 1D array vs dealing with a 2D array with a singleton\n",
    "    !       dimension.\n",
    "    !\n",
    "    real*4, dimension(NUMBER_INPUTS), intent(in)   :: input\n",
    "    real*4, dimension(NUMBER_OUTPUTS), intent(out) :: output\n",
    "\n",
    "    real*4, dimension(NUMBER_INPUTS)               :: normalized_input\n",
    "    integer                                        :: output_index\n",
    "\n",
    "    ! Normalize the non-temporal inputs so they're in the range [-1, 1].\n",
    "    normalized_input(RADIUS_INDEX)          = (log10( input(RADIUS_INDEX) ) - RADIUS_LOG_MEAN) / RADIUS_LOG_WIDTH\n",
    "    normalized_input(TEMPERATURE_INDEX)     = (input(TEMPERATURE_INDEX) - TEMPERATURE_MEAN) / TEMPERATURE_WIDTH\n",
    "    normalized_input(SALINITY_INDEX)        = (log10( input(SALINITY_INDEX) ) - SALINITY_LOG_MEAN) / SALINITY_LOG_WIDTH\n",
    "    normalized_input(AIR_TEMPERATURE_INDEX) = (input(AIR_TEMPERATURE_INDEX) - AIR_TEMPERATURE_MEAN) / AIR_TEMPERATURE_WIDTH\n",
    "    normalized_input(RH_INDEX)              = (input(RH_INDEX) - RH_MEAN) / RH_WIDTH\n",
    "    normalized_input(RHOA_INDEX)            = (input(RHOA_INDEX) - RHOA_MEAN) / RHOA_WIDTH\n",
    "\n",
    "    ! Our integration time remains as is.\n",
    "    normalized_input(TFINAL_INDEX)          = input(TFINAL_INDEX)\n",
    "\n",
    "    ! Compute x_1 = ReLU( W_1*I + b_1 ).\n",
    "    do output_index = 1, NUMBER_HIDDEN_LAYER1_NEURONS\n",
    "        layer1_intermediate(output_index) = &\n",
    "             max( sum( normalized_input(:) * layer1_weights(:, output_index) ) + layer1_biases(output_index), 0.0 )\n",
    "    end do\n",
    "\n",
    "    ! Compute x_2 = ReLU( W_2*x_1 + b_2 ).\n",
    "    do output_index = 1, NUMBER_HIDDEN_LAYER2_NEURONS\n",
    "        layer2_intermediate(output_index) = &\n",
    "             max( sum( layer1_intermediate(:) * layer2_weights(:, output_index) ) + layer2_biases(output_index), 0.0 )\n",
    "    end do\n",
    "\n",
    "    ! Compute x_3 = ReLU( W_3*x_2 + b_3 ).\n",
    "    do output_index = 1, NUMBER_HIDDEN_LAYER3_NEURONS\n",
    "        layer3_intermediate(output_index) = &\n",
    "             max( sum( layer2_intermediate(:) * layer3_weights(:, output_index) ) + layer3_biases(output_index), 0.0 )\n",
    "    end do\n",
    "\n",
    "    ! Compute O = W_4*x_3 + b_4.\n",
    "    do output_index = 1, NUMBER_OUTPUTS\n",
    "        output(output_index) = sum( layer3_intermediate(:) * output_weights(:, output_index) ) + output_biases(output_index)\n",
    "    end do\n",
    "\n",
    "    ! Scale the outputs to the expected ranges.\n",
    "    output(RADIUS_INDEX)      = 10.0**(output(RADIUS_INDEX) * RADIUS_LOG_WIDTH + RADIUS_LOG_MEAN)\n",
    "    output(TEMPERATURE_INDEX) = output(TEMPERATURE_INDEX) * TEMPERATURE_WIDTH + TEMPERATURE_MEAN\n",
    "\n",
    "end subroutine estimate\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "        for inference_line in inference_str.splitlines():\n",
    "            print( \"{:s}{:s}\".format( \n",
    "                indentation_str if len( inference_line ) > 0 else \"\",\n",
    "                inference_line ),\n",
    "                   file=output_fp )\n",
    "    \n",
    "    def write_module_epilog( model_state, output_fp ):\n",
    "        \"\"\"\n",
    "        Writes the module's epilog to the supplied file handle.\n",
    "        \n",
    "        Takes 2 arguments:\n",
    "        \n",
    "          model_state     - Unused argument kept for a consistent signature.\n",
    "          output_fp       - File handle to write to.\n",
    "          \n",
    "        Returns nothing.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        epilog_str = \"end module droplet_model\"\n",
    "\n",
    "        print( epilog_str, file=output_fp )\n",
    "    \n",
    "    # Module subroutines, inside of a \"contains\" block, are indented twice.\n",
    "    indentation_str = \" \" * 8\n",
    "    \n",
    "    with open( output_path, \"w\" ) as output_fp:\n",
    "        # Write out the beginning of the module.  This includes the definitions for\n",
    "        # the layers' weights and biases but not the method that initializes them.\n",
    "        write_module_prolog( model_name, model_state, output_fp )\n",
    "\n",
    "        # Write out both of the subroutines that comprise this model.\n",
    "        write_model_initializaiton_routine( model_state, output_fp, indentation_str )\n",
    "        write_model_inference_routine( model_state, output_fp, indentation_str )\n",
    "        \n",
    "        # Write out the end of the module.\n",
    "        write_module_epilog( model_state, output_fp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42311d19",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Overwrite the module used by NTLP so future simulations use the current weights.\n",
    "if write_weights_flag:\n",
    "    \n",
    "    # Pick the most descriptive name we have so researchers have an idea where\n",
    "    # the weights came from.\n",
    "    if load_model_flag:\n",
    "        exported_model_name = model_load_path.split( \"/\" )[-1]\n",
    "    else:\n",
    "        exported_model_name = model_name\n",
    "    \n",
    "    exported_file_path = \"../../droplet_model.f90\"\n",
    "    \n",
    "    generate_fortran_module( exported_model_name,\n",
    "                             model.state_dict(),\n",
    "                             exported_file_path )\n",
    "    \n",
    "    print( \"Wrote model weights and inferencing code to '{:s}'.\".format(\n",
    "        exported_file_path ) )\n",
    "else:\n",
    "    print( \"Skipped writing model weights.\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
