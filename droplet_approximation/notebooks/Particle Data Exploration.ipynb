{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a52040a7",
   "metadata": {},
   "source": [
    "# Particle Data Exploration\n",
    "This notebook serves to explore a particle dataset traced from an NTLP simulation.\n",
    "\n",
    "Goals include:\n",
    "- Understanding ranges of data and their distributions\n",
    "- Identifying invalid data and outliers, including BE failures and cold particles\n",
    "- Visualizing particle events as reported via NTLP's `history.nc` NetCDF file\n",
    "- Visualizing particle behavior and its environment throughout their lifetimes\n",
    "- Resampling particles onto a common timeline so their characteristics/environment are comparable\n",
    "\n",
    "Provides configuration for the following datasets:\n",
    "- Pi chamber - 1-way coupling, relative humidity set at 103%\n",
    "- Pi chamber - 2-way coupling, normal relative humidity\n",
    "- cfog\n",
    "- Fatima\n",
    "- Spray\n",
    "\n",
    "Unless otherwise noted, the datasets were traced with 2-way coupling.  Additional datasets are easily added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f749f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import glob\n",
    "import multiprocessing\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# To compute the mode of out parameters.\n",
    "from scipy import stats\n",
    "\n",
    "import droplet_approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c45893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the top-level simulations/ data directory.\n",
    "#\n",
    "# NOTE: This is set to an invalid value as there is no way to set a sensible default.\n",
    "#       The next cell halts execution if it's not set.\n",
    "#\n",
    "simulations_data_root = None\n",
    "\n",
    "# Name of the simulation we're investigating.\n",
    "#\n",
    "# NOTE: This must match one of the names in the next cell!\n",
    "#\n",
    "simulation_name = \"Pi Chamber - 1-way Coupling, RH~103%\"\n",
    "#simulation_name = \"Pi Chamber - 2-way Coupling\"\n",
    "#simulation_name = \"cfog\"\n",
    "#simulation_name = \"Fatima\"\n",
    "#simulation_name = \"Spray\"\n",
    "\n",
    "# Number of processes to use for parallel operations.  Zero means use one process\n",
    "# per core on the system.\n",
    "number_processes = 0\n",
    "\n",
    "# Use a large fanout degree for our raw particle files directory hierarchy.\n",
    "#\n",
    "# NOTE: This must match how the particle directories were constructed.  Do *not* change\n",
    "#       this unless you've updated the common datasets as well.\n",
    "#\n",
    "dirs_per_level   = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999c400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we know where our data resides, otherwise stop execution.\n",
    "if simulations_data_root is None:\n",
    "    raise ValueError( \"Must set simulations_data_root to run this notebook!\" )\n",
    "\n",
    "# Do we need to default the number of processes to use?\n",
    "if number_processes == 0:\n",
    "    number_processes = os.cpu_count()\n",
    "    \n",
    "# Map the simulation name to its directory beneath the simulations data root.\n",
    "if simulation_name == \"Pi Chamber - 1-way Coupling, RH~103%\":\n",
    "    simulation_directory_name = \"pi_chamber-1way_rh103\"\n",
    "elif simulation_name == \"Pi Chamber - 2-way Coupling\":\n",
    "    simulation_directory_name = \"pi_chamber-2way\"\n",
    "elif simulation_name == \"cfog\":\n",
    "    simulation_directory_name = \"cfog\"\n",
    "elif simulation_name == \"Fatima\":\n",
    "    simulation_directory_name = \"fatima\"\n",
    "elif simulation_name == \"Spray\":\n",
    "    simulation_directory_name = \"spray\"\n",
    "else:\n",
    "    raise ValueError( \"Unknown simulation_name!\" )\n",
    "\n",
    "# Top-level directory of this simulation.\n",
    "simulation_root = \"{:s}/{:s}/\".format( simulations_data_root, simulation_directory_name )\n",
    "\n",
    "# Path to the top of the raw particle files directory hierarchy and its index.\n",
    "particles_root       = \"{:s}/particles\".format( simulation_root )\n",
    "particles_index_path = \"{:s}/particles.index\".format( particles_root )\n",
    "\n",
    "# Path to the NTLP particle trace root.\n",
    "ntlp_trace_root = simulation_root\n",
    "\n",
    "# Path to the NTLP history NetCDF file.\n",
    "ntlp_history_path = \"{:s}/history.nc\".format( simulation_root )\n",
    "\n",
    "# List of NTLP particle trace paths.\n",
    "ntlp_trace_paths = glob.glob( os.path.join( ntlp_trace_root, \"be_dump_*.data\" ) )\n",
    "\n",
    "print( \"Investigating '{:s}'.  Key directories are:\\n\"\n",
    "       \"\\n\"\n",
    "       \"    Top-level:         {:s}\\n\"\n",
    "       \"    Particles:         {:s}\\n\"\n",
    "       \"    Particles index:   {:s}\\n\"\n",
    "       \"    NTLP traces:       {:s}\\n\"\n",
    "       \"    NTLP history.nc:   {:s}\\n\"\n",
    "       \"\\n\"\n",
    "       \"{:d} NTLP trace{:s} found.\".format(\n",
    "           simulation_name,\n",
    "           simulation_directory_name,\n",
    "           particles_root,\n",
    "           particles_index_path,\n",
    "           ntlp_trace_root,\n",
    "           ntlp_history_path,\n",
    "           len( ntlp_trace_paths ),\n",
    "           \"\" if len( ntlp_trace_paths ) == 1 else \"s\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6fc9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_floats_from_binary_file( file_path ):\n",
    "    \"\"\"\n",
    "    Reads 36-byte binary records from a file into a 2D NumPy array.\n",
    "    Suitable for debugging.\n",
    "    \n",
    "    Takes 1 argument:\n",
    "    \n",
    "      file_path - Path to the binary file to read.\n",
    "      \n",
    "    Returns 1 value:\n",
    "    \n",
    "      floats_array - NumPy array, shaped N x 9 where N is the number of records, containing\n",
    "                     the data in file_path interpreted as 32-bit floating point values.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read all data as 32-bit floats\n",
    "    data = np.fromfile( file_path, dtype=np.float32 )\n",
    "\n",
    "    # Reshape to records of 9 elements (2 integers + 7 floats).\n",
    "    records = data.reshape( -1, 9 )\n",
    "\n",
    "    # Extract only the last 7 columns (floats).\n",
    "    floats_array = records[:, 2:]\n",
    "\n",
    "    return floats_array\n",
    "\n",
    "def flag_invalid_data( df_row ):\n",
    "    \"\"\"\n",
    "    Flags invalid data in a particles DataFrame.  XXX: be more descriptive\n",
    "    \n",
    "    This function is intended to be applied to a particles DataFrame.\n",
    "    \n",
    "    Takes 1 argument:\n",
    "    \n",
    "      df_row - Particles DataFrame with one row to examine for invalid values.\n",
    "      \n",
    "    Returns 1 value:\n",
    "    \n",
    "      new_columns - Pandas Series containing the newly created columns representing\n",
    "                    invalid data in df_row.  The Series does not name the columns\n",
    "                    but contain the following:\n",
    "                    \n",
    "\n",
    "                      1. Invalid input BE radii\n",
    "                      2. Invalid input BE particle temperatures\n",
    "                      3. Invalid salt masses\n",
    "                      4. Invalid air temperatures\n",
    "                      5. Invalid relative humidities\n",
    "                      6. Invalid air densities\n",
    "                      7. Invalid integration times\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    invalid_input_radius_flag      = (np.any( df_row[\"input be radii\"] <= 0.0 ) or\n",
    "                                      np.any( df_row[\"input be radii\"] >= 1e-2 ))\n",
    "    invalid_input_temperature_flag = (np.any( df_row[\"input be temperatures\"] <= 270.0 ) or\n",
    "                                      np.any( df_row[\"input be temperatures\"] >= 320.0 ))\n",
    "    invalid_salt_flag              = (np.any( np.log10( df_row[\"salt masses\"] ) < -22 ) or\n",
    "                                      np.any( np.log10( df_row[\"salt masses\"] ) > -10))\n",
    "    invalid_air_temperature_flag   = (np.any( df_row[\"air temperatures\"] <= 270.0 ) or\n",
    "                                      np.any( df_row[\"air temperatures\"] >= 320.0 ))\n",
    "    invalid_rh_flag                = (np.any( df_row[\"relative humidities\"] < 0.85 ) or\n",
    "                                      np.any( df_row[\"relative humidities\"] > 1.15 ))\n",
    "    invalid_air_density_flag       = (np.any( df_row[\"air densities\"] < 0.8 ) or\n",
    "                                      np.any( df_row[\"air densities\"] > 1.2 ))\n",
    "    invalid_dt_flag                = np.any( df_row[\"integration times\"] <= 0.0 )\n",
    "    \n",
    "    new_columns = pd.Series( [invalid_input_radius_flag,\n",
    "                              invalid_input_temperature_flag,\n",
    "                              invalid_salt_flag,                            \n",
    "                              invalid_air_temperature_flag,\n",
    "                              invalid_rh_flag,\n",
    "                              invalid_air_density_flag,\n",
    "                              invalid_dt_flag],\n",
    "                             index=flag_column_names )\n",
    "    \n",
    "    return new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c439811",
   "metadata": {},
   "source": [
    "# Reading Particle Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd87786",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_particle_ids = np.fromfile( particles_index_path, dtype=np.int32 )\n",
    "\n",
    "parallel_read_flag = True\n",
    "\n",
    "if parallel_read_flag:\n",
    "    particles_df = droplet_approximation.batch_read_particles_data( particles_root,\n",
    "                                                                    unique_particle_ids,\n",
    "                                                                    dirs_per_level )\n",
    "else:\n",
    "    particles_df = droplet_approximation.read_particles_data( particles_root,\n",
    "                                                              unique_particle_ids,\n",
    "                                                              dirs_per_level )\n",
    "\n",
    "print( \"{:d} particles in the DataFrame.\".format( \n",
    "    len( particles_df ) ) )\n",
    "print( \"{:d} total observations.\".format( \n",
    "    particles_df[\"number observations\"].sum() ))\n",
    "print( \"Last observation at {:.2f} seconds.\".format(\n",
    "    particles_df[\"death time\"].max() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad0796f",
   "metadata": {},
   "source": [
    "## Identify the Data Ranges\n",
    "Show the ranges for each of the droplet parameters based on all of the particles' observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbedd346",
   "metadata": {},
   "outputs": [],
   "source": [
    "radius_min            = np.log10( particles_df[\"input be radii\"].apply( np.min ).min() )\n",
    "radius_max            = np.log10( particles_df[\"input be radii\"].apply( np.max ).max() )\n",
    "temperature_min       = particles_df[\"input be temperatures\"].apply( np.min ).min()\n",
    "temperature_max       = particles_df[\"input be temperatures\"].apply( np.max ).max()\n",
    "salt_mass_min         = np.log10( particles_df[\"salt masses\"].apply( np.min ).min() )\n",
    "salt_mass_max         = np.log10( particles_df[\"salt masses\"].apply( np.max ).max() )\n",
    "air_temperature_min   = particles_df[\"air temperatures\"].apply( np.min ).min()\n",
    "air_temperature_max   = particles_df[\"air temperatures\"].apply( np.max ).max()\n",
    "relative_humidity_min = particles_df[\"relative humidities\"].apply( np.min ).min() * 100\n",
    "relative_humidity_max = particles_df[\"relative humidities\"].apply( np.max ).max() * 100\n",
    "air_density_min       = particles_df[\"air densities\"].apply( np.min ).min()\n",
    "air_density_max       = particles_df[\"air densities\"].apply( np.max ).max()\n",
    "dt_min                = np.log10( particles_df[\"integration times\"].apply( np.min ).min() )\n",
    "dt_max                = np.log10( particles_df[\"integration times\"].apply( np.max ).max() )\n",
    "\n",
    "# Display the ranges in a human-readable form.\n",
    "print( \"Data ranges:\\n\" \n",
    "       \"\\n\"\n",
    "       \"  log10(Radius)       [{:.2f}, {:.2f}] m\\n\"\n",
    "       \"  Temperature:        [{:.2f}, {:.2f}] K\\n\"\n",
    "       \"  log10(Salt mass):   [{:.2f}, {:.2f}] kg/m^3\\n\"\n",
    "       \"  Air temperature:    [{:.2f}, {:.2f}] K\\n\"\n",
    "       \"  Relative humidity:  [{:.2f}, {:.2f}] %\\n\"\n",
    "       \"  Air density:        [{:.2f}, {:.2f}] kg/m^3\\n\"\n",
    "       \"  log10(dt):          [{:.2f}, {:.2f}] s\".format(\n",
    "           radius_min, radius_max,\n",
    "           temperature_min, temperature_max,\n",
    "           salt_mass_min, salt_mass_max,\n",
    "           air_temperature_min, air_temperature_max,\n",
    "           relative_humidity_min, relative_humidity_max,\n",
    "           air_density_min, air_density_max,\n",
    "           dt_min, dt_max ) )\n",
    "\n",
    "# Now display them in a way that can be pasted into code.\n",
    "print()\n",
    "print( \"parameter_ranges = {{\\n\"\n",
    "       \"    \\\"radius\\\":             ({:f}, {:f}),\\n\"\n",
    "       \"    \\\"temperature\\\":        ({:f}, {:f}),\\n\"\n",
    "       \"    \\\"salt_mass\\\":          ({:f}, {:f}),\\n\"\n",
    "       \"    \\\"air_temperature\\\":    ({:f}, {:f}),\\n\"\n",
    "       \"    \\\"relative_humidity\\\":  ({:f}, {:f}),\\n\"\n",
    "       \"    \\\"rhoa\\\":               ({:f}, {:f}),\\n\"\n",
    "       \"    \\\"time\\\":               ({:f}, {:f})\\n\"\n",
    "       \"}}\".format(\n",
    "           radius_min, radius_max,\n",
    "           temperature_min, temperature_max,\n",
    "           salt_mass_min, salt_mass_max,\n",
    "           air_temperature_min, air_temperature_max,\n",
    "           relative_humidity_min, relative_humidity_max,\n",
    "           air_density_min, air_density_max,\n",
    "           dt_min, dt_max ) )\n",
    "\n",
    "# Finally, display them in a format that can be pasted into the data generation script.\n",
    "print()\n",
    "print( \"For use with generate_training_data.py:\\n\" )\n",
    "print( \"{:f}:{:f},{:f}:{:f},{:f}:{:f},{:f}:{:f},{:f}:{:f},{:f}:{:f},{:f}:{:f}\".format(\n",
    "           radius_min, radius_max,\n",
    "           temperature_min, temperature_max,\n",
    "           salt_mass_min, salt_mass_max,\n",
    "           air_temperature_min, air_temperature_max,\n",
    "           relative_humidity_min, relative_humidity_max,\n",
    "           air_density_min, air_density_max,\n",
    "           dt_min, dt_max ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe74f276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the number of observations per particle so we can get an idea of\n",
    "# how long they live.\n",
    "fig_h, ax_h = plt.subplots( 1, 1, figsize=(10, 6) )\n",
    "\n",
    "mean_number_observations   = particles_df[\"number observations\"].mean()\n",
    "median_number_observations = np.median( particles_df[\"number observations\"] )\n",
    "max_particle_id            = particles_df.index.max()\n",
    "\n",
    "ax_h.plot( particles_df[\"number observations\"], \".\", label=\"Observations\" )\n",
    "ax_h.plot( [0, max_particle_id],\n",
    "           [mean_number_observations, mean_number_observations],\n",
    "           \"-.\",\n",
    "           label=\"Mean\" )\n",
    "ax_h.plot( [0, max_particle_id],\n",
    "           [median_number_observations, median_number_observations],\n",
    "           \"-\",\n",
    "           label=\"Median\" )\n",
    "\n",
    "ax_h.set_title( \"Observations per Particle\\n\"\n",
    "                \"{:d} Particles\".format(\n",
    "                len( particles_df ) ) )\n",
    "ax_h.set_xlabel( \"Particle Identifier\" )\n",
    "ax_h.set_ylabel( \"Number Observations\" )\n",
    "ax_h.legend( loc=\"upper right\" )\n",
    "\n",
    "fig_h.tight_layout()\n",
    "\n",
    "print( \"Number of observations' statistics:\\n\"\n",
    "       \"\\n\"\n",
    "       \"  [min, max]:  [{:d}, {:d}]\\n\"\n",
    "       \"  Mean:        {:d}\\n\"\n",
    "       \"  Median:      {:d}\\n\".format(\n",
    "           particles_df[\"number observations\"].min(),\n",
    "           particles_df[\"number observations\"].max(),\n",
    "           int( particles_df[\"number observations\"].mean() ),\n",
    "           int( particles_df[\"number observations\"].median() ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b53f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report statistics on particle life times in simulation time.\n",
    "\n",
    "particles_duration = particles_df.apply( lambda x: x[\"death time\"] - x[\"birth time\"], axis=1 )\n",
    "\n",
    "simulation_end = particles_df[\"death time\"].max()\n",
    "\n",
    "number_surviving_particles = (particles_df[\"death time\"] == simulation_end).sum()\n",
    "\n",
    "print( \"Particle life times span [{:.2f}s, {:.2f}s] with a mean of {:.2f}s and median of {:.2f}s\".format(\n",
    "    particles_duration.min(),\n",
    "    particles_duration.max(),\n",
    "    particles_duration.mean(),\n",
    "    particles_duration.median() ) )\n",
    "\n",
    "print( \"{:d} particle{:s} ({:.2f}%) survived to the end of the simulation ({:.2f}s).\".format(\n",
    "    number_surviving_particles,\n",
    "    \"\" if number_surviving_particles == 1 else \"s\",\n",
    "    number_surviving_particles / len( particles_df ) * 100,\n",
    "    simulation_end ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd93b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_column_names = [\"invalid input be radius\",\n",
    "                     \"invalid input be temperature\",\n",
    "                     \"invalid salt mass\",\n",
    "                     \"invalid air temperature\",\n",
    "                     \"invalid relative humidity\",\n",
    "                     \"invalid air density\",\n",
    "                     \"invalid dt\"]\n",
    "\n",
    "# Flag particles based on the types of invalid observations they contain.\n",
    "particles_df[flag_column_names] = particles_df.apply( flag_invalid_data, axis=1 )\n",
    "\n",
    "number_particles = len( particles_df )\n",
    "\n",
    "number_invalid_radius          = particles_df[\"invalid input be radius\"].sum()\n",
    "number_invalid_temperature     = particles_df[\"invalid input be temperature\"].sum()\n",
    "number_invalid_salt_mass       = particles_df[\"invalid salt mass\"].sum()\n",
    "number_invalid_air_temperature = particles_df[\"invalid air temperature\"].sum()\n",
    "number_invalid_rh              = particles_df[\"invalid relative humidity\"].sum()\n",
    "number_invalid_air_density     = particles_df[\"invalid air density\"].sum()\n",
    "number_invalid_dt              = particles_df[\"invalid dt\"].sum()\n",
    "\n",
    "# Report what we found.\n",
    "print( \"Invalid counts for {:d} particles:\\n\"\n",
    "       \"\\n\"\n",
    "       \"    input radius:           {:d} ({:.2f}%)\\n\"\n",
    "       \"    input temperature:      {:d} ({:.2f}%)\\n\"\n",
    "       \"    input salt mass:        {:d} ({:.2f}%)\\n\"\n",
    "       \"    input air temperature:  {:d} ({:.2f}%)\\n\"\n",
    "       \"    input rh:               {:d} ({:.2f}%)\\n\"\n",
    "       \"    input air density:      {:d} ({:.2f}%)\\n\"\n",
    "       \"    dt:                     {:d} ({:.2f}%)\\n\"\n",
    "       .format(\n",
    "           number_particles,\n",
    "           number_invalid_radius,          number_invalid_radius / number_particles * 100.0,\n",
    "           number_invalid_temperature,     number_invalid_temperature / number_particles * 100.0,\n",
    "           number_invalid_salt_mass,       number_invalid_salt_mass / number_particles * 100.0,\n",
    "           number_invalid_air_temperature, number_invalid_air_temperature / number_particles * 100.0,\n",
    "           number_invalid_rh,              number_invalid_rh / number_particles * 100.0,\n",
    "           number_invalid_air_density,     number_invalid_air_density / number_particles * 100.0,\n",
    "           number_invalid_dt,              number_invalid_dt / number_particles * 100.0\n",
    "       ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f548f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( particles_df[particles_df[\"invalid input be radius\"]][\"input be radii\"].apply( np.min ) )\n",
    "print( particles_df[particles_df[\"invalid input be temperature\"]][\"input be temperatures\"].apply( np.min ) )\n",
    "print( particles_df[particles_df[\"invalid salt mass\"]][\"salt masses\"].apply( np.min ) )\n",
    "print( particles_df[particles_df[\"invalid air temperature\"]][\"air temperatures\"].apply( np.min ) )\n",
    "print( particles_df[particles_df[\"invalid relative humidity\"]][\"relative humidities\"].apply( np.min ) )\n",
    "print( particles_df[particles_df[\"invalid air density\"]][\"air densities\"].apply( np.min ) )\n",
    "print( particles_df[particles_df[\"invalid dt\"]][\"integration times\"].apply( np.min ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17875ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot backward Euler errors as a function of simulation time so we can see structure\n",
    "# in the failures.  Particles with multiple failures will stand out, as will systemic\n",
    "# failures shortly after particle creation.\n",
    "\n",
    "def get_failure_times_by_id( row ):\n",
    "    # Only look at observations where BE failed.\n",
    "    be_mask          = (row[\"be statuses\"] > 0)\n",
    "    \n",
    "    # Compute the particle's entire timeline and take the subset where BE failed.\n",
    "    simulation_times = (row[\"birth time\"] + np.cumsum( row[\"integration times\"] ) - row[\"integration times\"][0])[be_mask]\n",
    "    \n",
    "    # Replicate the particle's identifier once per BE failure.\n",
    "    particle_id      = row.name * np.ones( (1, be_mask.sum()), dtype=np.int32 )\n",
    "\n",
    "    # Create an array sized number_failures x 2.\n",
    "    return np.vstack( [simulation_times, particle_id] ).T\n",
    "    \n",
    "# How many particles don't have the be flag set?\n",
    "be_failure_mask     = (particles_df[\"number be failures\"] > 0)\n",
    "number_be_particles = be_failure_mask.sum()\n",
    "\n",
    "# Get the simulation times and particle identifiers for BE failures.\n",
    "failures_df   = particles_df[be_failure_mask]\n",
    "times_and_ids = np.vstack( failures_df.apply( get_failure_times_by_id, axis=1 ) )\n",
    "\n",
    "fig_h, ax_h = plt.subplots( 1, 1, figsize=(6, 6) )\n",
    "\n",
    "ax_h.plot( times_and_ids[:, 0],\n",
    "           times_and_ids[:, 1],\n",
    "           \".\" )\n",
    "ax_h.set_ylabel( \"Particle ID\" )\n",
    "ax_h.set_xlabel( \"Time (s)\" )\n",
    "ax_h.set_title( \"{:d} BE Failures Across {:d} Particles\\n\"\n",
    "                \"{:.2f}% Particles Had a Failure\".format( \n",
    "    times_and_ids.shape[0],\n",
    "    number_be_particles,\n",
    "    number_be_particles / len( particles_df ) * 100 ) )\n",
    "\n",
    "fig_h.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be748c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only a small fraction of particles will fail BE and, of those, the vast\n",
    "# majority fail once.  We construct our bins such that we always have a bin\n",
    "# covering one failure and then distribute the remaining bins across the\n",
    "# rest of the data.\n",
    "bin1_end = 1\n",
    "\n",
    "# Get a largish number of bins so we can see the shape of the failure\n",
    "# distribution.\n",
    "number_bins = 100\n",
    "\n",
    "# Get a convenience variable for the per-particle BE failure counts.\n",
    "number_be_failures = failures_df[\"number be failures\"] \n",
    "\n",
    "# We always have a bin from [0, bin1_end].  Construct the remaining\n",
    "# bin edges.\n",
    "remaining_data = number_be_failures[number_be_failures > bin1_end]\n",
    "if len( remaining_data ) > 0:\n",
    "    number_remaining_bins = number_bins - 1\n",
    "    remaining_edges       = np.linspace( bin1_end, \n",
    "                                         remaining_data.max(),\n",
    "                                         number_remaining_bins )\n",
    "else:\n",
    "    remaining_edges = [bin1_end]\n",
    "\n",
    "# Build the full array.\n",
    "failure_edges = np.concatenate( [[0, bin1_end], remaining_edges[1:]] )\n",
    "\n",
    "failure_counts, failure_edges = np.histogram( number_be_failures, bins=failure_edges )\n",
    "\n",
    "# Show the failures as a function of particle as well as the distribution.\n",
    "fig_h, ax_h = plt.subplots( 1, 2, figsize=(10, 6) )\n",
    "\n",
    "fig_h.suptitle( \"{:d} Backward Euler Failures Across {:d} Particles\".format(\n",
    "    number_be_failures.sum(),\n",
    "    number_be_particles ) )\n",
    "\n",
    "ax_h[0].plot( failures_df.index,\n",
    "              failures_df[\"number be failures\"], \n",
    "              \".\" )\n",
    "ax_h[0].set_title( \"Individual Particles\" )\n",
    "ax_h[0].set_xlabel( \"Particle Identifiers\" )\n",
    "ax_h[0].set_ylabel( \"BE Failure Count\" )\n",
    "\n",
    "ax_h[1].hist( failures_df[\"number be failures\"], bins=failure_edges )\n",
    "ax_h[1].set_title( \"Distribution of Failure Counts\\n\"\n",
    "                   \"{:d} Single BE Failures ({:.2f}%)\".format(\n",
    "                    failure_counts[0],\n",
    "                    failure_counts[0] / number_be_failures.sum() * 100 ) )\n",
    "ax_h[1].set_xlabel( \"Number of BE Failures Per Particle\" )\n",
    "if len( failure_counts ) > 1:\n",
    "    ax_h[1].set_ylim( (0, failure_counts[1]*1.1) )\n",
    "fig_h.tight_layout()\n",
    "\n",
    "nonzero_failures_mask = (particles_df[\"number be failures\"] > 0)\n",
    "lotsof_failures_mask  = (particles_df[\"number be failures\"] > 1)\n",
    "\n",
    "print( \"{:d} particles with no failures\".format( (~nonzero_failures_mask).sum() ) )\n",
    "print( \"{:d} particles with 1 or more failures\".format( nonzero_failures_mask.sum() ) )\n",
    "print( \"{:d} particles with 2 or more failures\".format( lotsof_failures_mask.sum() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a3563a",
   "metadata": {},
   "source": [
    "# Life Cycle Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f90447",
   "metadata": {},
   "outputs": [],
   "source": [
    "life_cycle_flag = False\n",
    "\n",
    "def print_variable_summary( variable, variable_name ):\n",
    "    variable_count = len( variable )\n",
    "    variable_mean  = np.mean( variable )\n",
    "    variable_mode  = stats.mode( variable, keepdims=False )\n",
    "    variable_min   = np.min( variable )\n",
    "    variable_max   = np.max( variable )\n",
    "    \n",
    "    print( \"{:s}:\\n\"\n",
    "           \"\\n\"\n",
    "           \"  Count:  {:d} values\\n\"\n",
    "           \"  Range:  [{:g}, {:g}]\\n\"\n",
    "           \"  Mean:   {:g}\\n\"\n",
    "           \"  Mode:   {:g} ({:d} times)\\n\".format(\n",
    "               variable_name,\n",
    "               variable_count,\n",
    "               variable_min, variable_max,\n",
    "               variable_mean,\n",
    "               variable_mode.mode, variable_mode.count\n",
    "           ) )\n",
    "\n",
    "if life_cycle_flag:\n",
    "    print_variable_summary( particles_df[\"particle id\"],        \"Particle ID\" )\n",
    "    print_variable_summary( particles_df[\"be flag\"],            \"BE Flag\" )\n",
    "    print_variable_summary( particles_df[\"input radius\"],       \"Input Radius (m)\" )\n",
    "    print_variable_summary( particles_df[\"output radius\"],      \"Output Radius (m)\" )\n",
    "    print_variable_summary( particles_df[\"input temperature\"],  \"Input Temperature (K)\" )\n",
    "    print_variable_summary( particles_df[\"output temperature\"], \"Output Temperature (K)\" )\n",
    "    print_variable_summary( particles_df[\"time\"],               \"Simulation Time (s)\" )\n",
    "    print_variable_summary( particles_df[\"integration time\"],   \"Delta t (s)\" )\n",
    "    print_variable_summary( particles_df[\"salinity\"],           \"Salt Mass (kg)\" )\n",
    "    print_variable_summary( particles_df[\"air density\"],        \"Air Density (rhoa)\" )\n",
    "    print_variable_summary( particles_df[\"air temperature\"],    \"Air Temperature (K)\" )\n",
    "    print_variable_summary( particles_df[\"relative humidity\"],  \"RH (%)\" )\n",
    "    print_variable_summary( particles_df[\"processor\"],          \"MPI Rank\" )\n",
    "else:\n",
    "    warnings.warn( \"Life cycle analysis isn't complete!  Finish me!\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae85782",
   "metadata": {},
   "source": [
    "# Reviewing Particle Event Counts From `history.nc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d541e9ca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history_nc = nc.Dataset( ntlp_history_path, mode=\"r\" )\n",
    "\n",
    "droplet_approximation.plot_particle_history( history_nc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate that we can account for every particle injected and destroyed.\n",
    "# No deep insight here, just that the history file correctly tracks each of\n",
    "# the key particle events.\n",
    "\n",
    "fig_h, ax_h = plt.subplots( 1, 1, figsize=(6, 6) )\n",
    "\n",
    "number_particles     = history_nc.variables[\"tnumpart\"][:]\n",
    "number_new_particles = history_nc.variables[\"tot_reintro\"][:]\n",
    "number_destroyed     = history_nc.variables[\"tnum_destroy\"][:]\n",
    "\n",
    "ax_h.plot( np.cumsum( number_new_particles - number_destroyed ),\n",
    "           \".\",\n",
    "           label=\"New + Destroyed\" )\n",
    "ax_h.plot( number_particles,\n",
    "           label=\"tnumpart\" )\n",
    "ax_h.plot( np.cumsum( number_new_particles - number_destroyed ) - number_particles,\n",
    "           label=\"Difference\" )\n",
    "ax_h.set_title( \"Total particles in simulation: {:d}\".format(\n",
    "    int( number_new_particles.sum() ) ) )\n",
    "ax_h.set_xlabel( \"Time(s)\" )\n",
    "ax_h.set_ylabel( \"Count\" )\n",
    "ax_h.legend()\n",
    "\n",
    "print( \"Difference between aggregate and cumulative sum?  {}\".format(\n",
    "    np.any( np.cumsum( number_new_particles - number_destroyed ) - number_particles > 0 )\n",
    "    ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cad2a81",
   "metadata": {},
   "source": [
    "# Understanding a Simulation's Particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65546eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the characteristics of a handful of random particles over their lifetime.\n",
    "\n",
    "number_particles = 10\n",
    "particle_indices = np.sort( np.random.randint( 0, len( particles_df ), number_particles ) )\n",
    "\n",
    "droplet_approximation.plot_particles( particles_df.iloc[particle_indices], time_range=[-np.inf, 3000] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aad5764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot long-lived particles, if they exist.\n",
    "\n",
    "number_particles            = 10\n",
    "minimum_number_observations = 3000\n",
    "\n",
    "selection_mask = (particles_df[\"number observations\"] > minimum_number_observations)\n",
    "long_particle_ids = particles_df[selection_mask].index\n",
    "\n",
    "if len( long_particle_ids ) > 0:\n",
    "    particle_ids = np.random.choice( long_particle_ids, size=number_particles, replace=False )\n",
    "    droplet_approximation.plot_particles( particles_df.loc[particle_ids] )\n",
    "else:\n",
    "    print( \"No particles had more than {:d} observation{:s}.\".format(\n",
    "        minimum_number_observations,\n",
    "        \"\" if minimum_number_observations == 1 else \"s\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6456c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following to measure percentages of the population having a characteristic\n",
    "# in a range of interest.\n",
    "particles_df[\"air densities\"].apply( lambda x: ((x>=1.22) & (x<1.25)).sum() ).sum() / particles_df[\"number observations\"].sum() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca60c052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the log-scale distribution of input BE radii throughout the simulation.\n",
    "# We look at the log-distribution so we see all sizes instead of only the\n",
    "# larger particles.\n",
    "#\n",
    "# NOTE: We don't visualize the output BE radii as those are identical except for\n",
    "#       particles that fail BE when injected.  Since only a small percentage (~5%)\n",
    "#       fail BE and retain their injection radius we skip plotting them as\n",
    "#       it merely shows a difference on the outlier line at the left side of\n",
    "#       the plot.\n",
    "#\n",
    "#       By construction all particles' observations are constructed so that\n",
    "#       they contain an input and an output so output-only observations\n",
    "#       (the last for each particle) have already been discarded.  This means\n",
    "#       that only the now-last observation's output BE radii are omitted which\n",
    "#       likely can be ignored (at least this is true for smaller simulations\n",
    "#       with O(100K) particles).\n",
    "#\n",
    "input_radii_concatenated = np.concatenate( particles_df[\"input be radii\"].tolist() )\n",
    "\n",
    "fig_h, ax_h = plt.subplots( 1, 1, figsize=(6, 6) )\n",
    "\n",
    "ax_h.hist( np.log10( input_radii_concatenated ),  bins=100, log=True )\n",
    "ax_h.minorticks_on()\n",
    "ax_h.set_title( \"{:s}\\n\"\n",
    "                \"All Particles ({:d} Observations)\".format(\n",
    "    simulation_name,\n",
    "    input_radii_concatenated.shape[0] ) )\n",
    "ax_h.set_xlabel( \"log10( Radius ) (m)\" )\n",
    "ax_h.set_ylabel( \"Counts (log-scale)\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536b03b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the log-scale distribution of input temperatures throughout the simulation.\n",
    "#\n",
    "# NOTE: We don't visualize the output temperatures as those are identical except for\n",
    "#       particles that fail BE when injected.  Since only a small percentage (~5%)\n",
    "#       fail BE and retain their injection temperature we skip plotting them.\n",
    "#\n",
    "#       By construction all particles' observations are constructed so that\n",
    "#       they contain an input and an output so output-only observations\n",
    "#       (the last for each particle) have already been discarded.  This means\n",
    "#       that only the now-last observation's output temperatures are omitted which\n",
    "#       likely can be ignored (at least this is true for smaller simulations\n",
    "#       with O(100K) particles).\n",
    "#\n",
    "\n",
    "input_temperatures_concatenated = np.concatenate( particles_df[\"input be temperatures\"].tolist() )\n",
    "\n",
    "fig_h, ax_h = plt.subplots( 1, 1, figsize=(6, 6) )\n",
    "\n",
    "ax_h.hist( input_temperatures_concatenated, bins=1000, log=True )\n",
    "ax_h.minorticks_on()\n",
    "ax_h.set_title( \"{:s}\\n\"\n",
    "                \"All Particles ({:d} Observations)\".format(\n",
    "    simulation_name,\n",
    "    input_temperatures_concatenated.shape[0] ) )\n",
    "ax_h.set_xlabel( \"Input Temperature (K)\" )\n",
    "if False:\n",
    "    ax_h.set_xlim( 281, 293 )\n",
    "ax_h.set_ylabel( \"Counts (log-scale)\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee96d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the log-scale distribution of particles' salt masses throughout\n",
    "# the simulation.  We look at the log10\n",
    "salt_masses_concatenated = np.concatenate( particles_df[\"salt masses\"].tolist() )\n",
    "\n",
    "fig_h, ax_h = plt.subplots( 1, 1, figsize=(6, 6) )\n",
    "\n",
    "ax_h.hist( np.log10( salt_masses_concatenated ), bins=100, log=True )\n",
    "ax_h.minorticks_on()\n",
    "ax_h.set_title( \"{:s}\\n\"\n",
    "                \"All Particles ({:d} Observations)\".format(\n",
    "    simulation_name,\n",
    "    salt_masses_concatenated.shape[0] ) )\n",
    "ax_h.set_xlabel( \"log10( Salt Mass ) (kg)\" )\n",
    "ax_h.set_ylabel( \"Counts (log-scale)\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b246a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the log-scale distribution of particles' air temperatures throughout\n",
    "# the simulation.\n",
    "air_temperatures_concatenated = np.concatenate( particles_df[\"air temperatures\"].tolist() )\n",
    "\n",
    "fig_h, ax_h = plt.subplots( 1, 1, figsize=(6, 6) )\n",
    "\n",
    "ax_h.hist( air_temperatures_concatenated, bins=100, log=True )\n",
    "ax_h.minorticks_on()\n",
    "ax_h.set_title( \"{:s}\\n\"\n",
    "                \"All Particles ({:d} Observations)\".format(\n",
    "    simulation_name,\n",
    "    air_temperatures_concatenated.shape[0] ) )\n",
    "ax_h.set_xlabel( \"Air Temperature (K)\" )\n",
    "ax_h.set_ylabel( \"Counts (log-scale)\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea81ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the log-scale distribution of relative humidities experienced by\n",
    "# particles throughout the simulation.  We look at the log-distribution\n",
    "# so to not be dominated by the mean relative humidity.\n",
    "rh_concatenated = np.concatenate( particles_df[\"relative humidities\"].tolist() )\n",
    "\n",
    "fig_h, ax_h = plt.subplots( 1, 1, figsize=(6, 6) )\n",
    "\n",
    "ax_h.hist( rh_concatenated * 100.0, bins=100, log=True )\n",
    "ax_h.minorticks_on()\n",
    "ax_h.set_title( \"{:s}\\n\"\n",
    "                \"All Particles ({:d} Observations)\".format(\n",
    "    simulation_name,\n",
    "    rh_concatenated.shape[0] ) )\n",
    "ax_h.set_xlabel( \"Relative Humidity (%)\" )\n",
    "ax_h.set_ylabel( \"Counts (log-scale)\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d9a7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the log-scale distribution of air densities experienced by\n",
    "# particles throughout the simulation.\n",
    "air_densities_concatenated = np.concatenate( particles_df[\"air densities\"].tolist() )\n",
    "\n",
    "fig_h, ax_h = plt.subplots( 1, 1, figsize=(6, 6) )\n",
    "\n",
    "ax_h.hist( air_densities_concatenated, bins=100, log=True )\n",
    "ax_h.minorticks_on()\n",
    "ax_h.set_title( \"{:s}\\n\"\n",
    "                \"All Particles ({:d} Observations)\".format(\n",
    "    simulation_name,\n",
    "    air_densities_concatenated.shape[0] ) )\n",
    "ax_h.set_xlabel( \"Air Density (kg/m^3)\" )\n",
    "ax_h.set_ylabel( \"Counts (log-scale)\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa3b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the log-scale distribution of integration times experienced by\n",
    "# particles throughout the simulation.\n",
    "dts_concatenated = np.concatenate( particles_df[\"integration times\"].tolist() )\n",
    "\n",
    "fig_h, ax_h = plt.subplots( 1, 1, figsize=(6, 6) )\n",
    "\n",
    "ax_h.hist( dts_concatenated, bins=100, log=True )\n",
    "ax_h.minorticks_on()\n",
    "ax_h.set_title( \"{:s}\\n\"\n",
    "                \"All Particles ({:d} Observations)\".format(\n",
    "    simulation_name,\n",
    "    dts_concatenated.shape[0] ) )\n",
    "ax_h.set_xlabel( \"Integration Times (s)\" )\n",
    "ax_h.set_ylabel( \"Counts (log-scale)\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f07b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the distribution of observations per particle so we can gain\n",
    "# insight into the lifetime of the particles.\n",
    "fig_h, ax_h = plt.subplots( 1, 2, figsize=(10, 6), sharex=True, sharey=True )\n",
    "\n",
    "simulation_end          = particles_df[\"death time\"].max()\n",
    "complete_particles_mask = (particles_df[\"death time\"] < simulation_end)\n",
    "\n",
    "fig_h.suptitle( \"{:s}\".format(\n",
    "    simulation_name ) )\n",
    "\n",
    "ax_h[0].hist( particles_df[\"number observations\"][complete_particles_mask], bins=1000 )\n",
    "ax_h[0].minorticks_on()\n",
    "ax_h[0].set_title( \"{:.1f} Observations/Particle\\n\"\n",
    "                   \"{:d} Destroyed Particles\".format( \n",
    "                    particles_df[\"number observations\"][complete_particles_mask].mean(),\n",
    "                    complete_particles_mask.sum() ) )\n",
    "ax_h[0].set_xlabel( \"Number of Observations\" )\n",
    "ax_h[0].set_ylabel( \"Counts\" )\n",
    "\n",
    "ax_h[1].hist( particles_df[\"number observations\"][~complete_particles_mask], bins=1000 )\n",
    "ax_h[1].minorticks_on()\n",
    "ax_h[1].set_title( \"{:.1f} Observations/Particle\\n\"\n",
    "                   \"{:d} Surviving Particles\".format( \n",
    "                    particles_df[\"number observations\"][~complete_particles_mask].mean(),\n",
    "                    (~complete_particles_mask).sum() ) )\n",
    "ax_h[1].set_xlabel( \"Number of Observations\" )\n",
    "ax_h[1].set_ylabel( \"Counts\" )\n",
    "\n",
    "#fig_h.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11e4311",
   "metadata": {},
   "source": [
    "# Cold Particle Analysis\n",
    "NTLP's backward Euler (BE) process can generate cold particles.  The following cells aim to\n",
    "understand when these occur for:\n",
    "\n",
    "- Assessing their impact when MLP is used\n",
    "- Attempting to fix BE so cold particles aren't generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8b014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cold_particles_with_be_failure( particle_series ):\n",
    "    \"\"\"\n",
    "    0 - No BE failures\n",
    "    1 - First BE failure doesn't match the first cold observation\n",
    "    2 - First BE failure coincides with the first cold observation\n",
    "    \"\"\"\n",
    "    #\n",
    "    # NOTE: This should only be called on particles that have a cold particle!\n",
    "    #\n",
    "    cold_particle_index    = np.where( particle_series[\"input be temperatures\"] < cold_threshold )[0][0]\n",
    "    first_be_failure_index = np.where( particle_series[\"be statuses\"] > 0 )[0]\n",
    "    \n",
    "    if len( first_be_failure_index ) == 0:\n",
    "        return 0\n",
    "    elif first_be_failure_index[0] != cold_particle_index:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# Temperature cutoff, in Kelvin, below which we declare the particle \"cold\".\n",
    "cold_threshold = 280\n",
    "\n",
    "# Particles that are considered \"cold\".\n",
    "cold_particles_mask        = particles_df.apply( lambda x: np.any( x[\"input be temperatures\"] < cold_threshold ), axis=1 )\n",
    "cold_particles_ids         = particles_df[cold_particles_mask].index\n",
    "\n",
    "# Number of cold observations for each particle.\n",
    "cold_particles_counts      = particles_df[cold_particles_mask].apply( lambda x: (x[\"input be temperatures\"] < cold_threshold ).sum(), axis=1 )\n",
    "\n",
    "# First location of each cold observation.  For the initial dataset, \n",
    "# all cold particles occurred once.\n",
    "cold_particles_location    = particles_df[cold_particles_mask].apply( lambda x: np.where( x[\"input be temperatures\"] < cold_threshold )[0][0], axis=1 )\n",
    "\n",
    "# Temperature of the first cold observation for each\n",
    "# particle.\n",
    "cold_particles_temperature = particles_df[cold_particles_mask].apply( lambda x: x[\"input be temperatures\"][np.where( x[\"input be temperatures\"] < cold_threshold )[0][0]], axis=1 )\n",
    "\n",
    "# First BE status for each cold particle.\n",
    "cold_particles_be_status = particles_df[cold_particles_mask].apply( cold_particles_with_be_failure, axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9cd710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "fig_h, ax_h = plt.subplots( 1, 1, figsize=(6, 6) )\n",
    "\n",
    "number_cold_particles = cold_particles_mask.sum()\n",
    "\n",
    "failure_modes = [\"on first BE failure\", \n",
    "                 \"BE failure doesn't match\", \n",
    "                 \"BE and cold match\"]\n",
    "\n",
    "for color_be_index, failure_mode in enumerate( failure_modes ):\n",
    "    mask = (cold_particles_be_status == color_be_index)\n",
    "    ax_h.scatter( particles_df[cold_particles_mask][mask].index,\n",
    "                  cold_particles_location[mask],\n",
    "                  label=failure_mode )\n",
    "\n",
    "ax_h.set_title( \"{:d} Cold Particles ({:.2f}%)\\n\"\n",
    "                \"Temperature < {:d}K\".format(\n",
    "                    number_cold_particles,\n",
    "                    number_cold_particles / len( particles_df ) * 100,\n",
    "                    cold_threshold\n",
    "    ) )\n",
    "ax_h.set_ylabel( \"Timestep Seen\" )\n",
    "ax_h.set_xlabel( \"Particle Number\" )\n",
    "ax_h.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a3bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_h, ax_h = plt.subplots( 1, 1, figsize=(6, 6) )\n",
    "\n",
    "ax_h.plot( particles_df[cold_particles_mask].index,\n",
    "           cold_particles_temperature,\n",
    "           \".\" )\n",
    "\n",
    "ax_h.set_title( \"Cold Particles Temperature\" )\n",
    "ax_h.set_ylabel( \"Temperature (K)\" )\n",
    "ax_h.set_xlabel( \"Particle Number\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0234bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactively look at how cold observations occur.  Previous temperature\n",
    "# is reasonable (as expected) and successive temperature recovers (as expected).\n",
    "# Implies that something with the previous observation's environment causes\n",
    "# BE to kind of succede/mostly fail.\n",
    "%matplotlib notebook\n",
    "\n",
    "fig_h, ax_h = plt.subplots( 1, 1, figsize=(6, 6) )\n",
    "\n",
    "cold_particle_index = 103\n",
    "cold_particle_id    = cold_particles_location.index[cold_particle_index]\n",
    "cold_particle       = particles_df.loc[cold_particle_id]\n",
    "\n",
    "ax_h.plot( np.cumsum( cold_particle[\"integration times\"] ) - cold_particle[\"integration times\"][0],\n",
    "           cold_particle[\"input be temperatures\"] )\n",
    "ax_h.set_title( \"Particle {:d} (Cold Index {:d})\\n\"\n",
    "                .format(\n",
    "    cold_particle_id,\n",
    "    cold_particle_index\n",
    "    ))\n",
    "ax_h.set_ylabel( \"Particle Temperature (K)\" )\n",
    "ax_h.set_xlabel( \"Timestep\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactively look at how cold observations occur.  Previous temperature\n",
    "# is reasonable (as expected) and successive temperature recovers (as expected).\n",
    "# Implies that something with the previous observation's environment causes\n",
    "# BE to kind of succede/mostly fail.\n",
    "%matplotlib notebook\n",
    "\n",
    "fig_h, ax_h = plt.subplots( 1, 1, figsize=(6, 6) )\n",
    "\n",
    "for cold_particle_id in cold_particles_ids:\n",
    "    cold_particle       = particles_df.loc[cold_particle_id]\n",
    "\n",
    "    ax_h.plot( #np.cumsum( cold_particle[\"integration times\"] ) - cold_particle[\"integration times\"][0],\n",
    "               cold_particle[\"input be temperatures\"],\n",
    "               \".\",\n",
    "               label=\"{:d}\".format( cold_particle.name ) )\n",
    "\n",
    "#ax_h.legend()\n",
    "#ax_h.set_title( \"Particle {:d} (Cold Index {:d})\\n\"\n",
    "#                .format(\n",
    "#    cold_particle_id,\n",
    "#    cold_particle_index\n",
    "#    ))\n",
    "ax_h.set_title( \"{:d} Cold Particles (Temperature < {:d}K)\".format(\n",
    "    len( cold_particles_ids ),\n",
    "    cold_threshold\n",
    "    ))\n",
    "ax_h.set_ylabel( \"Particle Temperature (K)\" )\n",
    "ax_h.set_xlabel( \"Timestep\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bca57da",
   "metadata": {},
   "outputs": [],
   "source": [
    "cold_particles_df = pd.DataFrame( cold_particles_location, columns=[\"cold_observation_index\"] )\n",
    "\n",
    "cold_particles_df[\"observation index\"]     = pd.Series( dtype=np.int32   )\n",
    "cold_particles_df[\"integration time\"]      = pd.Series( dtype=np.float32 )\n",
    "cold_particles_df[\"be status\"]             = pd.Series( dtype=np.int32   )\n",
    "cold_particles_df[\"input be radius\"]       = pd.Series( dtype=np.float32 )\n",
    "cold_particles_df[\"output be radius\"]      = pd.Series( dtype=np.float32 )\n",
    "cold_particles_df[\"input be temperature\"]  = pd.Series( dtype=np.float32 )\n",
    "cold_particles_df[\"output be temperature\"] = pd.Series( dtype=np.float32 )\n",
    "cold_particles_df[\"salt mass\"]             = pd.Series( dtype=np.float32 )\n",
    "cold_particles_df[\"air temperature\"]       = pd.Series( dtype=np.float32 )\n",
    "cold_particles_df[\"relative humidity\"]     = pd.Series( dtype=np.float32 )\n",
    "cold_particles_df[\"air density\"]           = pd.Series( dtype=np.float32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4c9107",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cold_particle_id in cold_particles_ids:\n",
    "    observation_index = cold_particles_location[cold_particle_id]\n",
    "    \n",
    "    cold_particles_df.at[cold_particle_id, \"observation index\"]     = observation_index - 1\n",
    "    cold_particles_df.at[cold_particle_id, \"integration time\"]      = particles_df.at[cold_particle_id, \"integration times\"][observation_index - 1]\n",
    "    cold_particles_df.at[cold_particle_id, \"be status\"]             = particles_df.at[cold_particle_id, \"be statuses\"][observation_index - 1]\n",
    "    cold_particles_df.at[cold_particle_id, \"previous be status\"]    = particles_df.at[cold_particle_id, \"be statuses\"][observation_index - 2]\n",
    "    cold_particles_df.at[cold_particle_id, \"input be radius\"]       = particles_df.at[cold_particle_id, \"input be radii\"][observation_index - 1]\n",
    "    cold_particles_df.at[cold_particle_id, \"output be radius\"]      = particles_df.at[cold_particle_id, \"output be radii\"][observation_index - 1]\n",
    "    cold_particles_df.at[cold_particle_id, \"input be temperature\"]  = particles_df.at[cold_particle_id, \"input be temperatures\"][observation_index - 1]\n",
    "    cold_particles_df.at[cold_particle_id, \"output be temperature\"] = particles_df.at[cold_particle_id, \"output be temperatures\"][observation_index - 1]\n",
    "    cold_particles_df.at[cold_particle_id, \"salt mass\"]             = particles_df.at[cold_particle_id, \"salt masses\"][observation_index - 1]\n",
    "    cold_particles_df.at[cold_particle_id, \"air temperature\"]       = particles_df.at[cold_particle_id, \"air temperatures\"][observation_index - 1]\n",
    "    cold_particles_df.at[cold_particle_id, \"relative humidity\"]     = particles_df.at[cold_particle_id, \"relative humidities\"][observation_index - 1]\n",
    "    cold_particles_df.at[cold_particle_id, \"air density\"]           = particles_df.at[cold_particle_id, \"air densities\"][observation_index - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00018d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_h, ax_h = plt.subplots( 4, 2, figsize=(10, 12) )\n",
    "\n",
    "number_bins = 25\n",
    "\n",
    "ax_h[0][0].hist( cold_particles_df[\"integration time\"], bins=number_bins )\n",
    "ax_h[0][0].set_title( \"Integration Time\" )\n",
    "ax_h[0][0].set_xlabel( \"Time (s)\" )\n",
    "ax_h[0][1].hist( cold_particles_df[\"be status\"], bins=number_bins, label=\"Cold Particle\" )\n",
    "ax_h[0][1].hist( cold_particles_df[\"previous be status\"], bins=number_bins, label=\"Previous Timestep\" )\n",
    "ax_h[0][1].set_title( \"BE Status\" )\n",
    "ax_h[0][1].set_xlabel( \"BE Failure Mode\" )\n",
    "ax_h[0][1].legend()\n",
    "\n",
    "# XXX: why is input radii busted?\n",
    "ax_h[1][0].hist( np.log10( cold_particles_df[\"input be radius\"] ), bins=number_bins )\n",
    "ax_h[1][0].set_title( \"Input Radius\" )\n",
    "ax_h[1][0].set_xlabel( \"Size (m)\" )\n",
    "ax_h[1][1].hist( np.log10( cold_particles_df[\"output be radius\"] ), bins=number_bins )\n",
    "ax_h[1][1].set_title( \"Output Radius\" )\n",
    "ax_h[1][1].set_xlabel( \"Size (m)\" )\n",
    "\n",
    "ax_h[2][0].hist( cold_particles_df[\"input be temperature\"], bins=number_bins )\n",
    "ax_h[2][0].set_title( \"Input Temperature\" )\n",
    "ax_h[2][0].set_xlabel( \"Temperature (K)\" )\n",
    "ax_h[2][1].hist( cold_particles_df[\"output be temperature\"], bins=number_bins )\n",
    "ax_h[2][1].set_title( \"Output Temperature\" )\n",
    "ax_h[2][1].set_xlabel( \"Temperature (K)\" )\n",
    "\n",
    "ax_h[3][0].hist( cold_particles_df[\"air temperature\"], bins=number_bins )\n",
    "ax_h[3][0].set_title( \"Air Temperature\" )\n",
    "ax_h[3][0].set_xlabel( \"Temperature (K)\" )\n",
    "ax_h[3][1].hist( cold_particles_df[\"relative humidity\"] * 100, bins=number_bins )\n",
    "ax_h[3][1].set_title( \"Relative Humidity\" )\n",
    "ax_h[3][1].set_xlabel( \"Percentage (%)\" )\n",
    "\n",
    "fig_h.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30af9988",
   "metadata": {},
   "source": [
    "# Resampling Particles Onto A Common Timeline\n",
    "Comparing multiple particles is easier when they have the same number of observations\n",
    "as this allows us to take averages, ratios, etc.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b067a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling particles onto a common timeline (\"Particle Life\").\n",
    "# This lets us see common shapes in the particle's states since\n",
    "# they're now interpolated onto a fixed number of points in time.\n",
    "\n",
    "import functools\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# The pi chamber development dataset's particles have ~1500 observations on average.\n",
    "# We pick a number of points that well retain most features while not balooning\n",
    "# our memory footprint.\n",
    "number_points = 1000\n",
    "\n",
    "def resample_particle_observations( particle_series, metric, number_points ):\n",
    "    \"\"\"\n",
    "    Interpolates the target metric onto a [0, 1] with a fixed number of points.\n",
    "    \"\"\"\n",
    "    \n",
    "    first_time = particle_series[\"integration times\"][0]\n",
    "    timeline   = np.cumsum( particle_series[\"integration times\"] )\n",
    "\n",
    "    linear_metric = interp1d( (timeline - first_time) / (timeline[-1] - first_time),\n",
    "                              particle_series[metric], \n",
    "                              kind=\"linear\" )\n",
    "    return linear_metric( np.linspace( 0.0, 1.0, number_points ) )\n",
    "\n",
    "simulation_end          = particles_df[\"death time\"].max()\n",
    "complete_particles_mask = (particles_df[\"death time\"] < simulation_end)\n",
    "\n",
    "resample_output_radii        = functools.partial( resample_particle_observations,\n",
    "                                                  metric=\"output be radii\",\n",
    "                                                  number_points=number_points )\n",
    "resample_output_temperatures = functools.partial( resample_particle_observations,\n",
    "                                                  metric=\"output be temperatures\",\n",
    "                                                  number_points=number_points )\n",
    "resample_salt_masses         = functools.partial( resample_particle_observations,\n",
    "                                                  metric=\"salt masses\",\n",
    "                                                  number_points=number_points )\n",
    "resample_air_temperatures    = functools.partial( resample_particle_observations,\n",
    "                                                  metric=\"air temperatures\",\n",
    "                                                  number_points=number_points )\n",
    "resample_relative_humidities = functools.partial( resample_particle_observations,\n",
    "                                                  metric=\"relative humidities\",\n",
    "                                                  number_points=number_points )\n",
    "resample_air_densities       = functools.partial( resample_particle_observations,\n",
    "                                                  metric=\"air densities\",\n",
    "                                                  number_points=number_points )\n",
    "\n",
    "radii_resampled          = np.vstack( particles_df[complete_particles_mask].apply( resample_output_radii, axis=1 ).tolist() )\n",
    "particle_temps_resampled = np.vstack( particles_df[complete_particles_mask].apply( resample_output_temperatures, axis=1 ).tolist() )\n",
    "salt_masses_resampled    = np.vstack( particles_df[complete_particles_mask].apply( resample_salt_masses, axis=1 ).tolist() )\n",
    "air_temps_resampled      = np.vstack( particles_df[complete_particles_mask].apply( resample_air_temperatures, axis=1 ).tolist() )\n",
    "rh_resampled             = np.vstack( particles_df[complete_particles_mask].apply( resample_relative_humidities, axis=1 ).tolist() )\n",
    "air_densities_resampled  = np.vstack( particles_df[complete_particles_mask].apply( resample_air_densities, axis=1 ).tolist() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f33251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate that resampling works as expected against a random particle.  Plot the resampled\n",
    "# metric against the original.\n",
    "particle_index = np.random.randint( 0, len( particles_df[complete_particles_mask] ) )\n",
    "\n",
    "dt = 1 / number_points\n",
    "\n",
    "particle = particles_df[complete_particles_mask].iloc[particle_index]\n",
    "\n",
    "fig_h, ax_h = plt.subplots( 1, 1, figsize=(10, 6) )\n",
    "\n",
    "fig_h.suptitle( \"Particle {:d} (Index {:d})\".format(\n",
    "    particle.name,\n",
    "    particle_index ) )\n",
    "\n",
    "#raise RuntimeError( \"Consolidate onto one plot\" )\n",
    "\n",
    "ax_h.plot( (np.cumsum( particle[\"integration times\"] ) - particle[\"integration times\"][0]) / (particle[\"death time\"] - particle[\"birth time\"]),\n",
    "           particle[\"relative humidities\"],\n",
    "           label=\"Observations\" )\n",
    "ax_h.plot( np.arange( number_points ) * dt,\n",
    "           rh_resampled[particle_index, :],\n",
    "           \".\",\n",
    "           label=\"Resampling\" )\n",
    "ax_h.set_xlabel( \"Interpolated Time (s)\" )\n",
    "ax_h.set_ylabel( \"Relative Humidity (fractional)\" )\n",
    "ax_h.set_xlabel( \"Time (s)\" )\n",
    "ax_h.legend( loc=\"upper left\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebc9955",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_h, ax_h = plt.subplots( 1, 1, figsize=(6, 6) )\n",
    "\n",
    "ax_h.plot( np.arange( number_points ) * dt, radii_resampled.min( axis=0 ), label=\"Minimum\" )\n",
    "ax_h.plot( np.arange( number_points ) * dt, radii_resampled.mean( axis=0 ), label=\"Mean\" )\n",
    "ax_h.plot( np.arange( number_points ) * dt, np.median( radii_resampled, axis=0 ), label=\"Median\" )\n",
    "ax_h.plot( np.arange( number_points ) * dt, radii_resampled.max( axis=0 ), label=\"Maximum\" )\n",
    "ax_h.set_yscale( \"log\" )\n",
    "ax_h.set_title( \"Particle Radii Extrema\\n\"\n",
    "                \"{:d} Completed Particles\".format( complete_particles_mask.sum() ) )\n",
    "ax_h.set_xlabel( \"Interpolated Time (s)\" )\n",
    "ax_h.set_ylabel( \"Size (m)\" )\n",
    "ax_h.minorticks_on()\n",
    "ax_h.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a116ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_h, ax_h = plt.subplots( 1, 1, figsize=(6, 6) )\n",
    "\n",
    "ax_h.plot( np.arange( number_points ) * dt, particle_temps_resampled.min( axis=0 ), label=\"Minimum\" )\n",
    "ax_h.plot( np.arange( number_points ) * dt, particle_temps_resampled.mean( axis=0 ), label=\"Mean\" )\n",
    "ax_h.plot( np.arange( number_points ) * dt, np.median( particle_temps_resampled, axis=0 ), label=\"Median\" )\n",
    "ax_h.plot( np.arange( number_points ) * dt, particle_temps_resampled.max( axis=0 ), label=\"Maximum\" )\n",
    "ax_h.set_yscale( \"log\" )\n",
    "ax_h.set_title( \"Particle Temperatures Extrema\\n\"\n",
    "                \"{:d} Completed Particles\".format( complete_particles_mask.sum() ) )\n",
    "ax_h.set_xlabel( \"Interpolated Time (s)\" )\n",
    "ax_h.set_ylabel( \"Temperature (K)\" )\n",
    "ax_h.minorticks_on()\n",
    "ax_h.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823e3c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_h, ax_h = plt.subplots( 1, 1, figsize=(6, 6) )\n",
    "\n",
    "ax_h.plot( np.arange( number_points ) * dt, salt_masses_resampled.min( axis=0 ), label=\"Minimum\" )\n",
    "ax_h.plot( np.arange( number_points ) * dt, salt_masses_resampled.mean( axis=0 ), label=\"Mean\" )\n",
    "ax_h.plot( np.arange( number_points ) * dt, np.median( salt_masses_resampled, axis=0 ), label=\"Median\" )\n",
    "ax_h.plot( np.arange( number_points ) * dt, salt_masses_resampled.max( axis=0 ), label=\"Maximum\" )\n",
    "ax_h.set_yscale( \"log\" )\n",
    "ax_h.set_title( \"Salt Masses Extrema\\n\"\n",
    "                \"{:d} Completed Particles\".format( complete_particles_mask.sum() ) )\n",
    "ax_h.set_xlabel( \"Interpolated Time (s)\" )\n",
    "ax_h.set_ylabel( \"Mass (kg)\" )\n",
    "ax_h.minorticks_on()\n",
    "ax_h.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c3d051",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_h, ax_h = plt.subplots( 1, 1, figsize=(6, 6) )\n",
    "\n",
    "ax_h.plot( np.arange( number_points ) * dt, air_temps_resampled.min( axis=0 ), label=\"Minimum\" )\n",
    "ax_h.plot( np.arange( number_points ) * dt, air_temps_resampled.mean( axis=0 ), label=\"Mean\" )\n",
    "ax_h.plot( np.arange( number_points ) * dt, np.median( air_temps_resampled, axis=0 ), label=\"Median\" )\n",
    "ax_h.plot( np.arange( number_points ) * dt, air_temps_resampled.max( axis=0 ), label=\"Maximum\" )\n",
    "ax_h.set_yscale( \"log\" )\n",
    "ax_h.set_title( \"Air Temperatures Extrema\\n\"\n",
    "                \"{:d} Completed Particles\".format( complete_particles_mask.sum() ) )\n",
    "ax_h.set_xlabel( \"Interpolated Time (s)\" )\n",
    "ax_h.set_ylabel( \"Temperature (K)\" )\n",
    "ax_h.minorticks_on()\n",
    "ax_h.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76dc67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_h, ax_h = plt.subplots( 1, 1, figsize=(6, 6) )\n",
    "\n",
    "ax_h.plot( np.arange( number_points ) * dt, rh_resampled.min( axis=0 ), label=\"Minimum\" )\n",
    "ax_h.plot( np.arange( number_points ) * dt, rh_resampled.mean( axis=0 ), label=\"Mean\" )\n",
    "ax_h.plot( np.arange( number_points ) * dt, np.median( rh_resampled, axis=0 ), label=\"Median\" )\n",
    "ax_h.plot( np.arange( number_points ) * dt, rh_resampled.max( axis=0 ), label=\"Maximum\" )\n",
    "ax_h.set_title( \"Relative Humidity Extrema\\n\"\n",
    "                \"{:d} Completed Particles\".format( complete_particles_mask.sum() ) )\n",
    "ax_h.set_xlabel( \"Interpolated Time (s)\" )\n",
    "ax_h.set_ylabel( \"Relative Humidity (fractional)\" )\n",
    "ax_h.minorticks_on()\n",
    "ax_h.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c95be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_h, ax_h = plt.subplots( 1, 1, figsize=(6, 6) )\n",
    "\n",
    "ax_h.plot( np.arange( number_points ) * dt, air_densities_resampled.min( axis=0 ), label=\"Minimum\" )\n",
    "ax_h.plot( np.arange( number_points ) * dt, air_densities_resampled.mean( axis=0 ), label=\"Mean\" )\n",
    "ax_h.plot( np.arange( number_points ) * dt, np.median( air_densities_resampled, axis=0 ), label=\"Median\" )\n",
    "ax_h.plot( np.arange( number_points ) * dt, air_densities_resampled.max( axis=0 ), label=\"Maximum\" )\n",
    "ax_h.set_title( \"Air Density Extrema\\n\"\n",
    "                \"{:d} Completed Particles\".format( complete_particles_mask.sum() ) )\n",
    "ax_h.set_xlabel( \"Interpolated Time (s)\" )\n",
    "ax_h.set_ylabel( \"Air Density (?)\" )\n",
    "ax_h.minorticks_on()\n",
    "ax_h.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f4d43e",
   "metadata": {},
   "source": [
    "# Open Analysis Questions to Answer\n",
    "- When does BE fail?\n",
    "  - Does it happen multiple times to a particle?\n",
    "  - How often in the data set?\n",
    "- Why are there super cold particles? ~210K\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
