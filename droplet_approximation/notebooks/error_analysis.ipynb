{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772aafd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "# Limit numpy to 1 thread so that\n",
    "# we can parallelize the error analysis\n",
    "# properly\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "from droplet_approximation import *\n",
    "\n",
    "# Likewise limit pytroch to 1 thread\n",
    "torch.set_num_threads( 1 )\n",
    "torch.set_num_interop_threads( 1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb50436",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do NOT edit this cell. Instead, make any changes you want in the cell below it by setting\n",
    "# these variables.\n",
    "\n",
    "# This model corresponds to \"Model Box Uncoupled 400M l1 Residual 14 Epochs\" in the group drive\n",
    "model_load_paths = [\"../models/network_box_uncoupled_400M_l1_residual_epoch_14.pt\"]\n",
    "model_names      = [ path.split( \"/\" )[-1].split( \".\" )[0].replace( \"_\", \" \" ) for path in model_load_paths ]\n",
    "\n",
    "# This gets passed to `set_parameter_ranges` in the notebook and in\n",
    "# the analysis processes.\n",
    "parameter_ranges = None\n",
    "\n",
    "# Commit for each model\n",
    "commit_SHAs = [\"369bacc0ba5c7367f17ec71707a1df64afd1b6f5\"]\n",
    "\n",
    "# Change this to fit wherever testing data is stored\n",
    "particles_root  = \"/afs/crc.nd.edu/group/RichterLab/droplet_approximation/data/simulations/pi_chamber-rh103-1way/particles\"\n",
    "dirs_per_level  = 256\n",
    "\n",
    "# This controls how much of the data to load.\n",
    "# The notebook will load subset_fraction / dirs_per_level\n",
    "# of the overall dataset.\n",
    "subset_fraction = 256\n",
    "\n",
    "# Controls whether to do the graphing/analysis with iterative or direct inference\n",
    "iterative = False\n",
    "\n",
    "# Good CUSUM parameters for non-iterative, could probably be dialed in more\n",
    "cusum_error_tolerance = np.array( [ 0.005, 0.05 ] )\n",
    "cusum_error_threshold = np.array( [ 0.02, 0.20 ] )\n",
    "\n",
    "norm = standard_norm\n",
    "\n",
    "# How many processes/batches to run the analysis with\n",
    "# Defaults number_processes to multiprocessing.cpu_count() - 1\n",
    "number_processes = 0\n",
    "number_batches = 1\n",
    "\n",
    "\n",
    "# Maximum number of deviation clusters to identify\n",
    "max_clusters = 7\n",
    "\n",
    "# This sets the x-y-z limits for the deviation cluster graph.\n",
    "# Since there are some deviations at very far flung parts of\n",
    "# parameter space, without explicitly setting these ranges,\n",
    "# the deviations are all smooshed together on the graph.\n",
    "set_deviation_graph_limits = False\n",
    "deviation_graph_x_range    = ( -.5,2.0 )\n",
    "deviation_graph_y_range    = ( -6.75, -2.50 )\n",
    "deviation_graph_z_range    = ( 1.0,1.07 )\n",
    "\n",
    "# How many of the worst particles to graph the trajectory of\n",
    "number_graphs = 3\n",
    "\n",
    "# The worst `number_graphs` particles will be picked\n",
    "# only from particles with deviations from all of the listed\n",
    "# clusters. If None, select any particle.\n",
    "# If using strict_graph_cluster_filter, select particles\n",
    "# with ONLY the specified deviations.\n",
    "\n",
    "#deviation_graph_cluster_filter = numpy.array( [0,1,2] )\n",
    "deviation_graph_cluster_filter = None\n",
    "strict_graph_cluster_filter    = False\n",
    "\n",
    "\n",
    "# This array determines where to pickle the analysis reports\n",
    "# to. Does not save if `None`. There must be one path for each\n",
    "# model.\n",
    "save_scores              = True\n",
    "pickled_score_save_paths = [ None for _ in range( len( model_load_paths ) ) ]\n",
    "\n",
    "# Determines whether to load scores from pickled files.\n",
    "# If true, deviation analysis WILL NOT BE RUN. Instead,\n",
    "# the notebook will load previous deviation analysis files\n",
    "# from the supplied paths.\n",
    "# There must be one pickled score path for each model.\n",
    "load_scores              = False\n",
    "pickled_score_load_paths = [ \"PATH_TO_PICKLED_SCORE.pkl\" for _ in range( len( model_load_paths ) ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bbfdc1-957d-4ec4-acbc-8c5673a2d8ba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Edit settings here:\n",
    "\n",
    "# These ranges correspond to \"Model Box Uncoupled 400M l1 Residual 14 Epochs\" in the group drive\n",
    "parameter_ranges = {\n",
    "    \"radius\": ( -6.75, -3.00 ),\n",
    "    \"relative_humidity\": ( 0.98, 1.11 )\n",
    "}\n",
    "\n",
    "score_report_dir = \"/afs/crc.nd.edu/group/RichterLab/droplet_approximation/data/analysis/tmp/\"\n",
    "\n",
    "# If you want to load less of the data, set the following to what fraction out of 256 you want loaded.\n",
    "# This allows for quick testing with the notebook without rerunning the entire dataset.\n",
    "#subset_fraction = 5\n",
    "\n",
    "#iterative = True\n",
    "# Good CUSUM parameters for iterative, could probably be dialed in more\n",
    "#cusum_error_tolerance = np.array( [ 0.02, 0.10 ] )\n",
    "#cusum_error_threshold = np.array( [ 0.08, 0.40 ] )\n",
    "# You might want more clusters to be identified for iterative as well. This number has not been tuned thoroughly\n",
    "#max_clusters = 12\n",
    "\n",
    "#set_deviation_graph_limits = True\n",
    "#deviation_graph_x_range    = ( -.5,2.0 )\n",
    "#deviation_graph_y_range    = ( -6.75, -2.50 )\n",
    "#deviation_graph_z_range    = ( 1.0,1.07 )\n",
    "\n",
    "# General format for these files will be \"(score_report_dir)(model_name)_(iterative if iterative)_scoring_report.pkl\"\n",
    "#save_scores              = True\n",
    "#pickled_score_save_paths = [ f\"{score_report_dir}{model_name.replace(\" \", \"_\")}{'_iterative' if iterative else ''}_scoring_report.pkl\" for model_name in model_names ]\n",
    "load_scores              = True\n",
    "pickled_score_load_paths = [ f\"{score_report_dir}{model_name.replace(\" \", \"_\")}{'_iterative' if iterative else ''}_scoring_report.pkl\" for model_name in model_names ]\n",
    "\n",
    "# This will select the worst particles deviations from ONLY clusters 1 and 3\n",
    "#strict_graph_cluster_filter    = True\n",
    "#deviation_graph_cluster_filter = np.array( [1, 3] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20253b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if number_processes == 0:\n",
    "    number_processes = multiprocessing.cpu_count() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2122b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to make it work with the current model. Remove if using a new model\n",
    "set_parameter_ranges( parameter_ranges )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae14a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_count = len( model_load_paths )\n",
    "\n",
    "model_list  = [ ResidualNet() for i in range( model_count ) ]\n",
    "\n",
    "for i in range( model_count ):\n",
    "    model_list[i].load_state_dict( torch.load( model_load_paths[i] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd8c8ef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load or calculate score_reports\n",
    "if load_scores:\n",
    "    score_reports = []\n",
    "    for load_path in pickled_score_load_paths:\n",
    "        try:\n",
    "            with open( load_path, \"rb\" ) as score_file:\n",
    "                score_reports.append( pickle.load( score_file ) )\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to open {load_path}. Encountered error:\\n {e}\")\n",
    "else:\n",
    "    # Extract subset_fraction/256 of the particles and score\n",
    "    ids_index = np.fromfile( particles_root + \"/particles.index\", dtype=np.int32 )\n",
    "    filtered_ids = ids_index[ ( ( ids_index // 256 ) % 256 < subset_fraction ) ]\n",
    "\n",
    "    score_reports = [ ScoringReport( particles_root,\n",
    "                                     filtered_ids, \n",
    "                                     dirs_per_level,\n",
    "                                     model_list[model_i], \n",
    "                                     model_names[model_i], \n",
    "                                     \"cpu\", \n",
    "                                     cusum_error_tolerance=cusum_error_tolerance, \n",
    "                                     cusum_error_threshold=cusum_error_threshold,\n",
    "                                     iterative=iterative,\n",
    "                                     norm=norm,\n",
    "                                     number_processes=number_processes,\n",
    "                                     number_batches=number_batches,\n",
    "                                     max_clusters=12,\n",
    "                                     parameter_ranges=parameter_ranges )\n",
    "                      for model_i in range ( model_count ) ]\n",
    "\n",
    "    # Dump pickled results\n",
    "    try:\n",
    "        for model_index, save_path in enumerate( pickled_score_save_paths ):\n",
    "            if save_path is None:\n",
    "                continue\n",
    "            with open( save_path, \"wb\" ) as score_file:\n",
    "                pickle.dump( score_reports[model_index], score_file )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save model {model_names[model_index]} to file {save_path} due to the following exception: \\n {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15938b8d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for score_report in score_reports:\n",
    "    # We set precision to 2 because otherwise everything is labeled\n",
    "    # with very long decimals. We can fix this more thoroughly later\n",
    "    with np.printoptions( precision=2 ):\n",
    "        fig,ax = score_report.plot_deviations(label_centers=False, thinning_ratio=1)\n",
    "\n",
    "    fig.set_size_inches( ( 8,12 ) )\n",
    "    if set_deviation_graph_limits:\n",
    "        ax.set_ylim3d( deviation_graph_x_range ) \n",
    "        ax.set_xlim3d( deviation_graph_y_range ) \n",
    "        ax.set_zlim3d( deviation_graph_z_range ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2d6477",
   "metadata": {},
   "outputs": [],
   "source": [
    "for score_report in score_reports:\n",
    "    score_reports[0].net_nrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd20f35-974c-4b70-99f0-cd1a0100ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array( [1,1,1,1,2,2,2,3,3,5,5,5,5,5,-2,-2,-2,1,2,2,1,1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01be8dc2-4eb2-425e-b468-ef48b905554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppNRMSE = np.array( list( score_reports[0].per_particle_nrmse.values() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d142a0a4-82b2-4a6f-b4c9-5adfddd90857",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_index, score_report in enumerate( score_reports ):\n",
    "    print( f\"For model {model_names[model_index]}:\\n\"\n",
    "         + f\"Overall NRMSE: {score_report.net_nrmse}\\n\"\n",
    "         + f\"Mean Per Particle NRMSE: {np.mean( ppNRMSE )}\\n\"\n",
    "         + f\"Median Per Particle NRMSE: {np.median( ppNRMSE )}\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7c52e1-0c13-4d4a-8b8a-01e1b6c6b624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corresponding particle df\n",
    "# Select the worst particles based on NRMSE\n",
    "\n",
    "if deviation_graph_cluster_filter is None:\n",
    "    target_particle_ids = heapq.nlargest( number_graphs, score_reports[0].per_particle_nrmse, \n",
    "                                          key=score_reports[0].per_particle_nrmse.get )\n",
    "else:\n",
    "    # Iterates over each particle's deviations to see if they contain the deviations\n",
    "    # in the cluster filter.\n",
    "    deviation_particle_ids      = score_reports[0].deviation_particle_ids\n",
    "    change_points               = np.array( np.where( deviation_particle_ids[1:] != deviation_particle_ids[:-1] )[0] ) + 1\n",
    "    filtered_per_particle_nrmse = {}\n",
    "    \n",
    "    start_index = 0\n",
    "    for end_index in change_points:\n",
    "        particle_id       = deviation_particle_ids[start_index]\n",
    "        particle_clusters = score_reports[0].deviation_clusters[start_index:end_index]\n",
    "        if np.all( np.isin( deviation_graph_cluster_filter, particle_clusters ) ):\n",
    "            # If strict filtering is on and there are additional deviations\n",
    "            # continue without adding the particle to the list.\n",
    "            if strict_graph_cluster_filter and not np.all( np.isin( particle_clusters, deviation_graph_cluster_filter ) ):\n",
    "                start_index = end_index\n",
    "                continue\n",
    "\n",
    "            filtered_per_particle_nrmse[particle_id] = score_reports[0].per_particle_nrmse[particle_id]\n",
    "\n",
    "        start_index = end_index\n",
    "\n",
    "    target_particle_ids = heapq.nlargest( number_graphs, filtered_per_particle_nrmse, \n",
    "        key=filtered_per_particle_nrmse.get )\n",
    "\n",
    "    \n",
    "target_particle_ids\n",
    "\n",
    "df = read_particles_data( particles_root, target_particle_ids, dirs_per_level )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958dad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each particle, each model will yield 3 figures.\n",
    "# The first will be the figure against BDF\n",
    "# The second will be the figure against BE (with deviations highlighted)\n",
    "# The third will be the figure's CUSUM analysis (calculated within the notebook)\n",
    "\n",
    "# This is the same colormap as in `scoring.py` for plotting deviations\n",
    "colormap = colors.ListedColormap( [\"red\", \"blue\", \"green\", \"orange\", \"black\", \"yellow\", \"teal\", \"gold\", \"magenta\", \"lightgreen\", \"navy\", \"dimgray\", \"lightcoral\"] )\n",
    "\n",
    "for particle_id in target_particle_ids:\n",
    "    particle_df      = df.loc[particle_id]\n",
    "    input_parameters = np.stack( particle_df[[\n",
    "        \"input radii\",\n",
    "        \"input temperatures\",\n",
    "        \"salt masses\",\n",
    "        \"air temperatures\",\n",
    "        \"relative humidities\",\n",
    "        \"air densities\",\n",
    "        \"integration times\"\n",
    "    ]].to_numpy(), axis=-1 )\n",
    "\n",
    "    be_mask             = be_success_mask( input_parameters[:, 0] )\n",
    "    actual_particle_times = np.delete( np.cumsum( np.insert( input_parameters[:, -1],\n",
    "                                                             0,\n",
    "                                                             0.0 )[:-1] ),\n",
    "                                      ~be_mask ) + particle_df[\"birth time\"]\n",
    "    input_parameters = input_parameters[be_mask]\n",
    "    times            = np.insert( np.cumsum( input_parameters[:, -1] ), 0, 0.0 )[:-1] + particle_df[\"birth time\"]\n",
    "\n",
    "    if iterative:\n",
    "        model_outputs = [ do_iterative_inference(\n",
    "                                input_parameters[:, :-1], \n",
    "                                times,\n",
    "                                model_list[model_index],\n",
    "                                \"cpu\"\n",
    "                            ) for model_index in range( model_count ) ]\n",
    "    else:\n",
    "        model_outputs = [ np.insert( \n",
    "                                    do_inference(\n",
    "                                        input_parameters[:, :-1],\n",
    "                                        input_parameters[:, -1],\n",
    "                                        model_list[model_index],\n",
    "                                        \"cpu\"\n",
    "                                    )[:-1, :],\n",
    "                                    0, \n",
    "                                    input_parameters[0, :2],\n",
    "                                    axis=0\n",
    "                                ) for model_index in range( model_count ) ]\n",
    "\n",
    "\n",
    "    bdf_output = do_iterative_bdf(\n",
    "        input_parameters[:, :-1],\n",
    "        times\n",
    "    )\n",
    "    be_output  = input_parameters[:, :2]\n",
    "\n",
    "    normed_model_outputs = [ norm( model_output ) for model_output in model_outputs ]\n",
    "    normed_bdf_output    = norm( bdf_output )\n",
    "    normed_be_output     = norm( be_output )\n",
    "\n",
    "    for model_index in range( model_count ): \n",
    "        fig_h_bdf, ax_h_bdf = analyze_model_particle_performance(\n",
    "            times,\n",
    "            bdf_output,\n",
    "            model_outputs[model_index],\n",
    "            norm\n",
    "        )\n",
    "\n",
    "        fig_h_bdf.suptitle( f\"Droplet trajectory overview for particle {particle_id} on model {model_names[model_index]} vs. BDF\\n SHA: {commit_SHAs[model_index]}\" ) \n",
    "\n",
    "        fig_h_be, ax_h_be = analyze_model_particle_performance(\n",
    "            times,\n",
    "            input_parameters[:, :2], \n",
    "            model_outputs[model_index],\n",
    "            norm\n",
    "        )\n",
    "\n",
    "        fig_h_be.suptitle( f\"Droplet trajectory overview for particle {particle_id} on model {model_names[model_index]} vs. BE\\n SHA: {commit_SHAs[model_index]}\" ) \n",
    "\n",
    "        fig_h_cusum, ax_h_cusum = plt.subplots( 2, 2, figsize=(9,8) )\n",
    "        fig_h_cusum.suptitle( f\"Droplet trajectory overview part 2 for particle {particle_id} on model { model_names[model_index]}\\n SHA: {commit_SHAs[model_index] }\" ) \n",
    "\n",
    "        model_cusum = np.abs( calculate_cusum( ( normed_be_output - normed_model_outputs[model_index] ).T, cusum_error_tolerance ) )\n",
    "\n",
    "        ax_h_cusum[0][0].set_title(\"Radius CUSUM chart\") \n",
    "        ax_h_cusum[0][0].plot( times, model_cusum[0].T, label=[\"positive radius cusum\", \"negative radius cusum\"] )\n",
    "        ax_h_cusum[0][0].set_xlabel( \"time (s)\" )\n",
    "        ax_h_cusum[0][0].axhline( y=cusum_error_threshold[0], color=\"red\",linewidth=1, linestyle=\"--\",label=\"cusum divergence threshold\" )\n",
    "\n",
    "        ax_h_cusum[0][0].set_ylabel(\"CUSUM\")\n",
    "\n",
    "        ax_h_cusum[0][1].plot( times, particle_df[\"relative humidities\"][be_mask] )\n",
    "        ax_h_cusum[0][1].set_title( \"RH versus time for Particle \" + str( model_index ) )\n",
    "        ax_h_cusum[0][1].set_xlabel( \"time (s)\" )\n",
    "        ax_h_cusum[0][1].set_ylabel( \"Relative Humidity (%)\" ) \n",
    "\n",
    "\n",
    "        ax_h_cusum[1][0].plot( times, particle_df[\"air temperatures\"][be_mask] - particle_df[\"input temperatures\"][be_mask] )\n",
    "        ax_h_cusum[1][0].set_title( f\"Temperature Difference for Particle {particle_id}\" )\n",
    "        ax_h_cusum[1][0].set_xlabel( \"time (s)\" )\n",
    "        ax_h_cusum[1][0].set_ylabel( \"Air Temperature (K)\" ) \n",
    "\n",
    "        ax_h_cusum[1][1].plot( times, particle_df[\"air temperatures\"][be_mask] )\n",
    "        ax_h_cusum[1][1].set_title( f\"Air Temperatures for Particle {particle_id}\")\n",
    "        ax_h_cusum[1][1].set_xlabel( \"time (s)\" )\n",
    "        ax_h_cusum[1][1].set_ylabel( \"time step (s)\" ) \n",
    "\n",
    "        for k, deviation_index in enumerate( np.where( score_reports[model_index].deviation_particle_ids == particle_id )[0] ):\n",
    "            deviation_parameter = score_reports[model_index].deviation_parameters[deviation_index]\n",
    "            deviation_time      = score_reports[model_index].deviation_times[deviation_index]\n",
    "            deviation_cluster   = score_reports[model_index].deviation_clusters[deviation_index]\n",
    "\n",
    "            line_label = f\"{deviation_parameter.name.lower()} deviation, cluster {deviation_cluster}\"\n",
    "\n",
    "            if deviation_parameter == DeviationParameter.RADIUS:\n",
    "                ax_h_be[0][0].axvline( x=deviation_time,linewidth=1, linestyle=\"--\", label=line_label, color=colormap( deviation_cluster ) )\n",
    "                ax_h_be[1][0].axvline( x=deviation_time,linewidth=1, linestyle=\"--\", label=line_label, color=colormap( deviation_cluster ) )\n",
    "                \n",
    "                ax_h_cusum[0][0].axvline( x=deviation_time, linewidth=1, linestyle=\"--\", label=line_label, color=colormap( deviation_cluster ) )\n",
    "                ax_h_cusum[0][1].axvline( x=deviation_time, linewidth=1, linestyle=\"--\", label=line_label, color=colormap( deviation_cluster ) )\n",
    "                ax_h_cusum[1][0].axvline( x=deviation_time, linewidth=1, linestyle=\"--\", label=line_label, color=colormap( deviation_cluster ) )\n",
    "                ax_h_cusum[1][1].axvline( x=deviation_time, linewidth=1, linestyle=\"--\", label=line_label, color=colormap( deviation_cluster ) )\n",
    "            else:\n",
    "                ax_h_be[0][1].axvline( x=deviation_time,linewidth=1, linestyle=\"--\", label=line_label, color=colormap( deviation_cluster ) )\n",
    "                ax_h_be[1][1].axvline( x=deviation_time,linewidth=1, linestyle=\"--\", label=line_label, color=colormap( deviation_cluster ) )\n",
    "                \n",
    "                ax_h_cusum[0][1].axvline( x=deviation_time, linewidth=1, linestyle=\"--\", label=line_label, color=colormap( deviation_cluster ) )\n",
    "                ax_h_cusum[1][0].axvline( x=deviation_time, linewidth=1, linestyle=\"--\", label=line_label, color=colormap( deviation_cluster ) )\n",
    "                ax_h_cusum[1][1].axvline( x=deviation_time, linewidth=1, linestyle=\"--\", label=line_label, color=colormap( deviation_cluster ) )\n",
    "\n",
    "        ax_h_cusum[0][0].legend()\n",
    "        ax_h_cusum[0][1].legend()\n",
    "        ax_h_cusum[1][0].legend()\n",
    "        ax_h_cusum[1][1].legend()\n",
    "\n",
    "        ax_h_be[0][0].legend()\n",
    "        ax_h_be[0][1].legend()\n",
    "        ax_h_be[1][0].legend()\n",
    "        ax_h_be[1][1].legend()\n",
    "        \n",
    "        fig_h_bdf.tight_layout()\n",
    "        fig_h_be.tight_layout()\n",
    "        fig_h_cusum.tight_layout()\n",
    "\n",
    "        fig_h_bdf.show()\n",
    "        fig_h_be.show()\n",
    "        fig_h_cusum.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace0d1a7-ea76-498e-bc66-8696e728e0de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
